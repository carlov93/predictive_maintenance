{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader\n",
    "import torch.optim as optim\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# own Modules \n",
    "from models_sub_net_ls import LstmMse_LatentSpace, LstmMle_LatentSpace, AnalysisLayer\n",
    "from data_preperator import DataPreperatorPrediction\n",
    "from data_set import DataSet\n",
    "from predictor import PredictorMse, PredictorMle, PredictorMseLatentSpaceAnalyser, PredictorMleLatentSpaceAnalyser"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Take care of these things before training:\n",
    "- Select correct path and define droped_features\n",
    "- Change parameter of model\n",
    "- Change filed_location"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parameters phm data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "param = {\n",
    "    \"data\" : {\n",
    "        \"path\" : '../../data/phm_data_challenge/01_M01_DC_prediction_1.csv',\n",
    "        \"droped_feature\" : [\"stage\", \"Lot\", \"runnum\", \"recipe\", \"recipe_step\",\n",
    "                            \"up time\", \"ongoing time\", \n",
    "                            \"ETCHSOURCEUSAGE\", \"ETCHAUXSOURCETIMER\", \n",
    "                            \"ETCHAUX2SOURCETIMER\", \"FIXTURESHUTTERPOSITION\", \"ROTATIONSPEED\"\n",
    "                           ],\n",
    "        \"features_not_to_scale\": []\n",
    "    },\n",
    "    \"model\" : {\n",
    "        \"path\" : \"../../models/MLE_model/phm_dataFold xx_InputSize13_LayerLstm1_HiddenLstm15_HiddenFc75_Seq25.pt\",\n",
    "        \"input_size\" : 12,\n",
    "        \"n_hidden_lstm\" : 15,\n",
    "        \"sequence_size\" : 100,\n",
    "        \"batch_size\" : 50,\n",
    "        \"lstm_layer\" : 1,\n",
    "        \"n_hidden_fc_pred\": 75,\n",
    "        \"n_hidden_fc_ls\": 15,\n",
    "        \"dropout_rate_lstm\": 0.0,\n",
    "        \"dropout_rate_fc\": 0.2,\n",
    "        \"K\":1\n",
    "    },\n",
    "        \"anomaly_detection\" : {\n",
    "        \"threshold_anomaly\" : 0.3,\n",
    "        \"smooth_rate\" : 0.05,\n",
    "        \"no_standard_deviation\" : 2\n",
    "    },\n",
    "    \"results\": {\n",
    "        \"path\" : \"../visualisation/files/prediction/MLE/phm_mle_1.csv\",\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parameters artifical data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "param = {\n",
    "    \"data\" : {\n",
    "        \"path\" : \"./y_hat.csv\",\n",
    "        \"droped_feature\" : [\n",
    "                           ],\n",
    "        \"features_not_to_scale\": []\n",
    "    },\n",
    "    \"model\" : {\n",
    "        \"path\" : \"../../../../models/MLE_latent_space/artifical_2_signals_InputSize2_LayerLstm1_HiddenLstm15_HiddenFc_pred75_HiddenFc_ls7_Seq25.pt\",\n",
    "        \"input_size\" : 2,\n",
    "        \"n_hidden_lstm\" : 15,\n",
    "        \"sequence_size\" : 100,\n",
    "        \"batch_size\" : 50,\n",
    "        \"lstm_layer\" : 1,\n",
    "        \"n_hidden_fc_pred\": 75,\n",
    "        \"n_hidden_fc_ls\": 7,\n",
    "        \"dropout_rate_lstm\": 0.0,\n",
    "        \"dropout_rate_fc\": 0.2,\n",
    "        \"K\":1\n",
    "    },\n",
    "        \"anomaly_detection\" : {\n",
    "        \"threshold_anomaly\" : 0.3,\n",
    "        \"smooth_rate\" : 0.05,\n",
    "        \"no_standard_deviation\" : 2\n",
    "    },\n",
    "    \"results\": {\n",
    "        \"path\" : \"../../../visualisation/files/prediction/MLE_LS/artifical_2_signals_ls_7_yhat.csv\",\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parameters cpps data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "param = {\n",
    "    \"data\" : {\n",
    "        \"path\" : '../../data/cpps_data/cpps_data_predictive_maintenance.csv',\n",
    "        \"droped_feature\" : [\"status\"\n",
    "                           ],\n",
    "        \"features_not_to_scale\": []\n",
    "    },\n",
    "    \"model\" : {\n",
    "        \"path\" : \"../../models/MLE_latent_space/LS_cpps_data_InputSize10_LayerLstm1_HiddenLstm15_HiddenFc_pred75_HiddenFc_ls7_Seq25.pt\",\n",
    "        \"input_size\" : 10,\n",
    "        \"n_hidden_lstm\" : 15,\n",
    "        \"sequence_size\" : 100,\n",
    "        \"batch_size\" : 50,\n",
    "        \"lstm_layer\" : 1,\n",
    "        \"n_hidden_fc_pred\": 75,\n",
    "        \"n_hidden_fc_ls\": 7,\n",
    "        \"dropout_rate_lstm\": 0.0,\n",
    "        \"dropout_rate_fc\": 0.2,\n",
    "        \"K\":1\n",
    "    },\n",
    "        \"anomaly_detection\" : {\n",
    "        \"threshold_anomaly\" : 0.3,\n",
    "        \"smooth_rate\" : 0.05,\n",
    "        \"no_standard_deviation\" : 2\n",
    "    },\n",
    "    \"results\": {\n",
    "        \"path\" : \"../../own_research/seperate_NN_for_LS_analysis?/latent_space/cpps_data_with_subnet.csv\"    # \"../visualisation/files/prediction/MLE_LS/cpps_data.csv\",\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Standarize Data\n",
    "First we have to apply normalisation to data. That is because the model works on the representation given by its input vectors. The scale of those numbers is part of the representation.\n",
    "We should apply the exact same scaling as for training data. That means storing the scale and offset used with your training data, and using that again. <br>\n",
    "__The mean and variance for each feature of the training data with which the model was trained (stake: 0.75):__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mean and Variance from phm Dataset\n",
    "droped features=\"stage\", \"Lot\", \"runnum\", \"recipe\", \"recipe_step\",\n",
    "                            \"up time\", \"ongoing time\", \n",
    "                            \"ETCHSOURCEUSAGE\", \"ETCHAUXSOURCETIMER\", \n",
    "                            \"ETCHAUX2SOURCETIMER\", \"FIXTURESHUTTERPOSITION\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_training_data =[ 0.21683119,  0.32121513,  0.31925213,  0.20097501,  0.45164471,  0.22914814,\n",
    "  0.11604865,  0.27421592,  0.24393222, -0.13974937, -0.09739598, -0.07313758,\n",
    "  0.18198089]\n",
    "var_training_data =[0.75261122, 0.90482986, 0.91105327, 0.75504036, 1.07026701, 0.76708319,\n",
    " 0.35172769, 0.83004988, 0.76964675, 0.57386915, 0.45912309, 0.2955709,\n",
    " 1.61493449]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mean and Variance from artifical Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_training_data= [-0.00393712, -0.01294209]\n",
    "var_training_data = [49.18936568,  0.34270256]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mean and Variance form cpps Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_training_data = [0.05330162, 0.03075699, -0.05636937, 0.0274802, 0.06536314, -0.04620979,-0.0745559, \n",
    "                      -0.08149049, -0.05318843, 0.11105582]\n",
    "var_training_data = [0.02905961, 0.04473883, 0.05254194, 0.05198144, 0.07337494, 0.0666981, 0.07593811, \n",
    "                     0.0393896, 0.08028017, 0.0594492]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_training_data = [0., 0.]\n",
    "var_training_data = [1., 1.]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(11900, 3)\n"
     ]
    }
   ],
   "source": [
    "data_preperator = DataPreperatorPrediction(path=param['data']['path'], \n",
    "                                           ignored_features = param[\"data\"][\"droped_feature\"],\n",
    "                                           mean_training_data=mean_training_data, \n",
    "                                           var_training_data=var_training_data, \n",
    "                                           first_order_difference=False \n",
    "                                          )                                  \n",
    "preprocessed_data = data_preperator.prepare_data()\n",
    "print(preprocessed_data.shape)\n",
    "\n",
    "dataset = DataSet(preprocessed_data, \n",
    "                  timesteps=param[\"model\"][\"sequence_size\"])\n",
    "data_loader = DataLoader(dataset, \n",
    "                         batch_size=param['model']['batch_size'], \n",
    "                         num_workers=0, \n",
    "                         shuffle=False, \n",
    "                         drop_last=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data of batch: 0\n",
      "Size of input data: torch.Size([50, 100, 3])\n",
      "Size of target data: torch.Size([50, 3])\n",
      "Data of batch: 1\n",
      "Size of input data: torch.Size([50, 100, 3])\n",
      "Size of target data: torch.Size([50, 3])\n"
     ]
    }
   ],
   "source": [
    "for batch_idx, data in enumerate(data_loader):\n",
    "    x,y = data\n",
    "    print('Data of batch: {}'.format(batch_idx))\n",
    "    print(\"Size of input data: {}\".format(x.size()))\n",
    "    print(\"Size of target data: {}\".format(y.size()))\n",
    "    if batch_idx >=1: break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define Model and load Parameters of trained model\n",
    "### Model for MSE and Latent Space Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = LstmMse_LatentSpace(batch_size=param['model']['batch_size'], \n",
    "                            input_dim=param['model']['input_size'], \n",
    "                            n_hidden_lstm=param['model']['n_hidden_lstm'], \n",
    "                            n_layers=param['model']['lstm_layer'],\n",
    "                            dropout_rate_lstm= param['model']['dropout_rate_lstm'],\n",
    "                            dropout_rate_fc= param['model']['dropout_rate_fc'],\n",
    "                            n_hidden_fc_prediction=param['model']['n_hidden_fc_pred'], \n",
    "                            n_hidden_fc_ls_analysis=param['model']['n_hidden_fc_ls']      \n",
    "                            )\n",
    "\n",
    "checkpoint = torch.load(param[\"model\"][\"path\"])\n",
    "model.load_state_dict(checkpoint['model_state_dict'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model for MLE and Latent Space Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "IncompatibleKeys(missing_keys=[], unexpected_keys=[])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = LstmMle_LatentSpace(batch_size=param['model']['batch_size'], \n",
    "                            input_dim=param['model']['input_size'], \n",
    "                            n_hidden_lstm=param['model']['n_hidden_lstm'], \n",
    "                            n_layers=param['model']['lstm_layer'],\n",
    "                            dropout_rate_lstm= param['model']['dropout_rate_lstm'],\n",
    "                            dropout_rate_fc= param['model']['dropout_rate_fc'],\n",
    "                            n_hidden_fc_prediction=param['model']['n_hidden_fc_pred'], \n",
    "                            n_hidden_fc_ls_analysis=param['model']['n_hidden_fc_ls'],\n",
    "                            K = param['model']['K']\n",
    "                            )\n",
    "\n",
    "checkpoint = torch.load(param[\"model\"][\"path\"])\n",
    "model.load_state_dict(checkpoint['model_state_dict'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialize Predictor\n",
    "### Predictor for MSE Model and Latent Space Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictor = PredictorMseLatentSpaceAnalyser(model=model,\n",
    "                                            path_data=param[\"data\"][\"path\"],\n",
    "                                            columns_to_ignore=param[\"data\"][\"droped_feature\"],\n",
    "                                            threshold_anomaly=param[\"anomaly_detection\"][\"threshold_anomaly\"]\n",
    "                                            )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Predictor for MLE Model and Latent Space Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictor = PredictorMleLatentSpaceAnalyser(model=model,\n",
    "                                            path_data=param[\"data\"][\"path\"],\n",
    "                                            columns_to_ignore=param[\"data\"][\"droped_feature\"],\n",
    "                                            threshold_anomaly=param[\"anomaly_detection\"][\"threshold_anomaly\"],\n",
    "                                            no_standard_deviation=param[\"anomaly_detection\"][\"no_standard_deviation\"]\n",
    "                                            )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start predicting.\n",
      "Current status: 0 samples are predicted.\n",
      "Current status: 5000 samples are predicted.\n",
      "Current status: 10000 samples are predicted.\n",
      "End of prediction.\n"
     ]
    }
   ],
   "source": [
    "print(\"Start predicting.\")\n",
    "# Write header\n",
    "with open(param[\"results\"][\"path\"], \"a+\") as file:\n",
    "            [file.write(column+\";\") for column in predictor.create_column_names_result()]\n",
    "            file.write(\"\\n\")\n",
    "\n",
    "for batch_number, (input_data, target_data) in enumerate(data_loader):\n",
    "    # Predict sensor values in mini-batches\n",
    "    batch_results = predictor.predict(input_data, target_data)\n",
    "    \n",
    "    # Write results to csv file\n",
    "    with open(param[\"results\"][\"path\"], \"a\") as file:\n",
    "        for batch in batch_results:\n",
    "            # Each result component of a singe prediction (ID, target, prediction, loss, latent space ...) is stored in lists\n",
    "            # thus we have to unpack the list and seperate values with ;\n",
    "            for value in batch:\n",
    "                file.write(str(value)+\";\")\n",
    "            file.write(\"\\n\")\n",
    "\n",
    "    # Print status \n",
    "    if (batch_number*param['model']['batch_size'])%5000 == 0:\n",
    "        print(\"Current status: \" + str(param['model']['batch_size']*batch_number) + \" samples are predicted.\")\n",
    "\n",
    "print(\"End of prediction.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tag anomalous samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_prediction = pd.read_csv(param[\"results\"][\"path\"], sep=\";\")\n",
    "# Values of column \"loss\" are exponentially smoothed and stored in a new column \"smoothed loss\"\n",
    "# New column \"anomaly\" is created and sample is taged with 1 if anomalous behaviour (if smoothed loss is over threshold)\n",
    "results = predictor.detect_anomaly(results_prediction, param[\"anomaly_detection\"][\"smooth_rate\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>sine_signal mu predicted target</th>\n",
       "      <th>sawtooth_signal mu predicted target</th>\n",
       "      <th>sine_signal mu predicted mu predicted</th>\n",
       "      <th>sawtooth_signal mu predicted mu predicted</th>\n",
       "      <th>sine_signal mu predicted sigma predicted</th>\n",
       "      <th>sawtooth_signal mu predicted sigma predicted</th>\n",
       "      <th>mean normalised residual</th>\n",
       "      <th>sine_signal mu predicted normalised residual</th>\n",
       "      <th>sawtooth_signal mu predicted normalised residual</th>\n",
       "      <th>latent_space_0</th>\n",
       "      <th>latent_space_1</th>\n",
       "      <th>latent_space_2</th>\n",
       "      <th>latent_space_3</th>\n",
       "      <th>latent_space_4</th>\n",
       "      <th>latent_space_5</th>\n",
       "      <th>latent_space_6</th>\n",
       "      <th>Anomaly Sensor_1</th>\n",
       "      <th>Anomaly Sensor_2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>200.0</td>\n",
       "      <td>-0.049047</td>\n",
       "      <td>-0.759869</td>\n",
       "      <td>-0.098082</td>\n",
       "      <td>-0.606751</td>\n",
       "      <td>0.092884</td>\n",
       "      <td>0.761129</td>\n",
       "      <td>0.163374</td>\n",
       "      <td>0.527920</td>\n",
       "      <td>-0.201173</td>\n",
       "      <td>0.211074</td>\n",
       "      <td>-0.543295</td>\n",
       "      <td>0.222157</td>\n",
       "      <td>-0.034048</td>\n",
       "      <td>-0.454977</td>\n",
       "      <td>-0.235163</td>\n",
       "      <td>0.249272</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>201.0</td>\n",
       "      <td>0.184273</td>\n",
       "      <td>-1.507484</td>\n",
       "      <td>0.055531</td>\n",
       "      <td>-1.313694</td>\n",
       "      <td>0.087663</td>\n",
       "      <td>0.232298</td>\n",
       "      <td>0.317183</td>\n",
       "      <td>1.468598</td>\n",
       "      <td>-0.834232</td>\n",
       "      <td>0.101639</td>\n",
       "      <td>-0.438303</td>\n",
       "      <td>0.278842</td>\n",
       "      <td>-0.028266</td>\n",
       "      <td>-0.835308</td>\n",
       "      <td>-0.287276</td>\n",
       "      <td>0.185812</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>202.0</td>\n",
       "      <td>0.380385</td>\n",
       "      <td>-1.345762</td>\n",
       "      <td>0.283976</td>\n",
       "      <td>-1.351027</td>\n",
       "      <td>0.085379</td>\n",
       "      <td>0.198988</td>\n",
       "      <td>0.577823</td>\n",
       "      <td>1.129189</td>\n",
       "      <td>0.026458</td>\n",
       "      <td>0.100282</td>\n",
       "      <td>-0.375407</td>\n",
       "      <td>0.282706</td>\n",
       "      <td>-0.013414</td>\n",
       "      <td>-0.816674</td>\n",
       "      <td>-0.348275</td>\n",
       "      <td>0.116948</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>203.0</td>\n",
       "      <td>0.572868</td>\n",
       "      <td>-1.161470</td>\n",
       "      <td>0.498016</td>\n",
       "      <td>-1.168659</td>\n",
       "      <td>0.085163</td>\n",
       "      <td>0.195662</td>\n",
       "      <td>0.457833</td>\n",
       "      <td>0.878926</td>\n",
       "      <td>0.036741</td>\n",
       "      <td>0.060221</td>\n",
       "      <td>-0.295059</td>\n",
       "      <td>0.261399</td>\n",
       "      <td>0.008664</td>\n",
       "      <td>-0.751295</td>\n",
       "      <td>-0.414880</td>\n",
       "      <td>0.076788</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>204.0</td>\n",
       "      <td>0.703928</td>\n",
       "      <td>-0.965978</td>\n",
       "      <td>0.668567</td>\n",
       "      <td>-0.981007</td>\n",
       "      <td>0.085837</td>\n",
       "      <td>0.195343</td>\n",
       "      <td>0.244443</td>\n",
       "      <td>0.411950</td>\n",
       "      <td>0.076937</td>\n",
       "      <td>0.023399</td>\n",
       "      <td>-0.202114</td>\n",
       "      <td>0.223526</td>\n",
       "      <td>0.014928</td>\n",
       "      <td>-0.677177</td>\n",
       "      <td>-0.480439</td>\n",
       "      <td>0.048368</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      ID  sine_signal mu predicted target  \\\n",
       "0  200.0                        -0.049047   \n",
       "1  201.0                         0.184273   \n",
       "2  202.0                         0.380385   \n",
       "3  203.0                         0.572868   \n",
       "4  204.0                         0.703928   \n",
       "\n",
       "   sawtooth_signal mu predicted target  sine_signal mu predicted mu predicted  \\\n",
       "0                            -0.759869                              -0.098082   \n",
       "1                            -1.507484                               0.055531   \n",
       "2                            -1.345762                               0.283976   \n",
       "3                            -1.161470                               0.498016   \n",
       "4                            -0.965978                               0.668567   \n",
       "\n",
       "   sawtooth_signal mu predicted mu predicted  \\\n",
       "0                                  -0.606751   \n",
       "1                                  -1.313694   \n",
       "2                                  -1.351027   \n",
       "3                                  -1.168659   \n",
       "4                                  -0.981007   \n",
       "\n",
       "   sine_signal mu predicted sigma predicted  \\\n",
       "0                                  0.092884   \n",
       "1                                  0.087663   \n",
       "2                                  0.085379   \n",
       "3                                  0.085163   \n",
       "4                                  0.085837   \n",
       "\n",
       "   sawtooth_signal mu predicted sigma predicted  mean normalised residual  \\\n",
       "0                                      0.761129                  0.163374   \n",
       "1                                      0.232298                  0.317183   \n",
       "2                                      0.198988                  0.577823   \n",
       "3                                      0.195662                  0.457833   \n",
       "4                                      0.195343                  0.244443   \n",
       "\n",
       "   sine_signal mu predicted normalised residual  \\\n",
       "0                                      0.527920   \n",
       "1                                      1.468598   \n",
       "2                                      1.129189   \n",
       "3                                      0.878926   \n",
       "4                                      0.411950   \n",
       "\n",
       "   sawtooth_signal mu predicted normalised residual  latent_space_0  \\\n",
       "0                                         -0.201173        0.211074   \n",
       "1                                         -0.834232        0.101639   \n",
       "2                                          0.026458        0.100282   \n",
       "3                                          0.036741        0.060221   \n",
       "4                                          0.076937        0.023399   \n",
       "\n",
       "   latent_space_1  latent_space_2  latent_space_3  latent_space_4  \\\n",
       "0       -0.543295        0.222157       -0.034048       -0.454977   \n",
       "1       -0.438303        0.278842       -0.028266       -0.835308   \n",
       "2       -0.375407        0.282706       -0.013414       -0.816674   \n",
       "3       -0.295059        0.261399        0.008664       -0.751295   \n",
       "4       -0.202114        0.223526        0.014928       -0.677177   \n",
       "\n",
       "   latent_space_5  latent_space_6  Anomaly Sensor_1  Anomaly Sensor_2  \n",
       "0       -0.235163        0.249272                 0                 0  \n",
       "1       -0.287276        0.185812                 0                 0  \n",
       "2       -0.348275        0.116948                 0                 0  \n",
       "3       -0.414880        0.076788                 0                 0  \n",
       "4       -0.480439        0.048368                 0                 0  "
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Combine prediction data with data which was not consider for inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "original_sensor_data = pd.read_csv(param[\"data\"][\"path\"])\n",
    "data_of_droped_feature = original_sensor_data.loc[:, param[\"data\"][\"droped_feature\"]+[\"ID\"]]\n",
    "complete_data = results.merge(right=data_of_droped_feature, how=\"inner\", on=\"ID\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save data to csv file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "complete_data.to_csv(param[\"results\"][\"path\"], sep=\";\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "masterarbeit",
   "language": "python",
   "name": "masterarbeit"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
