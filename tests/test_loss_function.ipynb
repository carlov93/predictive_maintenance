{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 263,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.nn.utils import clip_grad_norm_\n",
    "from torch.autograd import Variable\n",
    "from torch.autograd.function import Function\n",
    "from torchsummary import summary\n",
    "import pandas as pd\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import matplotlib.pyplot as plt\n",
    "import torch.optim as optim\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from keras.preprocessing.sequence import TimeseriesGenerator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 264,
   "metadata": {},
   "outputs": [],
   "source": [
    "hyperparam = {\n",
    "    \"data\" : {\n",
    "        \"stake_training_data\" : 0.75, \n",
    "        \"path\" : '../../../data/vega_shrinkwrapper_original/NewBlade/'\n",
    "    },\n",
    "    \"model\" : {\n",
    "        \"input_size\" : 7,\n",
    "        \"n_hidden\" : 25,\n",
    "        \"sequence_size\" : 20,\n",
    "        \"batch_size\" : 1,\n",
    "        \"lstm_layer\" : 3,\n",
    "    },\n",
    "    \"cycling_lr\" : {\n",
    "        # step_size is the number of training iterations (total samples/batch_size) per half cycle. \n",
    "        # Authors suggest setting step_size 2-8 x training iterations in epoch.\n",
    "        \"step_size\" : (1536/8)*2, \n",
    "        # Mode can be one of {triangular, triangular2, exp_range}\n",
    "        \"mode\" : \"triangular\", \n",
    "        \"gamma\" : 0.9995,\n",
    "        \"base_lr\" : 3e-2, \n",
    "        \"max_lr\" :0.1\n",
    "    },\n",
    "    \"training\": {\n",
    "        \"n_epochs\" : 20,\n",
    "        \"patience\" : 50,\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 265,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 1.9269,  1.4873,  0.9007, -2.1055,  0.6784, -1.2345, -0.0431],\n",
      "        [-1.6047, -0.7521,  1.6487, -0.3925, -1.4036, -0.7279, -0.5594],\n",
      "        [-0.7688,  0.7624,  1.6423, -0.1596, -0.4974,  0.4396, -0.7581],\n",
      "        [ 1.0783,  0.8008,  1.6806,  1.2791,  1.2964,  0.6105,  1.3347],\n",
      "        [-0.2316,  0.0418, -0.2516,  0.8599, -1.3847, -0.8712, -0.2234],\n",
      "        [ 1.7174,  0.3189, -0.4245,  0.3057, -0.7746, -0.8371, -0.9224],\n",
      "        [ 1.8113,  0.1606,  0.3672,  0.1754,  1.3852, -0.4459, -1.2024],\n",
      "        [ 0.7078, -1.0759,  0.5357,  1.1754,  0.5612, -0.4527, -0.7718]])\n",
      "tensor([[ 0.1453,  0.2311,  0.0087, -0.1423,  0.1971, -1.1441,  0.3383],\n",
      "        [ 1.6992,  2.8140,  0.3598, -0.0898,  0.4584, -0.5644,  1.0563],\n",
      "        [-1.4692,  1.4332,  0.7281, -0.7106, -0.6021,  0.9604,  0.4048],\n",
      "        [-1.3543, -0.4976,  0.4747, -0.1976,  1.2683,  1.2243,  0.0981],\n",
      "        [ 1.7423, -1.3527,  0.2191,  0.5526, -0.6788,  0.5743,  0.1877],\n",
      "        [-0.3576, -0.3165,  0.5886, -0.8905,  0.4098,  1.9312,  1.0119],\n",
      "        [-1.4364, -1.1299, -0.1360,  1.6354,  0.6547,  0.5760,  1.1415],\n",
      "        [ 0.0186, -1.8058,  0.9254, -0.3753,  1.0331, -0.6867,  0.6368]])\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(42)\n",
    "x = torch.randn(8, 7, dtype=torch.float)\n",
    "target_data = torch.randn(8, 7, dtype=torch.float)\n",
    "print(x)\n",
    "print(target_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 266,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SingleLayer(nn.Module):\n",
    "    def __init__(self, batch_size, input_dim, n_hidden, n_layers):\n",
    "        super(SingleLayer, self).__init__()\n",
    "        # Attributes for LSTM Network\n",
    "        self.input_dim = input_dim\n",
    "        self.n_hidden = n_hidden\n",
    "        self.n_layers = n_layers\n",
    "        self.batch_size = batch_size\n",
    "        \n",
    "        # Attribut for Gaussians\n",
    "        self.n_gaus_param = 1\n",
    "        \n",
    "        # Definition of NN layer\n",
    "        self.fc1 = nn.Linear(self.input_dim, self.n_hidden)\n",
    "        self.fc2 = nn.Linear(self.n_hidden, self.input_dim)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = torch.tanh(self.fc1(x))\n",
    "        out = self.fc2(x)\n",
    "        return out\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 267,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LossModule(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        \"\"\"\n",
    "        In the constructor we instantiate the module and assign them as\n",
    "        member variables.\n",
    "        \"\"\"\n",
    "        super(LossModule, self).__init__()\n",
    "\n",
    "    def forward(self, output, target_data):\n",
    "        \"\"\"\n",
    "        In the forward function we accept a Tensor of input data and we must return\n",
    "        a Tensor of output data. \n",
    "        \"\"\"\n",
    "        # Sum over each feature\n",
    "        mean_loss = torch.sum(((output - target_data) ** 2), dim=1) / 7\n",
    "        # Sum over all samples and take the mean to get mini-batch MSE\n",
    "        mean_loss = torch.sum(mean_loss) / 8\n",
    "        return mean_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 268,
   "metadata": {},
   "outputs": [],
   "source": [
    "class OwnLossFunction(Function):\n",
    "    @staticmethod\n",
    "    # bias is an optional argument\n",
    "    def forward(ctx, output, target_data):\n",
    "        # Select and transform data\n",
    "        ctx.save_for_backward(output, target_data)\n",
    "        \n",
    "        # Compute loss\n",
    "        # Sum over each feature\n",
    "        mean_loss = torch.sum(((output - target_data) ** 2), dim=1) / 7\n",
    "        # Sum over all samples and take the mean to get mini-batch MSE\n",
    "        mean_loss = torch.sum(mean_loss) / 8\n",
    "        return mean_loss\n",
    "    \n",
    "    @staticmethod\n",
    "    def backward(ctx, grad_input):\n",
    "        \"\"\"\n",
    "        grad_output = 1, because it is initialize by Optimizer \n",
    "        In the backward pass we need to compute the gradient of the loss\n",
    "        with respect to the input.\n",
    "        \"\"\"\n",
    "        output, target_data = ctx.saved_tensors # this is a tensor of models output\n",
    "        grad_output = 1/7*2.*(output-target_data)\n",
    "        #print(grad_output)\n",
    "        return grad_output, None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 269,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(0)\n",
    "model = SingleLayer(batch_size=hyperparam['model']['batch_size'], input_dim=hyperparam['model']['input_size'], \n",
    "             n_hidden=hyperparam['model']['n_hidden'], n_layers=hyperparam['model']['lstm_layer'])\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=0.01)  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loss function with nn.MSELoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 270,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start model training\n",
      "-------- epoch_no. 0 finished with training loss 0.9291448593139648--------\n",
      "-------- epoch_no. 1 finished with training loss 0.9243056774139404--------\n",
      "-------- epoch_no. 2 finished with training loss 0.9195181727409363--------\n",
      "-------- epoch_no. 3 finished with training loss 0.9147815108299255--------\n",
      "-------- epoch_no. 4 finished with training loss 0.9100947380065918--------\n",
      "-------- epoch_no. 5 finished with training loss 0.9054569602012634--------\n",
      "-------- epoch_no. 6 finished with training loss 0.9008673429489136--------\n",
      "-------- epoch_no. 7 finished with training loss 0.8963251113891602--------\n",
      "-------- epoch_no. 8 finished with training loss 0.8918293118476868--------\n",
      "-------- epoch_no. 9 finished with training loss 0.8873791694641113--------\n",
      "-------- epoch_no. 10 finished with training loss 0.8829740285873413--------\n",
      "-------- epoch_no. 11 finished with training loss 0.8786129355430603--------\n",
      "-------- epoch_no. 12 finished with training loss 0.8742952346801758--------\n",
      "-------- epoch_no. 13 finished with training loss 0.8700202107429504--------\n",
      "-------- epoch_no. 14 finished with training loss 0.8657870888710022--------\n",
      "-------- epoch_no. 15 finished with training loss 0.8615951538085938--------\n",
      "-------- epoch_no. 16 finished with training loss 0.8574437499046326--------\n",
      "-------- epoch_no. 17 finished with training loss 0.8533321619033813--------\n",
      "-------- epoch_no. 18 finished with training loss 0.8492597341537476--------\n",
      "-------- epoch_no. 19 finished with training loss 0.8452259302139282--------\n",
      "-------- epoch_no. 20 finished with training loss 0.8412299156188965--------\n"
     ]
    }
   ],
   "source": [
    "print(\"Start model training\")\n",
    "for epoch in range(hyperparam[\"training\"][\"n_epochs\"]+1):\n",
    "    # Empty list for recording performance \n",
    "    epoch_training_loss = []\n",
    "    epoch_validation_loss = []\n",
    "    \n",
    "    model.train()\n",
    "        \n",
    "    # Zero out gradient, else they will accumulate between minibatches\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    # Forward propagation\n",
    "    output = model(x)\n",
    "\n",
    "    # Calculate loss\n",
    "    criterion = nn.MSELoss()\n",
    "    loss = criterion(output, target_data)\n",
    "\n",
    "    # Backward pass\n",
    "    loss.backward()\n",
    "    #params = list(model.parameters())\n",
    "    #print(params[0].grad)\n",
    "\n",
    "    # Update parameters\n",
    "    optimizer.step()\n",
    "\n",
    "    print(\"-------- epoch_no. {} finished with training loss {}--------\".format(epoch, loss.item()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loss function with own Module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 271,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(0)\n",
    "model = SingleLayer(batch_size=hyperparam['model']['batch_size'], input_dim=hyperparam['model']['input_size'], \n",
    "             n_hidden=hyperparam['model']['n_hidden'], n_layers=hyperparam['model']['lstm_layer'])\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=0.01)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 272,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start model training\n",
      "-------- epoch_no. 0 finished with training loss 0.9291448593139648--------\n",
      "-------- epoch_no. 1 finished with training loss 0.9243056774139404--------\n",
      "-------- epoch_no. 2 finished with training loss 0.9195181727409363--------\n",
      "-------- epoch_no. 3 finished with training loss 0.9147815108299255--------\n",
      "-------- epoch_no. 4 finished with training loss 0.910094678401947--------\n",
      "-------- epoch_no. 5 finished with training loss 0.9054569602012634--------\n",
      "-------- epoch_no. 6 finished with training loss 0.9008672833442688--------\n",
      "-------- epoch_no. 7 finished with training loss 0.8963251113891602--------\n",
      "-------- epoch_no. 8 finished with training loss 0.891829252243042--------\n",
      "-------- epoch_no. 9 finished with training loss 0.8873792290687561--------\n",
      "-------- epoch_no. 10 finished with training loss 0.8829740285873413--------\n",
      "-------- epoch_no. 11 finished with training loss 0.8786129355430603--------\n",
      "-------- epoch_no. 12 finished with training loss 0.8742952942848206--------\n",
      "-------- epoch_no. 13 finished with training loss 0.8700201511383057--------\n",
      "-------- epoch_no. 14 finished with training loss 0.8657870292663574--------\n",
      "-------- epoch_no. 15 finished with training loss 0.8615951538085938--------\n",
      "-------- epoch_no. 16 finished with training loss 0.857443630695343--------\n",
      "-------- epoch_no. 17 finished with training loss 0.8533322215080261--------\n",
      "-------- epoch_no. 18 finished with training loss 0.8492597341537476--------\n",
      "-------- epoch_no. 19 finished with training loss 0.8452258706092834--------\n",
      "-------- epoch_no. 20 finished with training loss 0.8412299156188965--------\n"
     ]
    }
   ],
   "source": [
    "print(\"Start model training\")\n",
    "for epoch in range(hyperparam[\"training\"][\"n_epochs\"]+1):\n",
    "    # Empty list for recording performance \n",
    "    epoch_training_loss = []\n",
    "    epoch_validation_loss = []\n",
    "    \n",
    "    model.train()\n",
    "        \n",
    "    # Zero out gradient, else they will accumulate between minibatches\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    # Forward propagation\n",
    "    output = model(x)\n",
    "    #print(output)\n",
    "\n",
    "    # Calculate loss\n",
    "    criterion = LossModule()\n",
    "    loss = criterion(output, target_data)\n",
    "\n",
    "    # Backward pass\n",
    "    loss.backward()\n",
    "    #params = list(model.parameters())\n",
    "    #print(params[0].grad)\n",
    "\n",
    "    # Update parameters\n",
    "    optimizer.step()\n",
    "\n",
    "    print(\"-------- epoch_no. {} finished with training loss {}--------\".format(epoch, loss.item()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loss function with own Function -- !! Ist nicht gleich wie die anderen beiden Varianten !!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 273,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(0)\n",
    "model = SingleLayer(batch_size=hyperparam['model']['batch_size'], input_dim=hyperparam['model']['input_size'], \n",
    "             n_hidden=hyperparam['model']['n_hidden'], n_layers=hyperparam['model']['lstm_layer'])\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=0.01)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 274,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start model training\n",
      "-------- epoch_no. 0 finished with training loss 0.9291448593139648--------\n",
      "-------- epoch_no. 1 finished with training loss 0.8911610245704651--------\n",
      "-------- epoch_no. 2 finished with training loss 0.8562833666801453--------\n",
      "-------- epoch_no. 3 finished with training loss 0.8240970373153687--------\n",
      "-------- epoch_no. 4 finished with training loss 0.7942553758621216--------\n",
      "-------- epoch_no. 5 finished with training loss 0.766467809677124--------\n",
      "-------- epoch_no. 6 finished with training loss 0.7404906153678894--------\n",
      "-------- epoch_no. 7 finished with training loss 0.7161188125610352--------\n",
      "-------- epoch_no. 8 finished with training loss 0.6931804418563843--------\n",
      "-------- epoch_no. 9 finished with training loss 0.6715306043624878--------\n",
      "-------- epoch_no. 10 finished with training loss 0.6510477662086487--------\n",
      "-------- epoch_no. 11 finished with training loss 0.6316291093826294--------\n",
      "-------- epoch_no. 12 finished with training loss 0.6131881475448608--------\n",
      "-------- epoch_no. 13 finished with training loss 0.5956515073776245--------\n",
      "-------- epoch_no. 14 finished with training loss 0.578956663608551--------\n",
      "-------- epoch_no. 15 finished with training loss 0.5630500912666321--------\n",
      "-------- epoch_no. 16 finished with training loss 0.5478855967521667--------\n",
      "-------- epoch_no. 17 finished with training loss 0.5334227085113525--------\n",
      "-------- epoch_no. 18 finished with training loss 0.5196256041526794--------\n",
      "-------- epoch_no. 19 finished with training loss 0.5064622163772583--------\n",
      "-------- epoch_no. 20 finished with training loss 0.4939032196998596--------\n"
     ]
    }
   ],
   "source": [
    "print(\"Start model training\")\n",
    "\n",
    "for epoch in range(hyperparam[\"training\"][\"n_epochs\"]+1):\n",
    "    # Empty list for recording performance \n",
    "    epoch_training_loss = []\n",
    "    epoch_validation_loss = []\n",
    "    \n",
    "    model.train()\n",
    "        \n",
    "    # Zero out gradient, else they will accumulate between minibatches\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    # Forward propagation\n",
    "    output = model(x)\n",
    "\n",
    "    # Calculate loss\n",
    "    loss = OwnLossFunction.apply(output, target_data)\n",
    "\n",
    "    # Backward pass\n",
    "    loss.backward()\n",
    "    #params = list(model.parameters())\n",
    "    #print(params[0].grad)\n",
    "\n",
    "    # Update parameters\n",
    "    optimizer.step()\n",
    "\n",
    "    print(\"-------- epoch_no. {} finished with training loss {}--------\".format(epoch, loss.item()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "masterarbeit",
   "language": "python",
   "name": "masterarbeit"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
