{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader\n",
    "import torch.optim as optim\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# own Modules \n",
    "from models_sub_net_ls import LstmMse_LatentSpace, LstmMle_LatentSpace, AnalysisLayer\n",
    "from model_cell_state import LstmMleCellState\n",
    "from data_preperator import DataPreperatorPrediction\n",
    "from data_set import DataSet\n",
    "from predictor_latent_space import PredictorMseLatentSpaceAnalyser, PredictorMleLatentSpaceAnalyser\n",
    "from predictor_cell_state import PredictorMleCellState"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Take care of these things before training:\n",
    "- Select correct path and define droped_features\n",
    "- Change parameter of model\n",
    "- Change filed_location"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parameters phm data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "param = {\n",
    "    \"data\" : {\n",
    "        \"path\" : '../../data/phm_data_challenge/01_M01_DC_prediction_1.csv',\n",
    "        \"droped_feature\" : [\"stage\", \"Lot\", \"runnum\", \"recipe\", \"recipe_step\",\n",
    "                            \"up time\", \"ongoing time\", \n",
    "                            \"ETCHSOURCEUSAGE\", \"ETCHAUXSOURCETIMER\", \n",
    "                            \"ETCHAUX2SOURCETIMER\", \"FIXTURESHUTTERPOSITION\", \"ROTATIONSPEED\"\n",
    "                           ],\n",
    "        \"features_not_to_scale\": []\n",
    "    },\n",
    "    \"model\" : {\n",
    "        \"path\" : \"../../models/MLE_model/phm_dataFold xx_InputSize13_LayerLstm1_HiddenLstm15_HiddenFc75_Seq25.pt\",\n",
    "        \"input_size\" : 12,\n",
    "        \"n_hidden_lstm\" : 15,\n",
    "        \"sequence_size\" : 100,\n",
    "        \"batch_size\" : 50,\n",
    "        \"lstm_layer\" : 1,\n",
    "        \"n_hidden_fc_pred\": 75,\n",
    "        \"n_hidden_fc_ls\": 15,\n",
    "        \"dropout_rate_lstm\": 0.0,\n",
    "        \"dropout_rate_fc\": 0.2,\n",
    "        \"K\":1\n",
    "    },\n",
    "        \"anomaly_detection\" : {\n",
    "        \"threshold_anomaly\" : 0.3,\n",
    "        \"smooth_rate\" : 0.05,\n",
    "        \"no_standard_deviation\" : 2\n",
    "    },\n",
    "    \"results\": {\n",
    "        \"path\" : \"../visualisation/files/prediction/MLE/phm_mle_1.csv\",\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parameters artifical data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "param = {\n",
    "    \"data\" : {\n",
    "        \"path\" : '../../data/artifical_signals/artifical_2_signals_errors.csv',\n",
    "        \"droped_feature\" : [\"anomaly\"\n",
    "                           ],\n",
    "        \"features_not_to_scale\": []\n",
    "    },\n",
    "    \"model\" : {\n",
    "        \"path\" : \"../../models/MLE_model/artifical_2_signalsFold_xx_InputSize2_LayerLstm1_HiddenLstm15_HiddenFc75_Seq25.pt\",\n",
    "        \"input_size\" : 2,\n",
    "        \"n_hidden_lstm\" : 15,\n",
    "        \"sequence_size\" : 100,\n",
    "        \"batch_size\" : 50,\n",
    "        \"lstm_layer\" : 1,\n",
    "        \"n_hidden_fc_1\": 75, \n",
    "        \"n_hidden_fc_pred\": 75,\n",
    "        \"n_hidden_fc_ls\": 75,\n",
    "        \"dropout_rate_lstm\": 0.0,\n",
    "        \"dropout_rate_fc\": 0.2,\n",
    "        \"K\":1\n",
    "    },\n",
    "        \"anomaly_detection\" : {\n",
    "        \"threshold_anomaly\" : 0.3,\n",
    "        \"smooth_rate\" : 0.05,\n",
    "        \"no_standard_deviation\" : 2\n",
    "    },\n",
    "    \"results\": {\n",
    "        \"path\" : \"../visualisation/files/prediction/MLE/artifical_2_signals_cell_state.csv\",\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parameters cpps data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "param = {\n",
    "    \"data\" : {\n",
    "        \"path\" : '../../data/cpps_data/cpps_data_predictive_maintenance.csv',\n",
    "        \"droped_feature\" : [\"status\"\n",
    "                           ],\n",
    "        \"features_not_to_scale\": []\n",
    "    },\n",
    "    \"model\" : {\n",
    "        \"path\" : \"../../models/MLE_latent_space/LS_cpps_data_InputSize10_LayerLstm1_HiddenLstm15_HiddenFc_pred75_HiddenFc_ls7_Seq25.pt\",\n",
    "        \"input_size\" : 10,\n",
    "        \"n_hidden_lstm\" : 15,\n",
    "        \"sequence_size\" : 100,\n",
    "        \"batch_size\" : 50,\n",
    "        \"lstm_layer\" : 1,\n",
    "        \"n_hidden_fc_pred\": 75,\n",
    "        \"n_hidden_fc_ls\": 7,\n",
    "        \"dropout_rate_lstm\": 0.0,\n",
    "        \"dropout_rate_fc\": 0.2,\n",
    "        \"K\":1\n",
    "    },\n",
    "        \"anomaly_detection\" : {\n",
    "        \"threshold_anomaly\" : 0.3,\n",
    "        \"smooth_rate\" : 0.05,\n",
    "        \"no_standard_deviation\" : 2\n",
    "    },\n",
    "    \"results\": {\n",
    "        \"path\" : \"../../own_research/seperate_NN_for_LS_analysis?/latent_space/cpps_data_with_subnet.csv\"    # \"../visualisation/files/prediction/MLE_LS/cpps_data.csv\",\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Standarize Data\n",
    "First we have to apply normalisation to data. That is because the model works on the representation given by its input vectors. The scale of those numbers is part of the representation.\n",
    "We should apply the exact same scaling as for training data. That means storing the scale and offset used with your training data, and using that again. <br>\n",
    "__The mean and variance for each feature of the training data with which the model was trained (stake: 0.75):__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mean and Variance from phm Dataset\n",
    "droped features=\"stage\", \"Lot\", \"runnum\", \"recipe\", \"recipe_step\",\n",
    "                            \"up time\", \"ongoing time\", \n",
    "                            \"ETCHSOURCEUSAGE\", \"ETCHAUXSOURCETIMER\", \n",
    "                            \"ETCHAUX2SOURCETIMER\", \"FIXTURESHUTTERPOSITION\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_training_data =[ 0.21683119,  0.32121513,  0.31925213,  0.20097501,  0.45164471,  0.22914814,\n",
    "  0.11604865,  0.27421592,  0.24393222, -0.13974937, -0.09739598, -0.07313758,\n",
    "  0.18198089]\n",
    "var_training_data =[0.75261122, 0.90482986, 0.91105327, 0.75504036, 1.07026701, 0.76708319,\n",
    " 0.35172769, 0.83004988, 0.76964675, 0.57386915, 0.45912309, 0.2955709,\n",
    " 1.61493449]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mean and Variance from artifical Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_training_data= [-0.00393712, -0.01294209]\n",
    "var_training_data = [49.18936568,  0.34270256]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mean and Variance form cpps Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_training_data = [0.05330162, 0.03075699, -0.05636937, 0.0274802, 0.06536314, -0.04620979,-0.0745559, \n",
    "                      -0.08149049, -0.05318843, 0.11105582]\n",
    "var_training_data = [0.02905961, 0.04473883, 0.05254194, 0.05198144, 0.07337494, 0.0666981, 0.07593811, \n",
    "                     0.0393896, 0.08028017, 0.0594492]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(12000, 3)\n"
     ]
    }
   ],
   "source": [
    "data_preperator = DataPreperatorPrediction(path=param['data']['path'], \n",
    "                                           ignored_features = param[\"data\"][\"droped_feature\"],\n",
    "                                           mean_training_data=mean_training_data, \n",
    "                                           var_training_data=var_training_data, \n",
    "                                           first_order_difference=False \n",
    "                                          )                                  \n",
    "preprocessed_data = data_preperator.prepare_data()\n",
    "print(preprocessed_data.shape)\n",
    "\n",
    "dataset = DataSet(preprocessed_data, \n",
    "                  timesteps=param[\"model\"][\"sequence_size\"])\n",
    "data_loader = DataLoader(dataset, \n",
    "                         batch_size=param['model']['batch_size'], \n",
    "                         num_workers=0, \n",
    "                         shuffle=False, \n",
    "                         drop_last=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data of batch: 0\n",
      "Size of input data: torch.Size([50, 100, 3])\n",
      "Size of target data: torch.Size([50, 3])\n",
      "Data of batch: 1\n",
      "Size of input data: torch.Size([50, 100, 3])\n",
      "Size of target data: torch.Size([50, 3])\n"
     ]
    }
   ],
   "source": [
    "for batch_idx, data in enumerate(data_loader):\n",
    "    x,y = data\n",
    "    print('Data of batch: {}'.format(batch_idx))\n",
    "    print(\"Size of input data: {}\".format(x.size()))\n",
    "    print(\"Size of target data: {}\".format(y.size()))\n",
    "    if batch_idx >=1: break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define Model and load Parameters of trained model\n",
    "### Model for MSE and Latent Space Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = LstmMse_LatentSpace(batch_size=param['model']['batch_size'], \n",
    "                            input_dim=param['model']['input_size'], \n",
    "                            n_hidden_lstm=param['model']['n_hidden_lstm'], \n",
    "                            n_layers=param['model']['lstm_layer'],\n",
    "                            dropout_rate_lstm= param['model']['dropout_rate_lstm'],\n",
    "                            dropout_rate_fc= param['model']['dropout_rate_fc'],\n",
    "                            n_hidden_fc_prediction=param['model']['n_hidden_fc_pred'], \n",
    "                            n_hidden_fc_ls_analysis=param['model']['n_hidden_fc_ls']      \n",
    "                            )\n",
    "\n",
    "checkpoint = torch.load(param[\"model\"][\"path\"])\n",
    "model.load_state_dict(checkpoint['model_state_dict'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model for MLE and Latent Space Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "IncompatibleKeys(missing_keys=[], unexpected_keys=[])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = LstmMle_LatentSpace(batch_size=param['model']['batch_size'], \n",
    "                            input_dim=param['model']['input_size'], \n",
    "                            n_hidden_lstm=param['model']['n_hidden_lstm'], \n",
    "                            n_layers=param['model']['lstm_layer'],\n",
    "                            dropout_rate_lstm= param['model']['dropout_rate_lstm'],\n",
    "                            dropout_rate_fc= param['model']['dropout_rate_fc'],\n",
    "                            n_hidden_fc_prediction=param['model']['n_hidden_fc_pred'], \n",
    "                            n_hidden_fc_ls_analysis=param['model']['n_hidden_fc_ls'],\n",
    "                            K = param['model']['K']\n",
    "                            )\n",
    "\n",
    "checkpoint = torch.load(param[\"model\"][\"path\"])\n",
    "model.load_state_dict(checkpoint['model_state_dict'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model for MLE and Cell State"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "IncompatibleKeys(missing_keys=[], unexpected_keys=[])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = LstmMleCellState(batch_size=param['model']['batch_size'], \n",
    "                         input_dim=param['model']['input_size'], \n",
    "                         n_hidden_lstm=param['model']['n_hidden_lstm'], \n",
    "                         n_layers=param['model']['lstm_layer'],\n",
    "                         dropout_rate_lstm= param['model']['dropout_rate_lstm'],\n",
    "                         dropout_rate_fc= param['model']['dropout_rate_fc'],\n",
    "                         n_hidden_fc=param['model']['n_hidden_fc_1'],\n",
    "                         K = 1\n",
    "                         )\n",
    "\n",
    "checkpoint = torch.load(param[\"model\"][\"path\"])\n",
    "model.load_state_dict(checkpoint['model_state_dict'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialize Predictor\n",
    "### Predictor for MSE Model and Latent Space Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictor = PredictorMseLatentSpaceAnalyser(model=model,\n",
    "                                            path_data=param[\"data\"][\"path\"],\n",
    "                                            columns_to_ignore=param[\"data\"][\"droped_feature\"],\n",
    "                                            threshold_anomaly=param[\"anomaly_detection\"][\"threshold_anomaly\"]\n",
    "                                            )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Predictor for MLE Model and Latent Space Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictor = PredictorMleLatentSpaceAnalyser(model=model,\n",
    "                                            path_data=param[\"data\"][\"path\"],\n",
    "                                            columns_to_ignore=param[\"data\"][\"droped_feature\"],\n",
    "                                            threshold_anomaly=param[\"anomaly_detection\"][\"threshold_anomaly\"],\n",
    "                                            no_standard_deviation=param[\"anomaly_detection\"][\"no_standard_deviation\"]\n",
    "                                            )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Predictor for MLE Model and Cell State"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictor = PredictorMleCellState(model=model,\n",
    "                                 path_data=param[\"data\"][\"path\"],\n",
    "                                 columns_to_ignore=param[\"data\"][\"droped_feature\"],\n",
    "                                 no_standard_deviation=param[\"anomaly_detection\"][\"no_standard_deviation\"]\n",
    "                                 )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start predicting.\n",
      "Current status: 0 samples are predicted.\n",
      "Current status: 5000 samples are predicted.\n",
      "Current status: 10000 samples are predicted.\n",
      "End of prediction.\n"
     ]
    }
   ],
   "source": [
    "print(\"Start predicting.\")\n",
    "# Write header\n",
    "with open(param[\"results\"][\"path\"], \"a+\") as file:\n",
    "            [file.write(column+\";\") for column in predictor.create_column_names_result()]\n",
    "            file.write(\"\\n\")\n",
    "\n",
    "for batch_number, (input_data, target_data) in enumerate(data_loader):\n",
    "    # Predict sensor values in mini-batches\n",
    "    batch_results = predictor.predict(input_data, target_data)\n",
    "    \n",
    "    # Write results to csv file\n",
    "    with open(param[\"results\"][\"path\"], \"a\") as file:\n",
    "        for batch in batch_results:\n",
    "            # Each result component of a singe prediction (ID, target, prediction, loss, latent space ...) is stored in lists\n",
    "            # thus we have to unpack the list and seperate values with ;\n",
    "            for value in batch:\n",
    "                file.write(str(value)+\";\")\n",
    "            file.write(\"\\n\")\n",
    "\n",
    "    # Print status \n",
    "    if (batch_number*param['model']['batch_size'])%5000 == 0:\n",
    "        print(\"Current status: \" + str(param['model']['batch_size']*batch_number) + \" samples are predicted.\")\n",
    "\n",
    "print(\"End of prediction.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tag anomalous samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_prediction = pd.read_csv(param[\"results\"][\"path\"], sep=\";\")\n",
    "# Values of column \"loss\" are exponentially smoothed and stored in a new column \"smoothed loss\"\n",
    "# New column \"anomaly\" is created and sample is taged with 1 if anomalous behaviour (if smoothed loss is over threshold)\n",
    "results = predictor.detect_anomaly(results_prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>sine_signal target</th>\n",
       "      <th>sawtooth_signal target</th>\n",
       "      <th>sine_signal mu predicted</th>\n",
       "      <th>sawtooth_signal mu predicted</th>\n",
       "      <th>sine_signal sigma predicted</th>\n",
       "      <th>sawtooth_signal sigma predicted</th>\n",
       "      <th>mean normalised residual</th>\n",
       "      <th>sine_signal normalised residual</th>\n",
       "      <th>sawtooth_signal normalised residual</th>\n",
       "      <th>...</th>\n",
       "      <th>cell_state_7</th>\n",
       "      <th>cell_state_8</th>\n",
       "      <th>cell_state_9</th>\n",
       "      <th>cell_state_10</th>\n",
       "      <th>cell_state_11</th>\n",
       "      <th>cell_state_12</th>\n",
       "      <th>cell_state_13</th>\n",
       "      <th>cell_state_14</th>\n",
       "      <th>Anomaly Sensor_1</th>\n",
       "      <th>Anomaly Sensor_2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>100.0</td>\n",
       "      <td>0.093997</td>\n",
       "      <td>-1.728080</td>\n",
       "      <td>0.036548</td>\n",
       "      <td>0.396214</td>\n",
       "      <td>0.123970</td>\n",
       "      <td>1.176557</td>\n",
       "      <td>-0.671052</td>\n",
       "      <td>0.463412</td>\n",
       "      <td>-1.805517</td>\n",
       "      <td>...</td>\n",
       "      <td>0.715540</td>\n",
       "      <td>-1.919262</td>\n",
       "      <td>-0.404148</td>\n",
       "      <td>0.432210</td>\n",
       "      <td>0.337220</td>\n",
       "      <td>0.225963</td>\n",
       "      <td>-0.503775</td>\n",
       "      <td>-0.410294</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>101.0</td>\n",
       "      <td>0.214054</td>\n",
       "      <td>-1.268459</td>\n",
       "      <td>0.150598</td>\n",
       "      <td>-0.761678</td>\n",
       "      <td>0.116649</td>\n",
       "      <td>0.733722</td>\n",
       "      <td>-0.073355</td>\n",
       "      <td>0.543990</td>\n",
       "      <td>-0.690699</td>\n",
       "      <td>...</td>\n",
       "      <td>0.778251</td>\n",
       "      <td>-1.549377</td>\n",
       "      <td>-0.026401</td>\n",
       "      <td>0.282278</td>\n",
       "      <td>0.259377</td>\n",
       "      <td>0.482939</td>\n",
       "      <td>0.344591</td>\n",
       "      <td>-0.021480</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>102.0</td>\n",
       "      <td>0.350936</td>\n",
       "      <td>-1.853176</td>\n",
       "      <td>0.345036</td>\n",
       "      <td>-0.997262</td>\n",
       "      <td>0.117255</td>\n",
       "      <td>0.610446</td>\n",
       "      <td>-0.675896</td>\n",
       "      <td>0.050320</td>\n",
       "      <td>-1.402112</td>\n",
       "      <td>...</td>\n",
       "      <td>0.811790</td>\n",
       "      <td>-1.339358</td>\n",
       "      <td>0.351927</td>\n",
       "      <td>0.040743</td>\n",
       "      <td>0.251096</td>\n",
       "      <td>0.594160</td>\n",
       "      <td>0.803193</td>\n",
       "      <td>0.183734</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>103.0</td>\n",
       "      <td>0.564910</td>\n",
       "      <td>-0.301851</td>\n",
       "      <td>0.534023</td>\n",
       "      <td>-0.868312</td>\n",
       "      <td>0.119356</td>\n",
       "      <td>0.517821</td>\n",
       "      <td>0.676358</td>\n",
       "      <td>0.258784</td>\n",
       "      <td>1.093932</td>\n",
       "      <td>...</td>\n",
       "      <td>0.895934</td>\n",
       "      <td>-1.127712</td>\n",
       "      <td>0.654661</td>\n",
       "      <td>-0.163743</td>\n",
       "      <td>0.239933</td>\n",
       "      <td>0.648258</td>\n",
       "      <td>1.225374</td>\n",
       "      <td>0.366916</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>104.0</td>\n",
       "      <td>0.693297</td>\n",
       "      <td>-1.056769</td>\n",
       "      <td>0.718185</td>\n",
       "      <td>-0.807713</td>\n",
       "      <td>0.120772</td>\n",
       "      <td>0.509496</td>\n",
       "      <td>-0.347452</td>\n",
       "      <td>-0.206076</td>\n",
       "      <td>-0.488828</td>\n",
       "      <td>...</td>\n",
       "      <td>0.871465</td>\n",
       "      <td>-1.091353</td>\n",
       "      <td>0.734642</td>\n",
       "      <td>-0.373567</td>\n",
       "      <td>0.330165</td>\n",
       "      <td>0.569696</td>\n",
       "      <td>1.187358</td>\n",
       "      <td>0.218975</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 27 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      ID  sine_signal target  sawtooth_signal target  \\\n",
       "0  100.0            0.093997               -1.728080   \n",
       "1  101.0            0.214054               -1.268459   \n",
       "2  102.0            0.350936               -1.853176   \n",
       "3  103.0            0.564910               -0.301851   \n",
       "4  104.0            0.693297               -1.056769   \n",
       "\n",
       "   sine_signal mu predicted  sawtooth_signal mu predicted  \\\n",
       "0                  0.036548                      0.396214   \n",
       "1                  0.150598                     -0.761678   \n",
       "2                  0.345036                     -0.997262   \n",
       "3                  0.534023                     -0.868312   \n",
       "4                  0.718185                     -0.807713   \n",
       "\n",
       "   sine_signal sigma predicted  sawtooth_signal sigma predicted  \\\n",
       "0                     0.123970                         1.176557   \n",
       "1                     0.116649                         0.733722   \n",
       "2                     0.117255                         0.610446   \n",
       "3                     0.119356                         0.517821   \n",
       "4                     0.120772                         0.509496   \n",
       "\n",
       "   mean normalised residual  sine_signal normalised residual  \\\n",
       "0                 -0.671052                         0.463412   \n",
       "1                 -0.073355                         0.543990   \n",
       "2                 -0.675896                         0.050320   \n",
       "3                  0.676358                         0.258784   \n",
       "4                 -0.347452                        -0.206076   \n",
       "\n",
       "   sawtooth_signal normalised residual  ...  cell_state_7  cell_state_8  \\\n",
       "0                            -1.805517  ...      0.715540     -1.919262   \n",
       "1                            -0.690699  ...      0.778251     -1.549377   \n",
       "2                            -1.402112  ...      0.811790     -1.339358   \n",
       "3                             1.093932  ...      0.895934     -1.127712   \n",
       "4                            -0.488828  ...      0.871465     -1.091353   \n",
       "\n",
       "   cell_state_9  cell_state_10  cell_state_11  cell_state_12  cell_state_13  \\\n",
       "0     -0.404148       0.432210       0.337220       0.225963      -0.503775   \n",
       "1     -0.026401       0.282278       0.259377       0.482939       0.344591   \n",
       "2      0.351927       0.040743       0.251096       0.594160       0.803193   \n",
       "3      0.654661      -0.163743       0.239933       0.648258       1.225374   \n",
       "4      0.734642      -0.373567       0.330165       0.569696       1.187358   \n",
       "\n",
       "   cell_state_14  Anomaly Sensor_1  Anomaly Sensor_2  \n",
       "0      -0.410294                 0                 0  \n",
       "1      -0.021480                 0                 0  \n",
       "2       0.183734                 0                 0  \n",
       "3       0.366916                 0                 0  \n",
       "4       0.218975                 0                 0  \n",
       "\n",
       "[5 rows x 27 columns]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Combine prediction data with data which was not consider for inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "original_sensor_data = pd.read_csv(param[\"data\"][\"path\"])\n",
    "data_of_droped_feature = original_sensor_data.loc[:, param[\"data\"][\"droped_feature\"]+[\"ID\"]]\n",
    "complete_data = results.merge(right=data_of_droped_feature, how=\"inner\", on=\"ID\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save data to csv file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "complete_data.to_csv(param[\"results\"][\"path\"], sep=\";\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "masterarbeit",
   "language": "python",
   "name": "masterarbeit"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
