{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader\n",
    "import torch.optim as optim\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# own Modules \n",
    "from models_sub_net_ls import LstmMse_LatentSpace, LstmMle_LatentSpace, AnalysisLayer\n",
    "from models_mse import LstmMse\n",
    "from models_mle import LstmMle_1\n",
    "from data_preperator import DataPreperatorPrediction\n",
    "from data_set import DataSet\n",
    "from predictor import PredictorMse, PredictorMle, PredictorMseLatentSpaceAnalyser, PredictorMleLatentSpaceAnalyser"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Take care of these things before training:\n",
    "- Select correct path and define droped_features\n",
    "- Change parameter of model\n",
    "- Change filed_location"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parameters phm data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "param = {\n",
    "    \"data\" : {\n",
    "        \"path\" : '../../data/phm_data_challenge/01_M01_DC_prediction_1.csv',\n",
    "        \"droped_feature\" : [\"stage\", \"Lot\", \"runnum\", \"recipe\", \"recipe_step\",\n",
    "                            \"up time\", \"ongoing time\", \n",
    "                            \"ETCHSOURCEUSAGE\", \"ETCHAUXSOURCETIMER\", \n",
    "                            \"ETCHAUX2SOURCETIMER\", \"FIXTURESHUTTERPOSITION\"\n",
    "                           ],\n",
    "        \"features_not_to_scale\": []\n",
    "    },\n",
    "    \"model\" : {\n",
    "        \"path\" : \"../../models/MLE_model/phm_dataFold xx_InputSize13_LayerLstm1_HiddenLstm15_HiddenFc75_Seq25.pt\",\n",
    "        \"input_size\" : 13,\n",
    "        \"n_hidden_lstm\" : 15,\n",
    "        \"sequence_size\" : 100,\n",
    "        \"batch_size\" : 50,\n",
    "        \"lstm_layer\" : 1,\n",
    "        \"n_hidden_fc_pred\": 75,\n",
    "        \"n_hidden_fc_ls\": 7,\n",
    "        \"dropout_rate_lstm\": 0.0,\n",
    "        \"dropout_rate_fc\": 0.2,\n",
    "        \"K\":1\n",
    "    },\n",
    "        \"anomaly_detection\" : {\n",
    "        \"threshold_anomaly\" : 0.3,\n",
    "        \"smooth_rate\" : 0.05,\n",
    "        \"no_standard_deviation\" : 2\n",
    "    },\n",
    "    \"results\": {\n",
    "        \"path\" : \"../visualisation/files/prediction/MLE/phm_mle_1.csv\",\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parameters artifical data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "param = {\n",
    "    \"data\" : {\n",
    "        \"path\" : '../../data/artifical_signals/MLE_analysis/artifical_2_signals_errors.csv',\n",
    "        \"droped_feature\" : [\"anomaly\"\n",
    "                           ],\n",
    "        \"features_not_to_scale\": []\n",
    "    },\n",
    "    \"model\" : {\n",
    "        \"path\" : \"../../models/MLE_latent_space/artifical_2_signals_InputSize2_LayerLstm1_HiddenLstm15_HiddenFc_pred75_HiddenFc_ls7_Seq25.pt\",\n",
    "        \"input_size\" : 2,\n",
    "        \"n_hidden_lstm\" : 15,\n",
    "        \"sequence_size\" : 100,\n",
    "        \"batch_size\" : 50,\n",
    "        \"lstm_layer\" : 1,\n",
    "        \"n_hidden_fc_pred\": 75,\n",
    "        \"n_hidden_fc_ls\": 7,\n",
    "        \"dropout_rate_lstm\": 0.0,\n",
    "        \"dropout_rate_fc\": 0.2,\n",
    "        \"K\":1\n",
    "    },\n",
    "        \"anomaly_detection\" : {\n",
    "        \"threshold_anomaly\" : 0.3,\n",
    "        \"smooth_rate\" : 0.05,\n",
    "        \"no_standard_deviation\" : 2\n",
    "    },\n",
    "    \"results\": {\n",
    "        \"path\" : \"../visualisation/files/prediction/MLE_LS/artifical_2_signals.csv\",\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parameters cpps data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "param = {\n",
    "    \"data\" : {\n",
    "        \"path\" : '../../data/cpps_data/cpps_data_predictive_maintenance.csv',\n",
    "        \"droped_feature\" : [\"status\"\n",
    "                           ],\n",
    "        \"features_not_to_scale\": []\n",
    "    },\n",
    "    \"model\" : {\n",
    "        \"path\" : \"../../models/MLE_latent_space/cpps_data_InputSize10_LayerLstm1_HiddenLstm15_HiddenFc_pred75_HiddenFc_ls7_Seq25.pt\",\n",
    "        \"input_size\" : 10,\n",
    "        \"n_hidden_lstm\" : 15,\n",
    "        \"sequence_size\" : 100,\n",
    "        \"batch_size\" : 50,\n",
    "        \"lstm_layer\" : 1,\n",
    "        \"n_hidden_fc_pred\": 75,\n",
    "        \"n_hidden_fc_ls\": 7,\n",
    "        \"dropout_rate_lstm\": 0.0,\n",
    "        \"dropout_rate_fc\": 0.2,\n",
    "        \"K\":1\n",
    "    },\n",
    "        \"anomaly_detection\" : {\n",
    "        \"threshold_anomaly\" : 0.3,\n",
    "        \"smooth_rate\" : 0.05,\n",
    "        \"no_standard_deviation\" : 2\n",
    "    },\n",
    "    \"results\": {\n",
    "        \"path\" : \"../visualisation/files/prediction/MLE_LS/cpps_data.csv\",\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Standarize Data\n",
    "First we have to apply normalisation to data. That is because the model works on the representation given by its input vectors. The scale of those numbers is part of the representation.\n",
    "We should apply the exact same scaling as for training data. That means storing the scale and offset used with your training data, and using that again. <br>\n",
    "__The mean and variance for each feature of the training data with which the model was trained (stake: 0.75):__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mean and Variance from phm Dataset\n",
    "droped features=\"stage\", \"Lot\", \"runnum\", \"recipe\", \"recipe_step\",\n",
    "                            \"up time\", \"ongoing time\", \n",
    "                            \"ETCHSOURCEUSAGE\", \"ETCHAUXSOURCETIMER\", \n",
    "                            \"ETCHAUX2SOURCETIMER\", \"FIXTURESHUTTERPOSITION\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_training_data =[ 0.21683119,  0.32121513,  0.31925213,  0.20097501,  0.45164471,  0.22914814,\n",
    "  0.11604865,  0.27421592,  0.24393222, -0.13974937, -0.09739598, -0.07313758,\n",
    "  0.18198089]\n",
    "var_training_data =[0.75261122, 0.90482986, 0.91105327, 0.75504036, 1.07026701, 0.76708319,\n",
    " 0.35172769, 0.83004988, 0.76964675, 0.57386915, 0.45912309, 0.2955709,\n",
    " 1.61493449]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mean and Variance from artifical Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_training_data= [-0.00393712, -0.01294209]\n",
    "var_training_data = [49.18936568,  0.34270256]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mean and Variance form cpps Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_training_data = [0.05330162, 0.03075699, -0.05636937, 0.0274802, 0.06536314, -0.04620979,-0.0745559, \n",
    "                      -0.08149049, -0.05318843, 0.11105582]\n",
    "var_training_data = [0.02905961, 0.04473883, 0.05254194, 0.05198144, 0.07337494, 0.0666981, 0.07593811, \n",
    "                     0.0393896, 0.08028017, 0.0594492]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(150000, 11)\n"
     ]
    }
   ],
   "source": [
    "data_preperator = DataPreperatorPrediction(path=param['data']['path'], \n",
    "                                           ignored_features = param[\"data\"][\"droped_feature\"],\n",
    "                                           mean_training_data=mean_training_data, \n",
    "                                           var_training_data=var_training_data, \n",
    "                                           first_order_difference=False \n",
    "                                          )                                  \n",
    "preprocessed_data = data_preperator.prepare_data()\n",
    "print(preprocessed_data.shape)\n",
    "\n",
    "dataset = DataSet(preprocessed_data, \n",
    "                  timesteps=param[\"model\"][\"sequence_size\"])\n",
    "data_loader = DataLoader(dataset, \n",
    "                         batch_size=param['model']['batch_size'], \n",
    "                         num_workers=0, \n",
    "                         shuffle=False, \n",
    "                         drop_last=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data of batch: 0\n",
      "Size of input data: torch.Size([50, 100, 11])\n",
      "Size of target data: torch.Size([50, 11])\n",
      "Data of batch: 1\n",
      "Size of input data: torch.Size([50, 100, 11])\n",
      "Size of target data: torch.Size([50, 11])\n"
     ]
    }
   ],
   "source": [
    "for batch_idx, data in enumerate(data_loader):\n",
    "    x,y = data\n",
    "    print('Data of batch: {}'.format(batch_idx))\n",
    "    print(\"Size of input data: {}\".format(x.size()))\n",
    "    print(\"Size of target data: {}\".format(y.size()))\n",
    "    if batch_idx >=1: break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define Model and load Parameters of trained model\n",
    "### Model for MSE and Latent Space Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = LstmMse_LatentSpace(batch_size=param['model']['batch_size'], \n",
    "                            input_dim=param['model']['input_size'], \n",
    "                            n_hidden_lstm=param['model']['n_hidden_lstm'], \n",
    "                            n_layers=param['model']['lstm_layer'],\n",
    "                            dropout_rate_lstm= param['model']['dropout_rate_lstm'],\n",
    "                            dropout_rate_fc= param['model']['dropout_rate_fc'],\n",
    "                            n_hidden_fc_prediction=param['model']['n_hidden_fc_pred'], \n",
    "                            n_hidden_fc_ls_analysis=param['model']['n_hidden_fc_ls']      \n",
    "                            )\n",
    "\n",
    "checkpoint = torch.load(param[\"model\"][\"path\"])\n",
    "model.load_state_dict(checkpoint['model_state_dict'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model for MLE and Latent Space Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "IncompatibleKeys(missing_keys=[], unexpected_keys=[])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = LstmMle_LatentSpace(batch_size=param['model']['batch_size'], \n",
    "                            input_dim=param['model']['input_size'], \n",
    "                            n_hidden_lstm=param['model']['n_hidden_lstm'], \n",
    "                            n_layers=param['model']['lstm_layer'],\n",
    "                            dropout_rate_lstm= param['model']['dropout_rate_lstm'],\n",
    "                            dropout_rate_fc= param['model']['dropout_rate_fc'],\n",
    "                            n_hidden_fc_prediction=param['model']['n_hidden_fc_pred'], \n",
    "                            n_hidden_fc_ls_analysis=param['model']['n_hidden_fc_ls'],\n",
    "                            K = param['model']['K']\n",
    "                            )\n",
    "\n",
    "checkpoint = torch.load(param[\"model\"][\"path\"])\n",
    "model.load_state_dict(checkpoint['model_state_dict'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialize Predictor\n",
    "### Predictor for MSE Model and Latent Space Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictor = PredictorMseLatentSpaceAnalyser(model=model,\n",
    "                                            path_data=param[\"data\"][\"path\"],\n",
    "                                            columns_to_ignore=param[\"data\"][\"droped_feature\"],\n",
    "                                            threshold_anomaly=param[\"anomaly_detection\"][\"threshold_anomaly\"]\n",
    "                                            )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Predictor for MLE Model and Latent Space Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictor = PredictorMleLatentSpaceAnalyser(model=model,\n",
    "                                            path_data=param[\"data\"][\"path\"],\n",
    "                                            columns_to_ignore=param[\"data\"][\"droped_feature\"],\n",
    "                                            threshold_anomaly=param[\"anomaly_detection\"][\"threshold_anomaly\"],\n",
    "                                            no_standard_deviation=param[\"anomaly_detection\"][\"no_standard_deviation\"]\n",
    "                                            )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start predicting.\n",
      "Current status: 0 samples are predicted.\n",
      "Current status: 5000 samples are predicted.\n",
      "Current status: 10000 samples are predicted.\n",
      "Current status: 15000 samples are predicted.\n",
      "Current status: 20000 samples are predicted.\n",
      "Current status: 25000 samples are predicted.\n",
      "Current status: 30000 samples are predicted.\n",
      "Current status: 35000 samples are predicted.\n",
      "Current status: 40000 samples are predicted.\n",
      "Current status: 45000 samples are predicted.\n",
      "Current status: 50000 samples are predicted.\n",
      "Current status: 55000 samples are predicted.\n",
      "Current status: 60000 samples are predicted.\n",
      "Current status: 65000 samples are predicted.\n",
      "Current status: 70000 samples are predicted.\n",
      "Current status: 75000 samples are predicted.\n",
      "Current status: 80000 samples are predicted.\n",
      "Current status: 85000 samples are predicted.\n",
      "Current status: 90000 samples are predicted.\n",
      "Current status: 95000 samples are predicted.\n",
      "Current status: 100000 samples are predicted.\n",
      "Current status: 105000 samples are predicted.\n",
      "Current status: 110000 samples are predicted.\n",
      "Current status: 115000 samples are predicted.\n",
      "Current status: 120000 samples are predicted.\n",
      "Current status: 125000 samples are predicted.\n",
      "Current status: 130000 samples are predicted.\n",
      "Current status: 135000 samples are predicted.\n",
      "Current status: 140000 samples are predicted.\n",
      "Current status: 145000 samples are predicted.\n",
      "End of prediction.\n"
     ]
    }
   ],
   "source": [
    "print(\"Start predicting.\")\n",
    "# Write header\n",
    "with open(param[\"results\"][\"path\"], \"a+\") as file:\n",
    "            [file.write(column+\";\") for column in predictor.create_column_names_result()]\n",
    "            file.write(\"\\n\")\n",
    "\n",
    "for batch_number, (input_data, target_data) in enumerate(data_loader):\n",
    "    # Predict sensor values in mini-batches\n",
    "    batch_results = predictor.predict(input_data, target_data)\n",
    "    \n",
    "    # Write results to csv file\n",
    "    with open(param[\"results\"][\"path\"], \"a\") as file:\n",
    "        for batch in batch_results:\n",
    "            # Each result component of a singe prediction (ID, target, prediction, loss, latent space ...) is stored in lists\n",
    "            # thus we have to unpack the list and seperate values with ;\n",
    "            for value in batch:\n",
    "                file.write(str(value)+\";\")\n",
    "            file.write(\"\\n\")\n",
    "\n",
    "    # Print status \n",
    "    if (batch_number*param['model']['batch_size'])%5000 == 0:\n",
    "        print(\"Current status: \" + str(param['model']['batch_size']*batch_number) + \" samples are predicted.\")\n",
    "\n",
    "print(\"End of prediction.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tag anomalous samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_prediction = pd.read_csv(param[\"results\"][\"path\"], sep=\";\")\n",
    "# Values of column \"loss\" are exponentially smoothed and stored in a new column \"smoothed loss\"\n",
    "# New column \"anomaly\" is created and sample is taged with 1 if anomalous behaviour (if smoothed loss is over threshold)\n",
    "results = predictor.detect_anomaly(results_prediction, param[\"anomaly_detection\"][\"smooth_rate\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>signal_1 target</th>\n",
       "      <th>signal_2 target</th>\n",
       "      <th>signal_3 target</th>\n",
       "      <th>signal_4 target</th>\n",
       "      <th>signal_5 target</th>\n",
       "      <th>signal_6 target</th>\n",
       "      <th>signal_7 target</th>\n",
       "      <th>signal_8 target</th>\n",
       "      <th>signal_9 target</th>\n",
       "      <th>...</th>\n",
       "      <th>Anomaly Sensor_1</th>\n",
       "      <th>Anomaly Sensor_2</th>\n",
       "      <th>Anomaly Sensor_3</th>\n",
       "      <th>Anomaly Sensor_4</th>\n",
       "      <th>Anomaly Sensor_5</th>\n",
       "      <th>Anomaly Sensor_6</th>\n",
       "      <th>Anomaly Sensor_7</th>\n",
       "      <th>Anomaly Sensor_8</th>\n",
       "      <th>Anomaly Sensor_9</th>\n",
       "      <th>Anomaly Sensor_10</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>101.0</td>\n",
       "      <td>-0.892629</td>\n",
       "      <td>-1.455301</td>\n",
       "      <td>-1.159004</td>\n",
       "      <td>0.665475</td>\n",
       "      <td>0.270881</td>\n",
       "      <td>-1.595616</td>\n",
       "      <td>0.630266</td>\n",
       "      <td>0.166484</td>\n",
       "      <td>0.557938</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>102.0</td>\n",
       "      <td>-0.915688</td>\n",
       "      <td>-1.418095</td>\n",
       "      <td>-1.213050</td>\n",
       "      <td>0.677133</td>\n",
       "      <td>0.190997</td>\n",
       "      <td>-1.575921</td>\n",
       "      <td>0.572661</td>\n",
       "      <td>0.126792</td>\n",
       "      <td>0.511696</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>103.0</td>\n",
       "      <td>-0.953605</td>\n",
       "      <td>-1.338766</td>\n",
       "      <td>-1.305150</td>\n",
       "      <td>0.696980</td>\n",
       "      <td>0.055676</td>\n",
       "      <td>-1.532497</td>\n",
       "      <td>0.464926</td>\n",
       "      <td>0.049242</td>\n",
       "      <td>0.428981</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>104.0</td>\n",
       "      <td>-1.002367</td>\n",
       "      <td>-1.228261</td>\n",
       "      <td>-1.438691</td>\n",
       "      <td>0.734270</td>\n",
       "      <td>-0.131096</td>\n",
       "      <td>-1.478106</td>\n",
       "      <td>0.318320</td>\n",
       "      <td>-0.056791</td>\n",
       "      <td>0.319786</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>105.0</td>\n",
       "      <td>-1.060904</td>\n",
       "      <td>-1.090143</td>\n",
       "      <td>-1.593820</td>\n",
       "      <td>0.774040</td>\n",
       "      <td>-0.351596</td>\n",
       "      <td>-1.406483</td>\n",
       "      <td>0.139286</td>\n",
       "      <td>-0.187528</td>\n",
       "      <td>0.186531</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 59 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      ID  signal_1 target  signal_2 target  signal_3 target  signal_4 target  \\\n",
       "0  101.0        -0.892629        -1.455301        -1.159004         0.665475   \n",
       "1  102.0        -0.915688        -1.418095        -1.213050         0.677133   \n",
       "2  103.0        -0.953605        -1.338766        -1.305150         0.696980   \n",
       "3  104.0        -1.002367        -1.228261        -1.438691         0.734270   \n",
       "4  105.0        -1.060904        -1.090143        -1.593820         0.774040   \n",
       "\n",
       "   signal_5 target  signal_6 target  signal_7 target  signal_8 target  \\\n",
       "0         0.270881        -1.595616         0.630266         0.166484   \n",
       "1         0.190997        -1.575921         0.572661         0.126792   \n",
       "2         0.055676        -1.532497         0.464926         0.049242   \n",
       "3        -0.131096        -1.478106         0.318320        -0.056791   \n",
       "4        -0.351596        -1.406483         0.139286        -0.187528   \n",
       "\n",
       "   signal_9 target  ...  Anomaly Sensor_1  Anomaly Sensor_2  Anomaly Sensor_3  \\\n",
       "0         0.557938  ...                 0                 0                 0   \n",
       "1         0.511696  ...                 0                 0                 0   \n",
       "2         0.428981  ...                 0                 0                 0   \n",
       "3         0.319786  ...                 0                 0                 0   \n",
       "4         0.186531  ...                 0                 0                 0   \n",
       "\n",
       "   Anomaly Sensor_4  Anomaly Sensor_5  Anomaly Sensor_6  Anomaly Sensor_7  \\\n",
       "0                 0                 0                 0                 0   \n",
       "1                 0                 0                 0                 0   \n",
       "2                 0                 0                 0                 0   \n",
       "3                 0                 0                 0                 0   \n",
       "4                 0                 0                 0                 0   \n",
       "\n",
       "   Anomaly Sensor_8  Anomaly Sensor_9  Anomaly Sensor_10  \n",
       "0                 0                 0                  0  \n",
       "1                 0                 0                  0  \n",
       "2                 0                 0                  0  \n",
       "3                 0                 0                  0  \n",
       "4                 0                 0                  0  \n",
       "\n",
       "[5 rows x 59 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Combine prediction data with data which was not consider for inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "original_sensor_data = pd.read_csv(param[\"data\"][\"path\"])\n",
    "data_of_droped_feature = original_sensor_data.loc[:, param[\"data\"][\"droped_feature\"]+[\"ID\"]]\n",
    "complete_data = results.merge(right=data_of_droped_feature, how=\"inner\", on=\"ID\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save data to csv file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "complete_data.to_csv(param[\"results\"][\"path\"], sep=\";\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "masterarbeit",
   "language": "python",
   "name": "masterarbeit"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
