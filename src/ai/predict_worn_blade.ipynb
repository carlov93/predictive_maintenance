{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.nn.utils import clip_grad_norm_\n",
    "from torch.autograd import Variable\n",
    "import pandas as pd\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import matplotlib.pyplot as plt\n",
    "import torch.optim as optim\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from keras.preprocessing.sequence import TimeseriesGenerator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "hyperparam = {\n",
    "    \"data\" : {\n",
    "        \"stake_training_data\" : 0.75, \n",
    "        \"path\" : '../../data/vega_shrinkwrapper_original/NewBlade/'\n",
    "    },\n",
    "    \"model\" : {\n",
    "        \"input_size\" : 7,\n",
    "        \"n_hidden\" : 150,\n",
    "        \"sequence_size\" : 50,\n",
    "        \"batch_size\" : 1,\n",
    "        \"lstm_layer\" : 3,\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predict worn blade data\n",
    "## Standarize Worn Data\n",
    "First we have to apply normalisation to data. That is because the model works on the representation given by its input vectors. The scale of those numbers is part of the representation.\n",
    "We should apply the exact same scaling as for training data. That means storing the scale and offset used with your training data, and using that again. <br>\n",
    "__The mean and variance for each feature of the training data with which the model was trained (stake: 0.75):__\n",
    "\n",
    "```python\n",
    "mean_training_data = [-5.37536613e-02, -2.53111489e-04, -8.82854465e+05, 7.79034183e+02,1.45531178e+04, 1.37766733e+03, 6.50149764e-01]\n",
    "variance_training_data = [1.25303578e-01, 1.16898690e-03, 2.86060835e+06, 1.64515717e+06, 6.85728371e+06, 3.63196175e+05, 8.21463343e-03]\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "stake_training_data = 0.75"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataPreperatorTraining():\n",
    "    def __init__(self, path):\n",
    "        self.path = path\n",
    "        \n",
    "    def load_data(self):\n",
    "        return pd.read_csv(self.path)\n",
    "    \n",
    "    def preprocess_data(self, train_data):\n",
    "        # Remove time feature\n",
    "        train_data = train_data.drop(labels=\"timestamp\", axis=1)\n",
    "        # Initialise standard scaler\n",
    "        scaler = StandardScaler()\n",
    "        scaler.fit(train_data)\n",
    "        # Transform data\n",
    "        train_scaled = scaler.transform(train_data)\n",
    "        return train_scaled\n",
    "        \n",
    "    def provide_data(self):\n",
    "        dataset = self.load_data()\n",
    "        train_preprocessed = self.preprocess_data(dataset)\n",
    "        return train_preprocessed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataPreperatorPrediction():\n",
    "    def __init__(self, path_training, path_worn_blade):\n",
    "        self.path_training = path_training\n",
    "        self.path_worn_blade = path_worn_blade\n",
    "        \n",
    "    def load_data(self, path):\n",
    "        return pd.read_csv(path)\n",
    "    \n",
    "    def preprocess_data(self):\n",
    "        # Load training and worn-blade-data and select  \n",
    "        dataset = self.load_data(self.path_training)\n",
    "        amount_training_data = round(len(dataset)*stake_training_data)\n",
    "        training_data = dataset.iloc[0:amount_training_data,:]\n",
    "        worn_blade_data = self.load_data(self.path_worn_blade)\n",
    "        \n",
    "        #Remove time feature\n",
    "        training_data = training_data.drop(labels=\"timestamp\", axis=1).values\n",
    "        worn_blade_data = worn_blade_data.drop(labels=\"timestamp\", axis=1).values\n",
    "        \n",
    "        # Initialise standard scaler\n",
    "        scaler = StandardScaler()\n",
    "        scaler.fit(training_data)\n",
    "        \n",
    "        # Transform data for prediction with mean and variance of training data\n",
    "        preprocessed_data = scaler.transform(worn_blade_data)\n",
    "        return preprocessed_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load and scale training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = DataPreperatorTraining(path=hyperparam['data']['path']+'NewBlade001.csv')\n",
    "preprocessed_new_blade = train_loader.provide_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load worn blade dataset and scale them with mean and variance of training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_training = '../../data/vega_shrinkwrapper_original/NewBlade/NewBlade001.csv'\n",
    "path_worn_blade = '../../data/vega_shrinkwrapper_original/WornBlade/WornBlade001.csv'\n",
    "\n",
    "preprocessed_worn_blade = DataPreperatorPrediction(path_training = path_training, path_worn_blade = path_worn_blade).preprocess_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataSet(Dataset):\n",
    "    def __init__(self, data, timesteps):\n",
    "        # All data are loaded from csv file and converted to an numpy array\n",
    "        self.data = data\n",
    "        # Data generator is initialized, batch_size=1 is indipendent of neural network's batch_size \n",
    "        self.generator = TimeseriesGenerator(self.data, self.data, length=timesteps, batch_size=1)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        x, y = self.generator[index]\n",
    "        x_torch = torch.from_numpy(x)\n",
    "        # Dimension 0 with size 1 (created by TimeseriesGenerator because of batch_size=1) gets removed \n",
    "        # because DataLoader will add a dimension 0 with size=batch_size as well\n",
    "        x_torch = torch.squeeze(x_torch) # torch.Size([1, timesteps, 7]) --> torch.Size([timesteps, 7])\n",
    "        y_torch = torch.from_numpy(y)\n",
    "        return (x_torch.float(), y_torch.float()) \n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.generator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_worn_blade = DataSet(preprocessed_worn_blade, timesteps=50)\n",
    "dataset_new_blade = DataSet(preprocessed_new_blade, timesteps=50)\n",
    "\n",
    "data_loader_worn_blade = DataLoader(dataset_worn_blade, batch_size=1, num_workers=1, shuffle=False, drop_last=True)\n",
    "data_loader_new_blade = DataLoader(dataset_new_blade, batch_size=1, num_workers=1, shuffle=False, drop_last=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTM(nn.Module):\n",
    "    def __init__(self, batch_size, input_dim, n_hidden, n_layers):\n",
    "        super(LSTM, self).__init__()\n",
    "        # Attributes for LSTM Network\n",
    "        self.input_dim = input_dim\n",
    "        self.n_hidden = n_hidden\n",
    "        self.n_layers = n_layers\n",
    "        self.batch_size = batch_size\n",
    "        \n",
    "        # Definition of NN layer\n",
    "        # batch_first = True because dataloader creates batches and batch_size is 0. dimension\n",
    "        self.lstm = nn.LSTM(input_size = self.input_dim, hidden_size = self.n_hidden, num_layers = self.n_layers, batch_first = True)\n",
    "        self.fc_y_hat = nn.Linear(self.n_hidden, self.input_dim)\n",
    "        \n",
    "    def forward(self, input_data, hidden):\n",
    "        # Forward propagate LSTM\n",
    "        # LSTM in Pytorch return two results the first one usually called output and the second one (hidden_state, cell_state). \n",
    "        # As output the LSTM returns all the hidden_states for all the timesteps (seq), in other words all of the hidden states throughout\n",
    "        # the sequence\n",
    "        # As hidden_state the LSTM returns just the most recent hidden state\n",
    "        lstm_out, (hidden_state, cell_state) = self.lstm(input_data, hidden)\n",
    "        # Length of input data can varry \n",
    "        length_seq = input_data.size()[1]\n",
    "        # Select the output from the last sequence \n",
    "        last_out = lstm_out[:,length_seq-1,:]\n",
    "        out_y_hat = self.fc_y_hat(last_out)\n",
    "        return out_y_hat\n",
    "    \n",
    "    def init_hidden(self):\n",
    "        # This method is for initializing hidden state as well as cell state\n",
    "        # We need to detach the hidden state to prevent exploding/vanishing gradients\n",
    "        # The axes semantics are (num_layers, minibatch_size, hidden_dim)\n",
    "        h0 = torch.zeros(self.n_layers, self.batch_size, self.n_hidden, requires_grad=False)\n",
    "        c0 = torch.zeros(self.n_layers, self.batch_size, self.n_hidden, requires_grad=False)\n",
    "        return [t for t in (h0, c0)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predict sensor data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start predicting\n",
      "Finished\n"
     ]
    }
   ],
   "source": [
    "# Initiate and load model\n",
    "model = LSTM(batch_size=hyperparam['model']['batch_size'], input_dim=hyperparam['model']['input_size'], \n",
    "             n_hidden=hyperparam['model']['n_hidden'], n_layers=hyperparam['model']['lstm_layer'])\n",
    "\n",
    "PATH = \"../../models/MSE_model/best_model_aws.pt\"\n",
    "checkpoint = torch.load(PATH)\n",
    "model.load_state_dict(checkpoint['model_state_dict'])\n",
    "model.eval()\n",
    "\n",
    "# Create empty dataframe\n",
    "columns = [\"timestamp\", \"cut_torque_target\", \"cut_lag error_target\", \"cut_position_target\", \"cut speed_target\", \n",
    "           \"film_position_target\", \"film_speed_target\", \"film_lag_error_target\", \"cut_torque_predicted\", \n",
    "           \"cut_lag error_predicted\", \"cut_position_predicted\", \"cut speed_predicted\", \"film_position_predicted\", \n",
    "           \"film_speed_predicted\", \"film_lag_error_predicted\"]\n",
    "df = pd.DataFrame(columns=columns)\n",
    "\n",
    "print(\"Start predicting\")    \n",
    "##### Predict #####\n",
    "for batch_number, data in enumerate(data_loader_new_blade):\n",
    "    \n",
    "    input_data, target_data = data\n",
    "    hidden = model.init_hidden()\n",
    "\n",
    "    # Forward propagation\n",
    "    output = model(input_data, hidden)\n",
    "    \n",
    "    # Add values to dataframe \n",
    "    output = torch.squeeze(output)\n",
    "    target_data = torch.squeeze(target_data)\n",
    "    target_data_np = target_data.data.numpy().tolist()\n",
    "    predicted_data_np = output.data.numpy().tolist()\n",
    "    data = [batch_number] + target_data_np + predicted_data_np\n",
    "    df = df.append(pd.Series(data, index=df.columns ), ignore_index=True)\n",
    "\n",
    "# Save dataframe as csv file\n",
    "df.to_csv(\"../visualisation/files/prediction_training_data.csv\", sep=\";\", index=False)\n",
    "\n",
    "print(\"Finished\")  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1998, 15)"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "masterarbeit",
   "language": "python",
   "name": "masterarbeit"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
