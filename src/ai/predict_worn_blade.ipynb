{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.nn.utils import clip_grad_norm_\n",
    "from torch.autograd import Variable\n",
    "from torchsummary import summary\n",
    "import pandas as pd\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import matplotlib.pyplot as plt\n",
    "import torch.optim as optim\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from keras.preprocessing.sequence import TimeseriesGenerator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predict worn blade data\n",
    "## Standarize Worn Data\n",
    "First we have to apply normalisation to data. That is because the model works on the representation given by its input vectors. The scale of those numbers is part of the representation.\n",
    "We should apply the exact same scaling as for training data. That means storing the scale and offset used with your training data, and using that again. <br>\n",
    "__The mean and variance for each feature of the training data with which the model was trained (stake: 0.75):__\n",
    "\n",
    "```python\n",
    "mean_training_data = [-5.37536613e-02, -2.53111489e-04, -8.82854465e+05, 7.79034183e+02,1.45531178e+04, 1.37766733e+03, 6.50149764e-01]\n",
    "variance_training_data = [1.25303578e-01, 1.16898690e-03, 2.86060835e+06, 1.64515717e+06, 6.85728371e+06, 3.63196175e+05, 8.21463343e-03]\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "stake_training_data = 0.75"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataPreperatorPrediction():\n",
    "    def __init__(self, path_training, path_worn_blade):\n",
    "        self.path_training = path_training\n",
    "        self.path_worn_blade = path_worn_blade\n",
    "        \n",
    "    def load_data(self, path):\n",
    "        return pd.read_csv(path)\n",
    "    \n",
    "    def preprocess_data(self):\n",
    "        # Load training and worn-blade-data and select  \n",
    "        dataset = self.load_data(self.path_training)\n",
    "        amount_training_data = round(len(dataset)*stake_training_data)\n",
    "        training_data = dataset.iloc[0:amount_training_data,:]\n",
    "        worn_blade_data = self.load_data(self.path_worn_blade)\n",
    "        \n",
    "        #Remove time feature\n",
    "        training_data = training_data.drop(labels=\"Timestamp\", axis=1).values\n",
    "        worn_blade_data = worn_blade_data.drop(labels=\"Timestamp\", axis=1).values\n",
    "        \n",
    "        # Initialise standard scaler\n",
    "        scaler = StandardScaler()\n",
    "        scaler.fit(training_data)\n",
    "        \n",
    "        # Transform data for prediction with mean and variance of training data\n",
    "        preprocessed_data = scaler.transform(worn_blade_data)\n",
    "        return preprocessed_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_training = '../../data/vega_shrinkwrapper_original/NewBlade001.csv'\n",
    "path_worn_blade = '../../data/vega_shrinkwrapper_original/WornBlade001.csv'\n",
    "\n",
    "preprocessed_data = DataPreperatorPrediction(path_training = path_training, path_worn_blade = path_worn_blade).preprocess_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataSet(Dataset):\n",
    "    def __init__(self, data, timesteps):\n",
    "        # All data are loaded from csv file and converted to an numpy array\n",
    "        self.data = data\n",
    "        # Data generator is initialized, batch_size=1 is indipendent of neural network's batch_size \n",
    "        self.generator = TimeseriesGenerator(self.data, self.data, length=timesteps, batch_size=1)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        x, y = self.generator[index]\n",
    "        x_torch = torch.from_numpy(x)\n",
    "        # Dimension 0 with size 1 (created by TimeseriesGenerator because of batch_size=1) gets removed \n",
    "        # because DataLoader will add a dimension 0 with size=batch_size as well\n",
    "        x_torch = torch.squeeze(x_torch) # torch.Size([1, timesteps, 7]) --> torch.Size([timesteps, 7])\n",
    "        y_torch = torch.from_numpy(y)\n",
    "        return (x_torch.float(), y_torch.float()) \n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.generator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_worn_blade = DataSet(preprocessed_data, timesteps=20)\n",
    "\n",
    "data_loader_worn_blade = DataLoader(dataset_worn_blade, batch_size=8, num_workers=1, shuffle=False, drop_last=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "masterarbeit",
   "language": "python",
   "name": "masterarbeit"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
