{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Find base_lr and max_lr\n",
    "Calculate the upper bound of the learning rate for your model. The way to do this is to:\n",
    "1. Define an initial learning rate, the lower boundary of the range you want to test (let’s say 1e-7)\n",
    "2. Define an upper boundary of the range (let’s say 0.1)\n",
    "3. Define an exponential scheme to run through this step by step: <br>\n",
    "```python\n",
    "lr_lambda = lambda x: math.exp(x * math.log(end_lr / start_lr) / (lr_find_epochs * len( data_loader[\"train\"])))\n",
    "scheduler = torch.optim.lr_scheduler.LambdaLR(optimizer, lr_lambda)\n",
    "```\n",
    "<br>\n",
    "Note: Youdon’t take the ‘raw’ loss at each step, but the smoothed loss, being: loss = α . loss + (1- α). previous_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.nn.utils import clip_grad_norm_\n",
    "from torch.autograd import Variable\n",
    "from torchsummary import summary\n",
    "import pandas as pd\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import matplotlib.pyplot as plt\n",
    "import torch.optim as optim\n",
    "import math\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from keras.preprocessing.sequence import TimeseriesGenerator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "hyperparam = {\n",
    "    \"data\" : {\n",
    "        \"stake_training_data\" : 0.9, \n",
    "        \"path\" : '../../data/vega_shrinkwrapper_standardized/'\n",
    "    },\n",
    "    \"model\" : {\n",
    "        \"input_size\" : 7,\n",
    "        \"n_hidden\" : 150,\n",
    "        \"sequence_size\" : 20,\n",
    "        \"batch_size\" : 8,\n",
    "        \"lstm_layer\" : 3,\n",
    "    },\n",
    "    \"cycling_lr\" : {\n",
    "        # step_size is the number of training iterations (total samples/batch_size) per half cycle. \n",
    "        # Authors suggest setting step_size 2-8 x training iterations in epoch.\n",
    "        \"step_size\" : (2048/8)*4, \n",
    "        # Mode can be one of {triangular, triangular2, exp_range}\n",
    "        \"mode\" : \"triangular2\", \n",
    "        \"gamma\" : 0.9995,\n",
    "        \"base_lr\" : 3e-3, \n",
    "        \"max_lr\" :5e-3\n",
    "    },\n",
    "    \"training\": {\n",
    "        \"n_epochs\" : 10,\n",
    "        \"patience\" : 50,\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Split train and test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataSplitter():\n",
    "    def __init__(self, path):\n",
    "        self.path = path\n",
    "        \n",
    "    def load_data(self):\n",
    "        return pd.read_csv(self.path)\n",
    "        \n",
    "    def split_data(self, stake_training_data):\n",
    "        dataset = self.load_data()\n",
    "        amount_training_data = round(len(dataset)*stake_training_data)\n",
    "        train_data = dataset.iloc[0:amount_training_data,:]\n",
    "        validation_data = dataset.iloc[amount_training_data:,:]\n",
    "        return train_data, validation_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = DataSplitter(path=hyperparam['data']['path']+'NewBlade001.csv')\n",
    "train_data, validation_data = train_loader.split_data(stake_training_data=hyperparam['data']['stake_training_data'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataProvider(Dataset):\n",
    "    def __init__(self, data, timesteps):\n",
    "        # All data are loaded from csv file and converted to an numpy array\n",
    "        self.data = data.values\n",
    "        # Data generator is initialized, batch_size=1 is indipendent of neural network's batch_size \n",
    "        self.generator = TimeseriesGenerator(self.data, self.data, length=timesteps, batch_size=1)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        x, y = self.generator[index]\n",
    "        x_torch = torch.from_numpy(x)\n",
    "        # Dimension 0 with size 1 (created by TimeseriesGenerator because of batch_size=1) gets removed \n",
    "        # because DataLoader will add a dimension 0 with size=batch_size as well\n",
    "        x_torch = torch.squeeze(x_torch) # torch.Size([1, timesteps, 7]) --> torch.Size([timesteps, 7])\n",
    "        y_torch = torch.from_numpy(y)\n",
    "        return (x_torch.float(), y_torch.float()) \n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.generator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_train = DataProvider(train_data, timesteps=hyperparam[\"model\"][\"sequence_size\"])\n",
    "\n",
    "data_loader_training = DataLoader(dataset_train, batch_size=hyperparam[\"model\"][\"batch_size\"], \n",
    "                                  num_workers=1, shuffle=False, drop_last=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTM(nn.Module):\n",
    "    def __init__(self, batch_size, input_dim, n_hidden, n_layers):\n",
    "        super(LSTM, self).__init__()\n",
    "        # Attributes for LSTM Network\n",
    "        self.input_dim = input_dim\n",
    "        self.n_hidden = n_hidden\n",
    "        self.n_layers = n_layers\n",
    "        self.batch_size = batch_size\n",
    "        \n",
    "        # Attribut for Gaussians\n",
    "        self.n_gaus_param = 2\n",
    "        \n",
    "        # Definition of NN layer\n",
    "        # batch_first = True because dataloader creates batches and batch_size is 0. dimension\n",
    "        self.lstm = nn.LSTM(input_size = self.input_dim, hidden_size = self.n_hidden, num_layers = self.n_layers, batch_first = True)\n",
    "        self.fc1 = nn.Linear(self.n_hidden, self.n_gaus_param * self.input_dim)\n",
    "        \n",
    "    def forward(self, input_data):\n",
    "        # Forward propagate LSTM\n",
    "        # LSTM in Pytorch return two results the first one usually called output and the second one (hidden_state, cell_state). \n",
    "        # As output the LSTM returns all the hidden_states for all the timesteps (seq), in other words all of the hidden states throughout\n",
    "        # the sequence\n",
    "        # As hidden_state the LSTM returns just the most recent hidden state\n",
    "        lstm_out, (hidden_state, cell_state) = self.lstm(input_data)\n",
    "        # Length of input data can varry \n",
    "        length_seq = input_data.size()[1]\n",
    "        # Select the output from the last sequence \n",
    "        last_out = lstm_out[:,length_seq-1,:]\n",
    "        out = self.fc1(last_out)\n",
    "        #σ = exp(τ) guarantees σ > 0 and provides numerical stability in the learning process\n",
    "        return out\n",
    "    \n",
    "    def init_hidden(self):\n",
    "        # This method is for initializing hidden state as well as cell state\n",
    "        # We need to detach the hidden state to prevent exploding/vanishing gradients\n",
    "        # The axes semantics are (num_layers, minibatch_size, hidden_dim)\n",
    "        h0 = torch.zeros(self.n_layers, self.batch_size, self.n_hidden, requires_grad=False)\n",
    "        c0 = torch.zeros(self.n_layers, self.batch_size, self.n_hidden, requires_grad=False)\n",
    "        return [t for t in (h0, c0)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = LSTM(batch_size=hyperparam['model']['batch_size'], input_dim=hyperparam['model']['input_size'], \n",
    "             n_hidden=hyperparam['model']['n_hidden'], n_layers=hyperparam['model']['lstm_layer'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/carlovoss/jupyter_notebooks/masterarbeit/venv_pm/lib/python3.6/site-packages/torch/nn/modules/loss.py:443: UserWarning: Using a target size (torch.Size([8, 7])) that is different to the input size (torch.Size([8, 14])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "The size of tensor a (14) must match the size of tensor b (7) at non-singleton dimension 1",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-79-f75d4824a4e6>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     32\u001b[0m         \u001b[0mcriterion\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mMSELoss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m         \u001b[0mtarget_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtarget_data\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 34\u001b[0;31m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_data\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     35\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m         \u001b[0;31m# Backward pass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/jupyter_notebooks/masterarbeit/venv_pm/lib/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    491\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    492\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 493\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    494\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    495\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/jupyter_notebooks/masterarbeit/venv_pm/lib/python3.6/site-packages/torch/nn/modules/loss.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input, target)\u001b[0m\n\u001b[1;32m    441\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mweak_script_method\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    442\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 443\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmse_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreduction\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreduction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    444\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    445\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/jupyter_notebooks/masterarbeit/venv_pm/lib/python3.6/site-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36mmse_loss\u001b[0;34m(input, target, size_average, reduce, reduction)\u001b[0m\n\u001b[1;32m   2254\u001b[0m             \u001b[0mret\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mret\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mreduction\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'mean'\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mret\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2255\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2256\u001b[0;31m         \u001b[0mexpanded_input\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexpanded_target\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbroadcast_tensors\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2257\u001b[0m         \u001b[0mret\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_C\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_nn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmse_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mexpanded_input\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexpanded_target\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_Reduction\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_enum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreduction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2258\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mret\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/jupyter_notebooks/masterarbeit/venv_pm/lib/python3.6/site-packages/torch/functional.py\u001b[0m in \u001b[0;36mbroadcast_tensors\u001b[0;34m(*tensors)\u001b[0m\n\u001b[1;32m     60\u001b[0m                 [0, 1, 2]])\n\u001b[1;32m     61\u001b[0m     \"\"\"\n\u001b[0;32m---> 62\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_C\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_VariableFunctions\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbroadcast_tensors\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensors\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     63\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     64\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: The size of tensor a (14) must match the size of tensor b (7) at non-singleton dimension 1"
     ]
    }
   ],
   "source": [
    "# Experiment parameters\n",
    "lr_find_epochs = 1\n",
    "start_lr = 1e-7\n",
    "end_lr = 0.04\n",
    "\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=1.)  \n",
    "lr_lambda = lambda x: math.exp(x * math.log(end_lr / start_lr) / (lr_find_epochs * len(data_loader_training)))\n",
    "scheduler = torch.optim.lr_scheduler.LambdaLR(optimizer, lr_lambda)\n",
    "\n",
    "# Make lists to capture the logs\n",
    "\n",
    "lr_find_loss = []\n",
    "lr_find_lr = []\n",
    "\n",
    "iter = 0\n",
    "\n",
    "smoothing = 0.05\n",
    "\n",
    "for i in range(lr_find_epochs):\n",
    "    epoch_training_loss = []\n",
    "    print(\"epoch {}\".format(i))\n",
    "    for batch_number, (input_data, target_data) in enumerate(data_loader_training):\n",
    "    \n",
    "        # Training mode and zero gradients\n",
    "        model.train()\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Get outputs to calc loss\n",
    "        output = model(input_data)\n",
    "        \n",
    "        # Computate loss\n",
    "        criterion = nn.MSELoss()\n",
    "        target_data = torch.squeeze(target_data)\n",
    "        loss = criterion(output, target_data)\n",
    "\n",
    "        # Backward pass\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # Update LR\n",
    "        scheduler.step()\n",
    "        lr_step = optimizer.state_dict()[\"param_groups\"][0][\"lr\"]\n",
    "        lr_find_lr.append(lr_step)\n",
    "        print(lr_step)\n",
    "\n",
    "        # smooth the loss\n",
    "        if batch_number==0:\n",
    "          lr_find_loss.append(loss.item())\n",
    "        else:\n",
    "          loss = smoothing  * loss.item() + (1 - smoothing) * lr_find_loss[-1]\n",
    "          lr_find_loss.append(loss)\n",
    "\n",
    "        iter += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>x</th>\n",
       "      <th>y</th>\n",
       "      <th>log_value</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.000001</td>\n",
       "      <td>12.402070</td>\n",
       "      <td>-13.758686</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.000001</td>\n",
       "      <td>12.402589</td>\n",
       "      <td>-13.701861</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.000001</td>\n",
       "      <td>12.375348</td>\n",
       "      <td>-13.645036</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.000001</td>\n",
       "      <td>12.217886</td>\n",
       "      <td>-13.588212</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.000001</td>\n",
       "      <td>11.991036</td>\n",
       "      <td>-13.531387</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.000001</td>\n",
       "      <td>11.748523</td>\n",
       "      <td>-13.474562</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.000001</td>\n",
       "      <td>11.506554</td>\n",
       "      <td>-13.417737</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.000002</td>\n",
       "      <td>11.264870</td>\n",
       "      <td>-13.360913</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.000002</td>\n",
       "      <td>11.075808</td>\n",
       "      <td>-13.304088</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.000002</td>\n",
       "      <td>10.859153</td>\n",
       "      <td>-13.247263</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>0.000002</td>\n",
       "      <td>10.678584</td>\n",
       "      <td>-13.190438</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>0.000002</td>\n",
       "      <td>10.434924</td>\n",
       "      <td>-13.133613</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>0.000002</td>\n",
       "      <td>10.176138</td>\n",
       "      <td>-13.076789</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>0.000002</td>\n",
       "      <td>9.901517</td>\n",
       "      <td>-13.019964</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>0.000002</td>\n",
       "      <td>9.631032</td>\n",
       "      <td>-12.963139</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>0.000002</td>\n",
       "      <td>9.347387</td>\n",
       "      <td>-12.906314</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>0.000003</td>\n",
       "      <td>9.074659</td>\n",
       "      <td>-12.849490</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>0.000003</td>\n",
       "      <td>8.821922</td>\n",
       "      <td>-12.792665</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>0.000003</td>\n",
       "      <td>8.578814</td>\n",
       "      <td>-12.735840</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>0.000003</td>\n",
       "      <td>8.351185</td>\n",
       "      <td>-12.679015</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           x          y  log_value\n",
       "0   0.000001  12.402070 -13.758686\n",
       "1   0.000001  12.402589 -13.701861\n",
       "2   0.000001  12.375348 -13.645036\n",
       "3   0.000001  12.217886 -13.588212\n",
       "4   0.000001  11.991036 -13.531387\n",
       "5   0.000001  11.748523 -13.474562\n",
       "6   0.000001  11.506554 -13.417737\n",
       "7   0.000002  11.264870 -13.360913\n",
       "8   0.000002  11.075808 -13.304088\n",
       "9   0.000002  10.859153 -13.247263\n",
       "10  0.000002  10.678584 -13.190438\n",
       "11  0.000002  10.434924 -13.133613\n",
       "12  0.000002  10.176138 -13.076789\n",
       "13  0.000002   9.901517 -13.019964\n",
       "14  0.000002   9.631032 -12.963139\n",
       "15  0.000002   9.347387 -12.906314\n",
       "16  0.000003   9.074659 -12.849490\n",
       "17  0.000003   8.821922 -12.792665\n",
       "18  0.000003   8.578814 -12.735840\n",
       "19  0.000003   8.351185 -12.679015"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "data = pd.DataFrame(data={'iteration': range(len(lr_find_lr)), 'learning_rate': lr_find_lr, 'loss': lr_find_loss})\n",
    "data['log_value'] = np.log(data['learning_rate'])\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "f, ax = plt.subplots(figsize=(7, 7))\n",
    "plt.ylim(0, 0.1)\n",
    "plt.xlim(0, 3000)\n",
    "\n",
    "sns.scatterplot(x=data.iteration, y=data.learning_rate, ax=ax)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 504x504 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "f, ax = plt.subplots(figsize=(7, 7))\n",
    "ax.set(xscale=\"log\")\n",
    "sns.scatterplot(x=data.learning_rate, y=data.loss, ax=ax)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Example for possible outcome:__\n",
    "![](../../knowledge/pictures/learning_rate.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- A good upper bound (max_lr) is not on the lowest point, but about a factor of 10 to the left. (In this case 3e-3)\n",
    "- A good lower bound (base_lr), according to the paper and other sources, is the upper bound, divided by a factor 6."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "masterarbeit",
   "language": "python",
   "name": "masterarbeit"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
