{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn.utils import clip_grad_norm_\n",
    "from torch.utils.data import DataLoader\n",
    "import torch.optim as optim\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# own Modules \n",
    "from models import LstmMl\n",
    "from data_loader import DataPreperator, DataSet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "hyperparam = {\n",
    "    \"data\" : {\n",
    "        \"stake_training_data\" : 0.75, \n",
    "        \"path\" : '../../../data/vega_shrinkwrapper_original/NewBlade/'\n",
    "    },\n",
    "    \"model\" : {\n",
    "        \"input_size\" : 7,\n",
    "        \"n_hidden\" : 150,\n",
    "        \"sequence_size\" : 50,\n",
    "        \"batch_size\" : 8,\n",
    "        \"lstm_layer\" : 3,\n",
    "        \"gaussian\" : 1\n",
    "    },\n",
    "    \"cycling_lr\" : {\n",
    "        # step_size is the number of training iterations (total samples/batch_size) per half cycle. \n",
    "        # Authors suggest setting step_size 2-8 x training iterations in epoch.\n",
    "        \"step_size\" : (2048/8)*2, \n",
    "        # Mode can be one of {triangular, triangular2, exp_range}\n",
    "        \"mode\" : \"triangular\", \n",
    "        \"gamma\" : 0.9995,\n",
    "        \"base_lr\" : 0.016, \n",
    "        \"max_lr\" :0.1\n",
    "    },\n",
    "    \"training\": {\n",
    "        \"n_epochs\" : 100,\n",
    "        \"patience\" : 10,\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Split raw data into train and validation data and scale it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = DataPreperator(path=hyperparam['data']['path']+'NewBlade001.csv')\n",
    "train_data, validation_data = train_loader.provide_data(stake_training_data=hyperparam['data']['stake_training_data'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialize Dataset \n",
    "Time series data must be transformed into a structure of samples with `input` and `target` components before it can be used to fit a supervised learning model. <br>\n",
    "For a time series interested in one-step predictions, the observations at prior time steps, so-called lag observations, are used as `input` and the `target` is the observation at the current time step.\n",
    "\n",
    "For example, a univariate series can be expressed as a supervised learning problem with three time steps for `input` and one step as `target`, as follows:\n",
    "\n",
    "|input|target|\n",
    "|-----|------|\n",
    "[1, 2, 3]|[4]\n",
    "[2, 3, 4]|[5]\n",
    "[3, 4, 5]|[6]\n",
    "\n",
    "The Keras deep learning library provides the `TimeseriesGenerator` to automatically transform both univariate and multivariate time series data into such a format. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_train = DataSet(train_data, timesteps=hyperparam[\"model\"][\"sequence_size\"])\n",
    "dataset_validation = DataSet(validation_data, timesteps=hyperparam[\"model\"][\"sequence_size\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialize DataLoader\n",
    "Actually the data has a other size than in the table above because of multivariate time series data and because of using batches. <br>\n",
    "__First dimension__: batch size --> Defines the number of samples that will be propagated through the network simultaneously. <br>\n",
    "__Second dimension:__ timesteps --> Number of sequence which is passed into the LSTM <br>\n",
    "__Third dimension:__ input_dim --> Is the number of features. In this case data from 7 sensors, collected at the same time. <br>\n",
    "\n",
    "![](../../knowledge/pictures/input_shape.png)\n",
    "\n",
    "Data is shuffled because each mini batch is indipendent from each other, but samples of a minibatch are in chronological order."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_loader_training = DataLoader(dataset_train, batch_size=hyperparam[\"model\"][\"batch_size\"], \n",
    "                                  num_workers=1, shuffle=True, drop_last=True)\n",
    "data_loader_validation = DataLoader(dataset_validation, batch_size=hyperparam[\"model\"][\"batch_size\"], \n",
    "                                    num_workers=1, shuffle=True, drop_last=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data of batch: 0\n",
      "Size of input data: torch.Size([8, 50, 7])\n",
      "Size of target data: torch.Size([8, 7])\n",
      "Data of batch: 1\n",
      "Size of input data: torch.Size([8, 50, 7])\n",
      "Size of target data: torch.Size([8, 7])\n"
     ]
    }
   ],
   "source": [
    "for batch_idx, data in enumerate(data_loader_training):\n",
    "    x,y = data\n",
    "    print('Data of batch: {}'.format(batch_idx))\n",
    "    print(\"Size of input data: {}\".format(x.size()))\n",
    "    print(\"Size of target data: {}\".format(y.size()))\n",
    "    if batch_idx >=1: break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define MaximumLikelihood Loss function as torch module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MaximumLikelihoodLoss(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        \"\"\"\n",
    "        In the constructor we instantiate the module and assign them as\n",
    "        member variables.\n",
    "        \"\"\"\n",
    "        super(MaximumLikelihoodLoss, self).__init__()\n",
    "\n",
    "    def forward(self, y_hat, sigma, target_data):\n",
    "        \"\"\"\n",
    "        In the forward function we accept a Tensor of input data and we must return\n",
    "        a Tensor of output data. \n",
    "        \"\"\"\n",
    "        # Compute loss\n",
    "        term = torch.pow(((target_data-y_hat)/sigma), 2) + 2 * torch.log(sigma)\n",
    "        loss_batches = torch.sum(input=term, dim=1)\n",
    "        mean_loss = torch.sum(loss_batches)/hyperparam['model']['batch_size']\n",
    "        return mean_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialize Neural Network\n",
    "__Parameters for LSTM Modul:__\n",
    "- input_size : The number of expected features in the input x\n",
    "- hidden_size :The number of features in the hidden state h\n",
    "- num_layers : Number of recurrent layers. E.g., setting num_layers=2 would mean stacking two LSTMs together to form a stacked LSTM, with the second LSTM taking in outputs of the first LSTM and computing the final results.\n",
    "- batch_first : If True, then the input __and output__ tensors are provided as (batch, seq, feature).\n",
    "- dropout – If non-zero, introduces a Dropout layer on the outputs of each LSTM layer except the last layer, with dropout probability equal to dropout. Default: 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(0)\n",
    "model = LstmMl(batch_size=hyperparam['model']['batch_size'], input_dim=hyperparam['model']['input_size'], \n",
    "             n_hidden=hyperparam['model']['n_hidden'], n_layers=hyperparam['model']['lstm_layer'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Loop\n",
    "An epoch consists of a learning cycle over all batches of training data and an evaluation of the most recent model with the testing data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start model training\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/carlovoss/jupyter_notebooks/masterarbeit/venv_pm/lib/python3.6/site-packages/torch/nn/functional.py:1374: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n",
      "  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([8, 7])\n",
      "torch.Size([8, 7])\n"
     ]
    },
    {
     "ename": "ZeroDivisionError",
     "evalue": "float division by zero",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mZeroDivisionError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-7-b081fb8dc4e6>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     82\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     83\u001b[0m     \u001b[0;31m# Save mean of loss over all validation iterations\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 84\u001b[0;31m     \u001b[0mmean_epoch_validation_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepoch_validation_loss\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepoch_validation_loss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     85\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     86\u001b[0m     \u001b[0;31m# Save training and validation loss to history\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mZeroDivisionError\u001b[0m: float division by zero"
     ]
    }
   ],
   "source": [
    "# Initialize optimizer and Cyclic Learning Rate\n",
    "# lr=1. because of scheduler (1*learning_rate_schedular)\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=1.)  \n",
    "\n",
    "scheduler = torch.optim.lr_scheduler.CyclicLR(optimizer=optimizer, base_lr=hyperparam['cycling_lr']['base_lr'], \n",
    "                                              max_lr=hyperparam['cycling_lr']['max_lr'], step_size_up=hyperparam['cycling_lr']['step_size'], \n",
    "                                              mode=hyperparam['cycling_lr']['mode'], gamma=hyperparam['cycling_lr']['gamma'])\n",
    "\n",
    "print(\"Start model training\")\n",
    "# Create lists to save training loss and validation loss of each epoch\n",
    "hist_loss = []\n",
    "lr_find_lr = []\n",
    "\n",
    "# Set first comparative value\n",
    "lowest_loss = 99\n",
    "trails = 0\n",
    "\n",
    "for epoch in range(hyperparam['training']['n_epochs']):\n",
    "    # Empty list for recording performance \n",
    "    epoch_training_loss = []\n",
    "    epoch_validation_loss = []\n",
    "    \n",
    "    model.train()\n",
    "    \n",
    "    ##### Training #####\n",
    "    for batch_number, data in enumerate(data_loader_training):\n",
    "        # The LSTM has to be reinitialised, otherwise the LSTM will treat a new batch as a continuation of a sequence.\n",
    "        # When batches of data are independent sequences, then you should reinitialise the hidden state before each batch. \n",
    "        # But if your data is made up of really long sequences and you cut it up into batches making sure that each batch \n",
    "        # follows on from the previous batch, then in that case you wouldn’t reinitialise the hidden state before each batch.\n",
    "        # In the current workflow of class DataProvoider independent sequences are returned. \n",
    "        \n",
    "        input_data, target_data = data\n",
    "        \n",
    "        hidden = model.init_hidden()\n",
    "        \n",
    "        # Zero out gradient, else they will accumulate between minibatches\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # Forward propagation\n",
    "        output = model(input_data, hidden)\n",
    "         \n",
    "        # Calculate loss\n",
    "        criterion = LossModule()\n",
    "        loss = criterion(output, target_data)\n",
    "        epoch_training_loss.append(loss.item())\n",
    "    \n",
    "        # Backward pass\n",
    "        loss.backward()\n",
    "        \n",
    "        # Update parameters\n",
    "        optimizer.step()\n",
    "        \n",
    "        # Update LR\n",
    "        scheduler.step()\n",
    "        lr_step = optimizer.state_dict()[\"param_groups\"][0][\"lr\"]\n",
    "        lr_find_lr.append(lr_step)\n",
    "        \n",
    "    # Save mean of loss over all training iterations\n",
    "    mean_epoch_training_loss = sum(epoch_training_loss) / float(len(epoch_training_loss))\n",
    "    \n",
    "    ##### Evaluation #####\n",
    "    for batch_number, data in enumerate(data_loader_validation):\n",
    "        input_data, target_data = data\n",
    "        model.eval()\n",
    "        hidden = model.init_hidden()\n",
    "        output = model(input_data, hidden)\n",
    "        \n",
    "        # Calculate loss\n",
    "        criterion = LossModule()\n",
    "        loss = criterion(output, target_data)\n",
    "        epoch_validation_loss.append(loss.item())\n",
    "    \n",
    "    # Save mean of loss over all validation iterations\n",
    "    mean_epoch_validation_loss = sum(epoch_validation_loss) / float(len(epoch_validation_loss))\n",
    "    \n",
    "    # Save training and validation loss to history\n",
    "    hist_loss.append({'epoch': epoch, \n",
    "                      'training': mean_epoch_training_loss, \n",
    "                      'validation': mean_epoch_validation_loss})\n",
    "    \n",
    "    print(\"-------- epoch_no. {} finished with eval loss {}--------\".format(epoch, mean_epoch_validation_loss))\n",
    "        \n",
    "    # Check after every evaluation whether the latest model is the best one or not\n",
    "    # If this is the case, set current score to best_score, reset trails and save the model.\n",
    "    if mean_epoch_validation_loss < lowest_loss:\n",
    "        trials = 0\n",
    "        lowest_loss = mean_epoch_validation_loss\n",
    "        torch.save({\n",
    "            'epoch': epoch,\n",
    "            'model_state_dict': model.state_dict(),\n",
    "            'optimizer_state_dict': optimizer.state_dict(),\n",
    "            'loss': mean_epoch_validation_loss\n",
    "        }, \"../../../models/ML_model/best_model.pt\")\n",
    "        print(\"Epoch {}: best model saved with loss: {}\".format(epoch, mean_epoch_validation_loss))\n",
    "    \n",
    "    # Else: Increase trails by one and start new epoch as long as not too many epochs \n",
    "    # were unsuccessful (controlled by patience)\n",
    "    else:\n",
    "        trials += 1\n",
    "        if trials >= hyperparam['training']['patience'] :\n",
    "            print(f'Early stopping on epoch {epoch}')\n",
    "            break  \n",
    "\n",
    "# Safe results to csv file\n",
    "df = pd.DataFrame(hist_loss)\n",
    "df.to_csv(\"../../visualisation/files/history_ML.csv\", sep=\";\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualization of cyclic learning rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'lr_find_lr' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-078545715e24>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlr_find_lr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m'y'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mlr_find_lr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'x'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0max\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msubplots\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfigsize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m7\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m7\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0msns\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlineplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0max\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0max\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'lr_find_lr' is not defined"
     ]
    }
   ],
   "source": [
    "x = range(len(lr_find_lr))\n",
    "data = pd.DataFrame(data={'y': lr_find_lr, 'x': x})\n",
    "f, ax = plt.subplots(figsize=(7, 7))\n",
    "sns.lineplot(x=data.x, y=data.y, ax=ax)\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "masterarbeit",
   "language": "python",
   "name": "masterarbeit"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
