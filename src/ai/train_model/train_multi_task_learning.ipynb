{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader\n",
    "import torch.optim as optim\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# own Modules \n",
    "from models import LstmMultiTaskLearning, AnalyseLayer\n",
    "from data_loader import DataPreperator, DataSet\n",
    "from trainer import TrainerMultiTaskLearning\n",
    "from loss_module import LossModuleMse"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Take care of these things before training:\n",
    "- Select correct path and define droped_features\n",
    "- Change parameter of model\n",
    "- Change step_size in cycling_lr\n",
    "- Change filed_location"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "param = {\n",
    "    \"data\" : {\n",
    "        \"stake_training_data\" : 0.75, \n",
    "        \"path\" : '../../../data/phm_data_challenge/01_M01_DC_preprocessed_grid_search.csv' ,\n",
    "    },\n",
    "    \"preprocessing\" : {\n",
    "        \"first_order_difference\" : False,\n",
    "        \"droped_features\": [\"ID\", \"ongoing time\", \"up time\", \"RLU\", \"runnum\"],\n",
    "        \"features_not_to_scale\": ['FIXTURESHUTTERPOSITION_0.0', 'FIXTURESHUTTERPOSITION_1.0',\n",
    "                                      'FIXTURESHUTTERPOSITION_2.0', 'FIXTURESHUTTERPOSITION_3.0',\n",
    "                                      'FIXTURESHUTTERPOSITION_255.0']\n",
    "    },\n",
    "    \"model\" : {\n",
    "        \"input_size\" : 21,\n",
    "        \"n_hidden_lstm\" : 100,\n",
    "        \"sequence_size\" : 50,\n",
    "        \"batch_size\" : 8,\n",
    "        \"lstm_layer\" : 2,\n",
    "        \"n_hidden_fc_pred\": 50,\n",
    "        \"n_hidden_fc_ls\": 5,\n",
    "        \"dropout_rate\": 0.2\n",
    "    },\n",
    "    \"cycling_lr\" : {\n",
    "        \"scheduler_active\" : True, \n",
    "        # step_size is the number of training iterations (total samples/batch_size) per half cycle. \n",
    "        # Authors suggest setting step_size 2-8 x training iterations in epoch.\n",
    "        \"step_size\" : (6000/8)*2, \n",
    "        # Mode can be one of {triangular, triangular2, exp_range}\n",
    "        \"mode\" : \"triangular\", \n",
    "        \"gamma\" : 0.9995,\n",
    "        \"base_lr\" : 0.016, \n",
    "        \"max_lr\" :0.1\n",
    "    },\n",
    "    \"training\": {\n",
    "        \"n_epochs\" : 100,\n",
    "        \"patience\" : 10,\n",
    "    },\n",
    "    \"filed_location\": {\n",
    "        \"trained_model\" : \"../../../models/MSE_multi_learning/\",\n",
    "        \"history\" : \"../../visualisation/files/history_training/multi_learning.csv\",\n",
    "        \"latent_space\" : \"./latent_space.csv\"\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing\n",
    "1. First order difference (if selected)\n",
    "2. Split data into train and validation data\n",
    "3. Scale train and validation data with train's mean and variance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6251\n"
     ]
    }
   ],
   "source": [
    "train_loader = DataPreperator(path=param['data']['path'], \n",
    "                              ignored_features=param['preprocessing']['droped_features'],\n",
    "                              stake_training_data=param['data']['stake_training_data'],\n",
    "                              features_not_to_scale=param['preprocessing']['features_not_to_scale'],\n",
    "                              first_order_difference=param[\"preprocessing\"][\"first_order_difference\"])\n",
    "train_data, validation_data = train_loader.prepare_data()\n",
    "print(len(train_data)+len(validation_data))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mean and variance from scale process (only of continious features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-0.00616403  0.0014136  -0.00775649  0.02678833  0.11496844  0.01260359\n",
      " -0.03450196  0.01716666  0.00792266  0.05113533  0.15137018 -0.15249734\n",
      " -0.06415179 -0.25143178  0.87424645 -0.30155094]\n",
      "[9.54757947e-01 9.65299841e-01 9.79715208e-01 9.95428027e-01\n",
      " 1.20228023e+00 9.84873713e-01 4.43684229e-01 1.01021270e+00\n",
      " 9.70240421e-01 6.98134612e-01 1.44322416e+00 9.29744851e-12\n",
      " 1.95742556e-05 1.38008019e-05 4.47681049e-05 1.81650562e-01]\n"
     ]
    }
   ],
   "source": [
    "mean, var = train_loader.provide_statistics()\n",
    "print(mean)\n",
    "print(var)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialize Dataset \n",
    "Time series data must be transformed into a structure of samples with `input` and `target` components before it can be used to fit a supervised learning model. <br>\n",
    "For a time series interested in one-step predictions, the observations at prior time steps, so-called lag observations, are used as `input` and the `target` is the observation at the current time step.\n",
    "\n",
    "For example, a univariate series can be expressed as a supervised learning problem with three time steps for `input` and one step as `target`, as follows:\n",
    "\n",
    "|input|target|\n",
    "|-----|------|\n",
    "[1, 2, 3]|[4]\n",
    "[2, 3, 4]|[5]\n",
    "[3, 4, 5]|[6]\n",
    "\n",
    "The Keras deep learning library provides the `TimeseriesGenerator` to automatically transform both univariate and multivariate time series data into such a format. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_train = DataSet(train_data, timesteps=param[\"model\"][\"sequence_size\"])\n",
    "dataset_validation = DataSet(validation_data, timesteps=param[\"model\"][\"sequence_size\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialize DataLoader\n",
    "Actually the data has a other size than in the table above because of multivariate time series data and because of using batches. <br>\n",
    "__First dimension__: batch size --> Defines the number of samples that will be propagated through the network simultaneously. <br>\n",
    "__Second dimension:__ timesteps --> Number of sequence which is passed into the LSTM <br>\n",
    "__Third dimension:__ input_dim --> Is the number of features. In this case data from 7 sensors, collected at the same time. <br>\n",
    "\n",
    "![](../../../knowledge/pictures/input_shape.png)\n",
    "\n",
    "Data is shuffled because each mini batch is indipendent from each other, but samples of a minibatch are in chronological order."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_loader_training = DataLoader(dataset_train, \n",
    "                                  batch_size=param[\"model\"][\"batch_size\"], \n",
    "                                  num_workers=4, \n",
    "                                  shuffle=True, \n",
    "                                  drop_last=True\n",
    "                                 )\n",
    "data_loader_validation = DataLoader(dataset_validation, \n",
    "                                    batch_size=param[\"model\"][\"batch_size\"], \n",
    "                                    num_workers=4, \n",
    "                                    shuffle=True, \n",
    "                                    drop_last=True\n",
    "                                   )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0\n",
      "Data of batch: 0\n",
      "Data of X: tensor([[[ 0.5193, -0.1893,  0.0753,  ...,  0.0000,  0.0000,  0.0000],\n",
      "         [ 0.5193, -0.1899,  0.0774,  ...,  0.0000,  0.0000,  0.0000],\n",
      "         [ 0.5193, -0.1897,  0.0780,  ...,  0.0000,  0.0000,  0.0000],\n",
      "         ...,\n",
      "         [ 0.5193, -0.1898,  0.0782,  ...,  0.0000,  0.0000,  0.0000],\n",
      "         [ 0.5193, -0.1896,  0.0785,  ...,  0.0000,  0.0000,  0.0000],\n",
      "         [ 0.5193, -0.1901,  0.0772,  ...,  0.0000,  0.0000,  0.0000]],\n",
      "\n",
      "        [[-0.3199, -1.0750, -1.1559,  ...,  0.0000,  0.0000,  0.0000],\n",
      "         [-0.7148, -1.1488, -1.1559,  ...,  0.0000,  0.0000,  0.0000],\n",
      "         [-0.8722, -1.1499, -1.1563,  ...,  0.0000,  0.0000,  0.0000],\n",
      "         ...,\n",
      "         [-1.3794, -1.1499, -1.1557,  ...,  0.0000,  0.0000,  0.0000],\n",
      "         [-1.3794, -1.1499, -1.1556,  ...,  0.0000,  0.0000,  0.0000],\n",
      "         [-1.3794, -1.1499, -1.1557,  ...,  0.0000,  0.0000,  0.0000]],\n",
      "\n",
      "        [[ 0.7403,  1.1028,  1.0972,  ...,  0.0000,  0.0000,  0.0000],\n",
      "         [ 0.7403,  1.1032,  1.0963,  ...,  0.0000,  0.0000,  0.0000],\n",
      "         [ 0.7403,  1.1027,  1.0988,  ...,  0.0000,  0.0000,  0.0000],\n",
      "         ...,\n",
      "         [-1.3673, -1.1499, -1.1559,  ...,  0.0000,  0.0000,  0.0000],\n",
      "         [-1.3691, -1.1499, -1.1562,  ...,  0.0000,  0.0000,  0.0000],\n",
      "         [-1.3698, -1.1499, -1.1562,  ...,  0.0000,  0.0000,  0.0000]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[ 0.8063,  1.1028,  1.0954,  ...,  0.0000,  0.0000,  0.0000],\n",
      "         [ 0.8063,  1.1027,  1.0992,  ...,  0.0000,  0.0000,  0.0000],\n",
      "         [ 0.8063,  1.1027,  1.0984,  ...,  0.0000,  0.0000,  0.0000],\n",
      "         ...,\n",
      "         [ 0.8063,  1.1043,  1.0981,  ...,  0.0000,  0.0000,  0.0000],\n",
      "         [ 0.8063,  1.1030,  1.0936,  ...,  0.0000,  0.0000,  0.0000],\n",
      "         [ 0.8063,  1.1033,  1.0989,  ...,  0.0000,  0.0000,  0.0000]],\n",
      "\n",
      "        [[ 0.5193, -0.1894,  0.0758,  ...,  0.0000,  0.0000,  0.0000],\n",
      "         [ 0.5193, -0.1896,  0.0781,  ...,  0.0000,  0.0000,  0.0000],\n",
      "         [ 0.5193, -0.1893,  0.0769,  ...,  0.0000,  0.0000,  0.0000],\n",
      "         ...,\n",
      "         [ 0.5193, -0.1895,  0.0816,  ...,  0.0000,  0.0000,  0.0000],\n",
      "         [ 0.5193, -0.1901,  0.0717,  ...,  0.0000,  0.0000,  0.0000],\n",
      "         [ 0.5193, -0.1898,  0.0789,  ...,  0.0000,  0.0000,  0.0000]],\n",
      "\n",
      "        [[ 0.8063,  1.1029,  1.0962,  ...,  0.0000,  0.0000,  0.0000],\n",
      "         [ 0.8063,  1.1028,  1.0966,  ...,  0.0000,  0.0000,  0.0000],\n",
      "         [ 0.8063,  1.1029,  1.0962,  ...,  0.0000,  0.0000,  0.0000],\n",
      "         ...,\n",
      "         [ 0.8063,  1.1029,  1.0945,  ...,  0.0000,  0.0000,  0.0000],\n",
      "         [ 0.8063,  1.1031,  1.0970,  ...,  0.0000,  0.0000,  0.0000],\n",
      "         [ 0.8063,  1.1038,  1.0990,  ...,  0.0000,  0.0000,  0.0000]]])\n",
      "Epoch: 1\n",
      "Data of batch: 0\n",
      "Data of X: tensor([[[ 0.8798,  1.1039,  1.0999,  ...,  0.0000,  0.0000,  0.0000],\n",
      "         [ 0.8798,  1.1038,  1.0966,  ...,  0.0000,  0.0000,  0.0000],\n",
      "         [ 0.8798,  1.1035,  1.0957,  ...,  0.0000,  0.0000,  0.0000],\n",
      "         ...,\n",
      "         [ 0.8798,  1.1027,  1.0934,  ...,  0.0000,  0.0000,  0.0000],\n",
      "         [ 0.8798,  1.1028,  1.0958,  ...,  0.0000,  0.0000,  0.0000],\n",
      "         [ 0.8798,  1.1030,  1.0932,  ...,  0.0000,  0.0000,  0.0000]],\n",
      "\n",
      "        [[ 0.8043,  1.1032,  1.0984,  ...,  0.0000,  0.0000,  0.0000],\n",
      "         [ 0.8043,  1.1033,  1.0954,  ...,  0.0000,  0.0000,  0.0000],\n",
      "         [ 0.8043,  1.1031,  1.0967,  ...,  0.0000,  0.0000,  0.0000],\n",
      "         ...,\n",
      "         [-1.3625, -1.1499, -1.1558,  ...,  0.0000,  0.0000,  0.0000],\n",
      "         [-1.3636, -1.1499, -1.1555,  ...,  0.0000,  0.0000,  0.0000],\n",
      "         [-1.3646, -1.1499, -1.1558,  ...,  0.0000,  0.0000,  0.0000]],\n",
      "\n",
      "        [[ 0.7757,  1.1041,  1.0965,  ...,  0.0000,  0.0000,  0.0000],\n",
      "         [ 0.7757,  1.1029,  1.0946,  ...,  0.0000,  0.0000,  0.0000],\n",
      "         [ 0.7757,  1.1032,  1.0977,  ...,  0.0000,  0.0000,  0.0000],\n",
      "         ...,\n",
      "         [-1.3601, -1.1499, -1.1560,  ...,  0.0000,  0.0000,  0.0000],\n",
      "         [-1.3613, -1.1499, -1.1559,  ...,  0.0000,  0.0000,  0.0000],\n",
      "         [-1.3636, -1.1499, -1.1559,  ...,  0.0000,  0.0000,  0.0000]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[-1.3794, -1.1499, -1.1557,  ...,  0.0000,  0.0000,  0.0000],\n",
      "         [-1.3796, -1.1499, -1.1562,  ...,  0.0000,  0.0000,  0.0000],\n",
      "         [-1.3796, -1.1499, -1.1556,  ...,  0.0000,  0.0000,  0.0000],\n",
      "         ...,\n",
      "         [ 0.5892, -0.1896,  0.0767,  ...,  0.0000,  0.0000,  0.0000],\n",
      "         [ 0.5892, -0.1895,  0.0791,  ...,  0.0000,  0.0000,  0.0000],\n",
      "         [ 0.5892, -0.1900,  0.0766,  ...,  0.0000,  0.0000,  0.0000]],\n",
      "\n",
      "        [[-1.3086, -1.1499, -1.1558,  ...,  0.0000,  0.0000,  0.0000],\n",
      "         [-1.3252, -1.1499, -1.1559,  ...,  0.0000,  0.0000,  0.0000],\n",
      "         [-1.3285, -1.1499, -1.1560,  ...,  0.0000,  0.0000,  0.0000],\n",
      "         ...,\n",
      "         [-1.3800, -1.1499, -1.1558,  ...,  0.0000,  0.0000,  0.0000],\n",
      "         [-1.3800, -1.1499, -1.1558,  ...,  0.0000,  0.0000,  0.0000],\n",
      "         [-1.3802, -1.1499, -1.1559,  ...,  0.0000,  0.0000,  0.0000]],\n",
      "\n",
      "        [[-1.3693, -1.1499, -1.1554,  ...,  0.0000,  0.0000,  0.0000],\n",
      "         [-1.3715, -1.1499, -1.1556,  ...,  0.0000,  0.0000,  0.0000],\n",
      "         [-1.3721, -1.1499, -1.1561,  ...,  0.0000,  0.0000,  0.0000],\n",
      "         ...,\n",
      "         [ 0.5092, -0.1896,  0.0778,  ...,  1.0000,  0.0000,  0.0000],\n",
      "         [ 0.5092, -0.1894,  0.0733,  ...,  0.0000,  0.0000,  0.0000],\n",
      "         [ 0.5092, -0.1892,  0.0774,  ...,  0.0000,  0.0000,  0.0000]]])\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(2):\n",
    "    for batch_idx, data in enumerate(data_loader_training):\n",
    "        x,y = data\n",
    "        print(\"Epoch: {}\".format(epoch))\n",
    "        print('Data of batch: {}'.format(batch_idx))\n",
    "        print(\"Data of X: {}\".format(x))\n",
    "        if batch_idx >=0: break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data of batch: 0\n",
      "Size of input data: torch.Size([8, 50, 21])\n",
      "Size of target data: torch.Size([8, 21])\n",
      "Data of batch: 1\n",
      "Size of input data: torch.Size([8, 50, 21])\n",
      "Size of target data: torch.Size([8, 21])\n"
     ]
    }
   ],
   "source": [
    "for batch_idx, data in enumerate(data_loader_training):\n",
    "    x,y = data\n",
    "    print('Data of batch: {}'.format(batch_idx))\n",
    "    print(\"Size of input data: {}\".format(x.size()))\n",
    "    print(\"Size of target data: {}\".format(y.size()))\n",
    "    if batch_idx >=1: break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialize Neural Network\n",
    "__Parameters for LSTM Modul:__\n",
    "- input_size : The number of expected features in the input x\n",
    "- hidden_size :The number of features in the hidden state h\n",
    "- num_layers : Number of recurrent layers. E.g., setting num_layers=2 would mean stacking two LSTMs together to form a stacked LSTM, with the second LSTM taking in outputs of the first LSTM and computing the final results.\n",
    "- batch_first : If True, then the input __and output__ tensors are provided as (batch, seq, feature).\n",
    "- dropout â€“ If non-zero, introduces a Dropout layer on the outputs of each LSTM layer except the last layer, with dropout probability equal to dropout. Default: 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(0)\n",
    "model = LstmMultiTaskLearning(batch_size=param['model']['batch_size'], \n",
    "                              input_dim=param['model']['input_size'], \n",
    "                              n_hidden_lstm=param['model']['n_hidden_lstm'], \n",
    "                              n_layers=param['model']['lstm_layer'],\n",
    "                              dropout_rate= param['model']['dropout_rate'],\n",
    "                              n_hidden_fc_prediction=param['model']['n_hidden_fc_pred'], \n",
    "                              n_hidden_fc_ls_analysis=param['model']['n_hidden_fc_ls'], \n",
    "                              file_location_latent_space=param[\"filed_location\"][\"latent_space\"]         \n",
    "                              )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define MSE Loss function as torch module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = LossModuleMse(param[\"model\"][\"input_size\"], param[\"model\"][\"batch_size\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialize Optimizer and Cyclic Learning Rate Scheduler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.SGD(model.parameters(), lr=1.)  \n",
    "scheduler = torch.optim.lr_scheduler.CyclicLR(optimizer=optimizer, \n",
    "                                              base_lr=param['cycling_lr']['base_lr'], \n",
    "                                              max_lr=param['cycling_lr']['max_lr'], \n",
    "                                              step_size_up=param['cycling_lr']['step_size'], \n",
    "                                              mode=param['cycling_lr']['mode'],\n",
    "                                              gamma=param['cycling_lr']['gamma']\n",
    "                                             )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialize Trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = TrainerMultiTaskLearning(model=model,\n",
    "                                   optimizer=optimizer,\n",
    "                                   scheduler=scheduler,\n",
    "                                   scheduler_active = param[\"cycling_lr\"][\"scheduler_active\"],\n",
    "                                   criterion=criterion, \n",
    "                                   location_model=param[\"filed_location\"][\"trained_model\"], \n",
    "                                   location_stats=param[\"filed_location\"][\"history\"], \n",
    "                                   patience=param['training']['patience']\n",
    "                                   )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Loop\n",
    "An epoch consists of a learning cycle over all batches of training data and an evaluation of the most recent model with the testing data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------- epoch_no. 0 finished with eval loss 1.2538799390433326--------\n",
      "Epoch 0: best model saved with loss: 1.2538799390433326\n",
      "-------- epoch_no. 1 finished with eval loss 1.0297310153051975--------\n",
      "Epoch 1: best model saved with loss: 1.0297310153051975\n",
      "-------- epoch_no. 2 finished with eval loss 0.9183272661697086--------\n",
      "Epoch 2: best model saved with loss: 0.9183272661697086\n",
      "-------- epoch_no. 3 finished with eval loss 0.8508673845322201--------\n",
      "Epoch 3: best model saved with loss: 0.8508673845322201\n",
      "-------- epoch_no. 4 finished with eval loss 0.8041733030288939--------\n",
      "Epoch 4: best model saved with loss: 0.8041733030288939\n",
      "-------- epoch_no. 5 finished with eval loss 0.7687423527109349--------\n",
      "Epoch 5: best model saved with loss: 0.7687423527109349\n",
      "-------- epoch_no. 6 finished with eval loss 0.7450863083969529--------\n",
      "Epoch 6: best model saved with loss: 0.7450863083969529\n",
      "-------- epoch_no. 7 finished with eval loss 0.7253599358752133--------\n",
      "Epoch 7: best model saved with loss: 0.7253599358752133\n",
      "-------- epoch_no. 8 finished with eval loss 0.7061304585161733--------\n",
      "Epoch 8: best model saved with loss: 0.7061304585161733\n",
      "-------- epoch_no. 9 finished with eval loss 0.6912022450850124--------\n",
      "Epoch 9: best model saved with loss: 0.6912022450850124\n",
      "-------- epoch_no. 10 finished with eval loss 0.6765937024782116--------\n",
      "Epoch 10: best model saved with loss: 0.6765937024782116\n",
      "-------- epoch_no. 11 finished with eval loss 0.6648597135430291--------\n",
      "Epoch 11: best model saved with loss: 0.6648597135430291\n",
      "-------- epoch_no. 12 finished with eval loss 0.6553410257860612--------\n",
      "Epoch 12: best model saved with loss: 0.6553410257860612\n",
      "-------- epoch_no. 13 finished with eval loss 0.6475982225738686--------\n",
      "Epoch 13: best model saved with loss: 0.6475982225738686\n",
      "-------- epoch_no. 14 finished with eval loss 0.6393060174765726--------\n",
      "Epoch 14: best model saved with loss: 0.6393060174765726\n",
      "-------- epoch_no. 15 finished with eval loss 0.6327841957358437--------\n",
      "Epoch 15: best model saved with loss: 0.6327841957358437\n",
      "-------- epoch_no. 16 finished with eval loss 0.626458571862616--------\n",
      "Epoch 16: best model saved with loss: 0.626458571862616\n",
      "-------- epoch_no. 17 finished with eval loss 0.6203723476355406--------\n",
      "Epoch 17: best model saved with loss: 0.6203723476355406\n",
      "-------- epoch_no. 18 finished with eval loss 0.6147152082582216--------\n",
      "Epoch 18: best model saved with loss: 0.6147152082582216\n",
      "-------- epoch_no. 19 finished with eval loss 0.6099492095657205--------\n",
      "Epoch 19: best model saved with loss: 0.6099492095657205\n",
      "-------- epoch_no. 20 finished with eval loss 0.606008203680005--------\n",
      "Epoch 20: best model saved with loss: 0.606008203680005\n",
      "-------- epoch_no. 21 finished with eval loss 0.6023762011591447--------\n",
      "Epoch 21: best model saved with loss: 0.6023762011591447\n",
      "-------- epoch_no. 22 finished with eval loss 0.5989363468165353--------\n",
      "Epoch 22: best model saved with loss: 0.5989363468165353\n",
      "-------- epoch_no. 23 finished with eval loss 0.5948234064466725--------\n",
      "Epoch 23: best model saved with loss: 0.5948234064466725\n",
      "-------- epoch_no. 24 finished with eval loss 0.59161887574527--------\n",
      "Epoch 24: best model saved with loss: 0.59161887574527\n",
      "-------- epoch_no. 25 finished with eval loss 0.5887286627129187--------\n",
      "Epoch 25: best model saved with loss: 0.5887286627129187\n",
      "-------- epoch_no. 26 finished with eval loss 0.5859182569249959--------\n",
      "Epoch 26: best model saved with loss: 0.5859182569249959\n",
      "-------- epoch_no. 27 finished with eval loss 0.5834593757366141--------\n",
      "Epoch 27: best model saved with loss: 0.5834593757366141\n",
      "-------- epoch_no. 28 finished with eval loss 0.5816861361535692--------\n",
      "Epoch 28: best model saved with loss: 0.5816861361535692\n",
      "-------- epoch_no. 29 finished with eval loss 0.5790603277610198--------\n",
      "Epoch 29: best model saved with loss: 0.5790603277610198\n",
      "-------- epoch_no. 30 finished with eval loss 0.5767083473784623--------\n",
      "Epoch 30: best model saved with loss: 0.5767083473784623\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/carlovoss/anaconda3/lib/python3.6/multiprocessing/queues.py\", line 240, in _feed\n",
      "    send_bytes(obj)\n",
      "  File \"/Users/carlovoss/anaconda3/lib/python3.6/multiprocessing/connection.py\", line 200, in send_bytes\n",
      "    self._send_bytes(m[offset:offset + size])\n",
      "  File \"/Users/carlovoss/anaconda3/lib/python3.6/multiprocessing/connection.py\", line 404, in _send_bytes\n",
      "    self._send(header + buf)\n",
      "  File \"/Users/carlovoss/anaconda3/lib/python3.6/multiprocessing/connection.py\", line 368, in _send\n",
      "    n = write(self._handle, buf)\n",
      "BrokenPipeError: [Errno 32] Broken pipe\n",
      "  File \"/Users/carlovoss/anaconda3/lib/python3.6/multiprocessing/queues.py\", line 240, in _feed\n",
      "    send_bytes(obj)\n",
      "  File \"/Users/carlovoss/anaconda3/lib/python3.6/multiprocessing/connection.py\", line 200, in send_bytes\n",
      "    self._send_bytes(m[offset:offset + size])\n",
      "  File \"/Users/carlovoss/anaconda3/lib/python3.6/multiprocessing/connection.py\", line 404, in _send_bytes\n",
      "    self._send(header + buf)\n",
      "  File \"/Users/carlovoss/anaconda3/lib/python3.6/multiprocessing/connection.py\", line 368, in _send\n",
      "    n = write(self._handle, buf)\n",
      "BrokenPipeError: [Errno 32] Broken pipe\n",
      "  File \"/Users/carlovoss/anaconda3/lib/python3.6/multiprocessing/queues.py\", line 240, in _feed\n",
      "    send_bytes(obj)\n",
      "  File \"/Users/carlovoss/anaconda3/lib/python3.6/multiprocessing/connection.py\", line 200, in send_bytes\n",
      "    self._send_bytes(m[offset:offset + size])\n",
      "  File \"/Users/carlovoss/anaconda3/lib/python3.6/multiprocessing/connection.py\", line 404, in _send_bytes\n",
      "    self._send(header + buf)\n",
      "  File \"/Users/carlovoss/anaconda3/lib/python3.6/multiprocessing/connection.py\", line 368, in _send\n",
      "    n = write(self._handle, buf)\n",
      "BrokenPipeError: [Errno 32] Broken pipe\n",
      "  File \"/Users/carlovoss/anaconda3/lib/python3.6/multiprocessing/queues.py\", line 240, in _feed\n",
      "    send_bytes(obj)\n",
      "  File \"/Users/carlovoss/anaconda3/lib/python3.6/multiprocessing/connection.py\", line 200, in send_bytes\n",
      "    self._send_bytes(m[offset:offset + size])\n",
      "  File \"/Users/carlovoss/anaconda3/lib/python3.6/multiprocessing/connection.py\", line 404, in _send_bytes\n",
      "    self._send(header + buf)\n",
      "  File \"/Users/carlovoss/anaconda3/lib/python3.6/multiprocessing/connection.py\", line 368, in _send\n",
      "    n = write(self._handle, buf)\n",
      "BrokenPipeError: [Errno 32] Broken pipe\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-13-65786c29b6c6>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparam\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'training'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'n_epochs'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0;31m# Train with batches\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m     \u001b[0mmean_epoch_training_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_loader_training\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0;31m# Evaluate\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/jupyter_notebooks/masterarbeit/src/ai/utils/trainer.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, data_loader_training)\u001b[0m\n\u001b[1;32m    154\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    155\u001b[0m             \u001b[0;31m# Backward pass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 156\u001b[0;31m             \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    157\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    158\u001b[0m             \u001b[0;31m# Update parameters\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/jupyter_notebooks/masterarbeit/venv_pm/lib/python3.6/site-packages/torch/tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph)\u001b[0m\n\u001b[1;32m    105\u001b[0m                 \u001b[0mproducts\u001b[0m\u001b[0;34m.\u001b[0m \u001b[0mDefaults\u001b[0m \u001b[0mto\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    106\u001b[0m         \"\"\"\n\u001b[0;32m--> 107\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    108\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    109\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/jupyter_notebooks/masterarbeit/venv_pm/lib/python3.6/site-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables)\u001b[0m\n\u001b[1;32m     91\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[1;32m     92\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 93\u001b[0;31m         allow_unreachable=True)  # allow_unreachable flag\n\u001b[0m\u001b[1;32m     94\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     95\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Create lists to save training loss and validation loss of each epoch\n",
    "hist_loss = []\n",
    "torch.manual_seed(0)\n",
    "\n",
    "for epoch in range(param['training']['n_epochs']):\n",
    "    # Train with batches \n",
    "    mean_epoch_training_loss = trainer.train(data_loader_training)\n",
    "\n",
    "    # Evaluate\n",
    "    mean_epoch_validation_loss = trainer.evaluate(data_loader_validation, hist_loss, epoch)\n",
    "\n",
    "    # Cache History\n",
    "    trainer.cache_history_training(hist_loss, epoch, mean_epoch_training_loss, mean_epoch_validation_loss)\n",
    "\n",
    "    # Save model if its the best one since the last change in configuration of hyperparameters\n",
    "    status_ok = trainer.save_model(epoch, mean_epoch_validation_loss, param['model']['input_size'], \n",
    "                                   param['model']['lstm_layer'], param['model']['n_hidden_lstm'], \n",
    "                                   param['model']['n_hidden_fc_pred'], param[\"model\"][\"sequence_size\"],\n",
    "                                   param['model']['n_hidden_fc_ls']\n",
    "                                  )\n",
    "    if not status_ok:\n",
    "        break\n",
    "\n",
    "# Safe results to csv file\n",
    "df = pd.DataFrame(hist_loss)\n",
    "df.to_csv(param[\"filed_location\"][\"history\"], sep=\";\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualization of cyclic learning rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = range(len(lr_find_lr))\n",
    "data = pd.DataFrame(data={'y': lr_find_lr, 'x': x})\n",
    "f, ax = plt.subplots(figsize=(7, 7))\n",
    "sns.lineplot(x=data.x, y=data.y, ax=ax)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "masterarbeit",
   "language": "python",
   "name": "masterarbeit"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
