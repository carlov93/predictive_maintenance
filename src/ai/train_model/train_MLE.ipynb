{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader\n",
    "import torch.optim as optim\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# own Modules \n",
    "#from models import LstmMle\n",
    "from data_loader import DataPreperator, DataSet\n",
    "from trainer import Trainer\n",
    "#from loss_module import LossModuleMle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "\n",
    "class LstmMle(nn.Module):\n",
    "    def __init__(self, batch_size, input_dim, n_hidden_lstm, n_layers, dropout_rate, n_hidden_fc):\n",
    "        super(LstmMle, self).__init__()\n",
    "        # Attributes for LSTM Network\n",
    "        self.input_dim = input_dim\n",
    "        self.n_hidden_lstm = n_hidden_lstm\n",
    "        self.n_layers = n_layers\n",
    "        self.batch_size = batch_size\n",
    "        self.dropout_rate = dropout_rate\n",
    "        self.n_hidden_fc = n_hidden_fc\n",
    "        \n",
    "        # Definition of NN layer\n",
    "        # batch_first = True because dataloader creates batches and batch_size is 0. dimension\n",
    "        self.lstm = nn.LSTM(input_size = self.input_dim, hidden_size = self.n_hidden_lstm, num_layers = self.n_layers, batch_first = True, dropout = self.dropout_rate)\n",
    "        self.fc1 = nn.Linear(self.n_hidden_lstm, self.n_hidden_fc)\n",
    "        self.dropout = nn.Dropout(p=self.dropout_rate)\n",
    "        self.fc_y_hat = nn.Linear(self.n_hidden_fc, self.input_dim)\n",
    "        self.fc_tau = nn.Linear(self.n_hidden_fc, self.input_dim)\n",
    "        \n",
    "    def forward(self, input_data, hidden):\n",
    "        # Forward propagate LSTM\n",
    "        # LSTM in Pytorch return two results: the first one usually called output \n",
    "        # and the second one (hidden_state, cell_state). \n",
    "        lstm_out, (hidden_state, cell_state) = self.lstm(input_data, hidden)\n",
    "\n",
    "        # LSTM returns as output all the hidden_states for all the timesteps (seq), \n",
    "        # in other words all of the hidden states throughout\n",
    "        # the sequence.\n",
    "        # Thus we have to select the output from the last sequence (last hidden state of sequence)\n",
    "        # Length of input data can varry \n",
    "        length_seq = input_data.size()[1]\n",
    "        last_out = lstm_out[:,length_seq-1,:]\n",
    "\n",
    "        # Forward path through the subsequent fully connected tanh activation \n",
    "        # neural network with 2q output channels\n",
    "        out = self.fc1(last_out)\n",
    "        out = self.dropout(out)\n",
    "        out = F.tanh(out)\n",
    "        y_hat = self.fc_y_hat(out)\n",
    "        tau = self.fc_tau(out)\n",
    "        \n",
    "        print(\"y_hat: \".format(y_hat))\n",
    "        print(\"tau: \".format(tau))\n",
    "        print(\"-------------------\")\n",
    "        \n",
    "        return [y_hat, tau]\n",
    "    \n",
    "    def init_hidden(self):\n",
    "        # This method is for initializing hidden state as well as cell state\n",
    "        # We need to detach the hidden state to prevent exploding/vanishing gradients\n",
    "        h0 = torch.zeros(self.n_layers, self.batch_size, self.n_hidden_lstm, requires_grad=False)\n",
    "        c0 = torch.zeros(self.n_layers, self.batch_size, self.n_hidden_lstm, requires_grad=False)\n",
    "        return [t for t in (h0, c0)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LossModuleMle(torch.nn.Module):\n",
    "    def __init__(self, input_size, batch_size):\n",
    "        \"\"\"\n",
    "        In the constructor we instantiate the module and assign them as\n",
    "        member variables.\n",
    "        \"\"\"\n",
    "        super(LossModuleMle, self).__init__()\n",
    "        self.input_size = input_size\n",
    "        self.batch_size = batch_size\n",
    "\n",
    "    def forward(self, output, target_data):\n",
    "        \"\"\"\n",
    "        In the forward function we accept a Tensor of input data and we must return\n",
    "        a Tensor of output data.\n",
    "        We are minimizing the the negative log likelihood loss function.\n",
    "        We write σ_t = exp(τ_t) to guarantee σ > 0 and to provide numerical stability in the learning process.\n",
    "        \"\"\"\n",
    "        # Extract elements from output\n",
    "        y_hat, tau = output\n",
    "        \n",
    "        print(\"y_hat: \".format(y_hat))\n",
    "        print(\"tau: \".format(tau))\n",
    "        \n",
    "        # Compute loss\n",
    "        term = torch.pow((target_data - y_hat) / torch.exp(tau), 2) + 2 * tau\n",
    "        loss_batches = torch.sum(input=term, dim=1) / self.input_size\n",
    "        mean_loss = torch.sum(loss_batches)/self.batch_size\n",
    "        \n",
    "        print(\"Term: \".format(term))\n",
    "        print(\"Loss per batch: \".format(loss_batches))\n",
    "        print(\"Mean loss: \".format(mean_loss))\n",
    "        \n",
    "        return mean_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "param = {\n",
    "    \"data\" : {\n",
    "        \"stake_training_data\" : 0.75, \n",
    "        \"path\" : '../../../data/phm_data_challenge/01_M01_DC_preprocessed_grid_search.csv' ,\n",
    "    },\n",
    "    \"preprocessing\" : {\n",
    "        \"first_order_difference\" : False,\n",
    "        \"droped_features\": [\"ID\", \"ongoing time\", \"up time\", \"RLU\", \"runnum\"],\n",
    "        \"features_not_to_scale\": ['FIXTURESHUTTERPOSITION_0.0', 'FIXTURESHUTTERPOSITION_1.0',\n",
    "                                      'FIXTURESHUTTERPOSITION_2.0', 'FIXTURESHUTTERPOSITION_3.0',\n",
    "                                      'FIXTURESHUTTERPOSITION_255.0']\n",
    "    },\n",
    "    \"model\" : {\n",
    "        \"input_size\" : 21,\n",
    "        \"n_hidden_lstm\" : 100,\n",
    "        \"sequence_size\" : 50,\n",
    "        \"batch_size\" : 8,\n",
    "        \"lstm_layer\" : 2,\n",
    "        \"n_hidden_fc\": 50,\n",
    "        \"dropout_rate\": 0.2\n",
    "    },\n",
    "    \"cycling_lr\" : {\n",
    "        \"scheduler_active\" : True, \n",
    "        # step_size is the number of training iterations (total samples/batch_size) per half cycle. \n",
    "        # Authors suggest setting step_size 2-8 x training iterations in epoch.\n",
    "        \"step_size\" : (12500/8)*2, \n",
    "        # Mode can be one of {triangular, triangular2, exp_range}\n",
    "        \"mode\" : \"triangular\", \n",
    "        \"gamma\" : 0.9995,\n",
    "        \"base_lr\" : 0.016, \n",
    "        \"max_lr\" :0.1\n",
    "    },\n",
    "    \"training\": {\n",
    "        \"n_epochs\" : 100,\n",
    "        \"patience\" : 10,\n",
    "    },\n",
    "    \"filed_location\": {\n",
    "        \"trained_model\" : \"../../../models/MLE_model/xxxxxxxx\",\n",
    "        \"history\" : \"../../visualisation/files/history_training/MLExxxxxxxxx.csv\"\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing\n",
    "1. First order difference (if selected)\n",
    "2. Split data into train and validation data\n",
    "3. Scale train and validation data with train's mean and variance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6251\n"
     ]
    }
   ],
   "source": [
    "train_loader = DataPreperator(path=param['data']['path'], \n",
    "                              ignored_features=param['preprocessing']['droped_features'],\n",
    "                              stake_training_data=param['data']['stake_training_data'],\n",
    "                              features_not_to_scale=param['preprocessing']['features_not_to_scale'],\n",
    "                              first_order_difference=param[\"preprocessing\"][\"first_order_difference\"])\n",
    "train_data, validation_data = train_loader.prepare_data()\n",
    "print(len(train_data)+len(validation_data))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mean and variance from scale process (only of continious features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-0.00616403  0.0014136  -0.00775649  0.02678833  0.11496844  0.01260359\n",
      " -0.03450196  0.01716666  0.00792266  0.05113533  0.15137018 -0.15249734\n",
      " -0.06415179 -0.25143178  0.87424645 -0.30155094]\n",
      "[9.54757947e-01 9.65299841e-01 9.79715208e-01 9.95428027e-01\n",
      " 1.20228023e+00 9.84873713e-01 4.43684229e-01 1.01021270e+00\n",
      " 9.70240421e-01 6.98134612e-01 1.44322416e+00 9.29744851e-12\n",
      " 1.95742556e-05 1.38008019e-05 4.47681049e-05 1.81650562e-01]\n"
     ]
    }
   ],
   "source": [
    "mean, var = train_loader.provide_statistics()\n",
    "print(mean)\n",
    "print(var)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialize Dataset \n",
    "Time series data must be transformed into a structure of samples with `input` and `target` components before it can be used to fit a supervised learning model. <br>\n",
    "For a time series interested in one-step predictions, the observations at prior time steps, so-called lag observations, are used as `input` and the `target` is the observation at the current time step.\n",
    "\n",
    "For example, a univariate series can be expressed as a supervised learning problem with three time steps for `input` and one step as `target`, as follows:\n",
    "\n",
    "|input|target|\n",
    "|-----|------|\n",
    "[1, 2, 3]|[4]\n",
    "[2, 3, 4]|[5]\n",
    "[3, 4, 5]|[6]\n",
    "\n",
    "The Keras deep learning library provides the `TimeseriesGenerator` to automatically transform both univariate and multivariate time series data into such a format. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_train = DataSet(train_data, timesteps=param[\"model\"][\"sequence_size\"])\n",
    "dataset_validation = DataSet(validation_data, timesteps=param[\"model\"][\"sequence_size\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialize DataLoader\n",
    "Actually the data has a other size than in the table above because of multivariate time series data and because of using batches. <br>\n",
    "__First dimension__: batch size --> Defines the number of samples that will be propagated through the network simultaneously. <br>\n",
    "__Second dimension:__ timesteps --> Number of sequence which is passed into the LSTM <br>\n",
    "__Third dimension:__ input_dim --> Is the number of features. In this case data from 7 sensors, collected at the same time. <br>\n",
    "\n",
    "![](../../../knowledge/pictures/input_shape.png)\n",
    "\n",
    "Data is shuffled because each mini batch is indipendent from each other, but samples of a minibatch are in chronological order."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_loader_training = DataLoader(dataset_train, \n",
    "                                  batch_size=param[\"model\"][\"batch_size\"], \n",
    "                                  num_workers=4, \n",
    "                                  shuffle=True, \n",
    "                                  drop_last=True\n",
    "                                 )\n",
    "data_loader_validation = DataLoader(dataset_validation, \n",
    "                                    batch_size=param[\"model\"][\"batch_size\"], \n",
    "                                    num_workers=4, \n",
    "                                    shuffle=True, \n",
    "                                    drop_last=True\n",
    "                                   )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data of batch: 0\n",
      "Size of input data: torch.Size([8, 50, 21])\n",
      "Size of target data: torch.Size([8, 21])\n",
      "Data of batch: 1\n",
      "Size of input data: torch.Size([8, 50, 21])\n",
      "Size of target data: torch.Size([8, 21])\n"
     ]
    }
   ],
   "source": [
    "for batch_idx, data in enumerate(data_loader_training):\n",
    "    x,y = data\n",
    "    print('Data of batch: {}'.format(batch_idx))\n",
    "    print(\"Size of input data: {}\".format(x.size()))\n",
    "    print(\"Size of target data: {}\".format(y.size()))\n",
    "    if batch_idx >=1: break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialize Neural Network\n",
    "__Parameters for LSTM Modul:__\n",
    "- input_size : The number of expected features in the input x\n",
    "- hidden_size :The number of features in the hidden state h\n",
    "- num_layers : Number of recurrent layers. E.g., setting num_layers=2 would mean stacking two LSTMs together to form a stacked LSTM, with the second LSTM taking in outputs of the first LSTM and computing the final results.\n",
    "- batch_first : If True, then the input __and output__ tensors are provided as (batch, seq, feature).\n",
    "- dropout – If non-zero, introduces a Dropout layer on the outputs of each LSTM layer except the last layer, with dropout probability equal to dropout. Default: 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(0)\n",
    "model = LstmMle(batch_size=param['model']['batch_size'], \n",
    "                input_dim=param['model']['input_size'], \n",
    "                n_hidden_lstm=param['model']['n_hidden_lstm'], \n",
    "                n_layers=param['model']['lstm_layer'],\n",
    "                dropout_rate= param['model']['dropout_rate'],\n",
    "                n_hidden_fc=param['model']['n_hidden_fc']\n",
    "                )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define MSE Loss function as torch module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = LossModuleMle(param[\"model\"][\"input_size\"], param[\"model\"][\"batch_size\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialize Optimizer and Cyclic Learning Rate Scheduler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.SGD(model.parameters(), lr=1.)  \n",
    "scheduler = torch.optim.lr_scheduler.CyclicLR(optimizer=optimizer, \n",
    "                                              base_lr=param['cycling_lr']['base_lr'], \n",
    "                                              max_lr=param['cycling_lr']['max_lr'], \n",
    "                                              step_size_up=param['cycling_lr']['step_size'], \n",
    "                                              mode=param['cycling_lr']['mode'],\n",
    "                                              gamma=param['cycling_lr']['gamma']\n",
    "                                             )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialize Trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = Trainer(model=model,\n",
    "                  optimizer=optimizer,\n",
    "                  scheduler=scheduler,\n",
    "                  scheduler_active = param[\"cycling_lr\"][\"scheduler_active\"],\n",
    "                  criterion=criterion, \n",
    "                  location_model=param[\"filed_location\"][\"trained_model\"], \n",
    "                  location_stats=param[\"filed_location\"][\"history\"], \n",
    "                  patience=param['training']['patience']\n",
    "                 )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Loop\n",
    "An epoch consists of a learning cycle over all batches of training data and an evaluation of the most recent model with the testing data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/carlovoss/jupyter_notebooks/masterarbeit/venv_pm/lib/python3.6/site-packages/torch/nn/functional.py:1374: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n",
      "  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "y_hat: \n",
      "tau: \n",
      "-------------------\n",
      "y_hat: \n",
      "tau: \n",
      "Term: \n",
      "Loss per batch: \n",
      "Mean loss: \n",
      "y_hat: \n",
      "tau: \n",
      "-------------------\n",
      "y_hat: \n",
      "tau: \n",
      "Term: \n",
      "Loss per batch: \n",
      "Mean loss: \n",
      "y_hat: \n",
      "tau: \n",
      "-------------------\n",
      "y_hat: \n",
      "tau: \n",
      "Term: \n",
      "Loss per batch: \n",
      "Mean loss: \n",
      "y_hat: \n",
      "tau: \n",
      "-------------------\n",
      "y_hat: \n",
      "tau: \n",
      "Term: \n",
      "Loss per batch: \n",
      "Mean loss: \n",
      "y_hat: \n",
      "tau: \n",
      "-------------------\n",
      "y_hat: \n",
      "tau: \n",
      "Term: \n",
      "Loss per batch: \n",
      "Mean loss: \n",
      "y_hat: \n",
      "tau: \n",
      "-------------------\n",
      "y_hat: \n",
      "tau: \n",
      "Term: \n",
      "Loss per batch: \n",
      "Mean loss: \n",
      "y_hat: \n",
      "tau: \n",
      "-------------------\n",
      "y_hat: \n",
      "tau: \n",
      "Term: \n",
      "Loss per batch: \n",
      "Mean loss: \n",
      "y_hat: \n",
      "tau: \n",
      "-------------------\n",
      "y_hat: \n",
      "tau: \n",
      "Term: \n",
      "Loss per batch: \n",
      "Mean loss: \n",
      "y_hat: \n",
      "tau: \n",
      "-------------------\n",
      "y_hat: \n",
      "tau: \n",
      "Term: \n",
      "Loss per batch: \n",
      "Mean loss: \n",
      "y_hat: \n",
      "tau: \n",
      "-------------------\n",
      "y_hat: \n",
      "tau: \n",
      "Term: \n",
      "Loss per batch: \n",
      "Mean loss: \n",
      "y_hat: \n",
      "tau: \n",
      "-------------------\n",
      "y_hat: \n",
      "tau: \n",
      "Term: \n",
      "Loss per batch: \n",
      "Mean loss: \n",
      "y_hat: \n",
      "tau: \n",
      "-------------------\n",
      "y_hat: \n",
      "tau: \n",
      "Term: \n",
      "Loss per batch: \n",
      "Mean loss: \n",
      "y_hat: \n",
      "tau: \n",
      "-------------------\n",
      "y_hat: \n",
      "tau: \n",
      "Term: \n",
      "Loss per batch: \n",
      "Mean loss: \n",
      "y_hat: \n",
      "tau: \n",
      "-------------------\n",
      "y_hat: \n",
      "tau: \n",
      "Term: \n",
      "Loss per batch: \n",
      "Mean loss: \n",
      "y_hat: \n",
      "tau: \n",
      "-------------------\n",
      "y_hat: \n",
      "tau: \n",
      "Term: \n",
      "Loss per batch: \n",
      "Mean loss: \n",
      "y_hat: \n",
      "tau: \n",
      "-------------------\n",
      "y_hat: \n",
      "tau: \n",
      "Term: \n",
      "Loss per batch: \n",
      "Mean loss: \n",
      "y_hat: \n",
      "tau: \n",
      "-------------------\n",
      "y_hat: \n",
      "tau: \n",
      "Term: \n",
      "Loss per batch: \n",
      "Mean loss: \n",
      "y_hat: \n",
      "tau: \n",
      "-------------------\n",
      "y_hat: \n",
      "tau: \n",
      "Term: \n",
      "Loss per batch: \n",
      "Mean loss: \n",
      "y_hat: \n",
      "tau: \n",
      "-------------------\n",
      "y_hat: \n",
      "tau: \n",
      "Term: \n",
      "Loss per batch: \n",
      "Mean loss: \n",
      "y_hat: \n",
      "tau: \n",
      "-------------------\n",
      "y_hat: \n",
      "tau: \n",
      "Term: \n",
      "Loss per batch: \n",
      "Mean loss: \n",
      "y_hat: \n",
      "tau: \n",
      "-------------------\n",
      "y_hat: \n",
      "tau: \n",
      "Term: \n",
      "Loss per batch: \n",
      "Mean loss: \n",
      "y_hat: \n",
      "tau: \n",
      "-------------------\n",
      "y_hat: \n",
      "tau: \n",
      "Term: \n",
      "Loss per batch: \n",
      "Mean loss: \n",
      "y_hat: \n",
      "tau: \n",
      "-------------------\n",
      "y_hat: \n",
      "tau: \n",
      "Term: \n",
      "Loss per batch: \n",
      "Mean loss: \n",
      "y_hat: \n",
      "tau: \n",
      "-------------------\n",
      "y_hat: \n",
      "tau: \n",
      "Term: \n",
      "Loss per batch: \n",
      "Mean loss: \n",
      "y_hat: \n",
      "tau: \n",
      "-------------------\n",
      "y_hat: \n",
      "tau: \n",
      "Term: \n",
      "Loss per batch: \n",
      "Mean loss: \n",
      "y_hat: \n",
      "tau: \n",
      "-------------------\n",
      "y_hat: \n",
      "tau: \n",
      "Term: \n",
      "Loss per batch: \n",
      "Mean loss: \n",
      "y_hat: \n",
      "tau: \n",
      "-------------------\n",
      "y_hat: \n",
      "tau: \n",
      "Term: \n",
      "Loss per batch: \n",
      "Mean loss: \n",
      "y_hat: \n",
      "tau: \n",
      "-------------------\n",
      "y_hat: \n",
      "tau: \n",
      "Term: \n",
      "Loss per batch: \n",
      "Mean loss: \n",
      "y_hat: \n",
      "tau: \n",
      "-------------------\n",
      "y_hat: \n",
      "tau: \n",
      "Term: \n",
      "Loss per batch: \n",
      "Mean loss: \n",
      "y_hat: \n",
      "tau: \n",
      "-------------------\n",
      "y_hat: \n",
      "tau: \n",
      "Term: \n",
      "Loss per batch: \n",
      "Mean loss: \n",
      "y_hat: \n",
      "tau: \n",
      "-------------------\n",
      "y_hat: \n",
      "tau: \n",
      "Term: \n",
      "Loss per batch: \n",
      "Mean loss: \n",
      "y_hat: \n",
      "tau: \n",
      "-------------------\n",
      "y_hat: \n",
      "tau: \n",
      "Term: \n",
      "Loss per batch: \n",
      "Mean loss: \n",
      "y_hat: \n",
      "tau: \n",
      "-------------------\n",
      "y_hat: \n",
      "tau: \n",
      "Term: \n",
      "Loss per batch: \n",
      "Mean loss: \n",
      "y_hat: \n",
      "tau: \n",
      "-------------------\n",
      "y_hat: \n",
      "tau: \n",
      "Term: \n",
      "Loss per batch: \n",
      "Mean loss: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/carlovoss/anaconda3/lib/python3.6/multiprocessing/queues.py\", line 240, in _feed\n",
      "    send_bytes(obj)\n",
      "  File \"/Users/carlovoss/anaconda3/lib/python3.6/multiprocessing/connection.py\", line 200, in send_bytes\n",
      "    self._send_bytes(m[offset:offset + size])\n",
      "  File \"/Users/carlovoss/anaconda3/lib/python3.6/multiprocessing/connection.py\", line 404, in _send_bytes\n",
      "    self._send(header + buf)\n",
      "  File \"/Users/carlovoss/anaconda3/lib/python3.6/multiprocessing/connection.py\", line 368, in _send\n",
      "    n = write(self._handle, buf)\n",
      "BrokenPipeError: [Errno 32] Broken pipe\n",
      "  File \"/Users/carlovoss/anaconda3/lib/python3.6/multiprocessing/queues.py\", line 240, in _feed\n",
      "    send_bytes(obj)\n",
      "  File \"/Users/carlovoss/anaconda3/lib/python3.6/multiprocessing/connection.py\", line 200, in send_bytes\n",
      "    self._send_bytes(m[offset:offset + size])\n",
      "  File \"/Users/carlovoss/anaconda3/lib/python3.6/multiprocessing/connection.py\", line 404, in _send_bytes\n",
      "    self._send(header + buf)\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/carlovoss/anaconda3/lib/python3.6/multiprocessing/connection.py\", line 368, in _send\n",
      "    n = write(self._handle, buf)\n",
      "BrokenPipeError: [Errno 32] Broken pipe\n",
      "  File \"/Users/carlovoss/anaconda3/lib/python3.6/multiprocessing/queues.py\", line 240, in _feed\n",
      "    send_bytes(obj)\n",
      "  File \"/Users/carlovoss/anaconda3/lib/python3.6/multiprocessing/connection.py\", line 200, in send_bytes\n",
      "    self._send_bytes(m[offset:offset + size])\n",
      "  File \"/Users/carlovoss/anaconda3/lib/python3.6/multiprocessing/connection.py\", line 404, in _send_bytes\n",
      "    self._send(header + buf)\n",
      "  File \"/Users/carlovoss/anaconda3/lib/python3.6/multiprocessing/connection.py\", line 368, in _send\n",
      "    n = write(self._handle, buf)\n",
      "BrokenPipeError: [Errno 32] Broken pipe\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/carlovoss/anaconda3/lib/python3.6/multiprocessing/queues.py\", line 240, in _feed\n",
      "    send_bytes(obj)\n",
      "  File \"/Users/carlovoss/anaconda3/lib/python3.6/multiprocessing/connection.py\", line 200, in send_bytes\n",
      "    self._send_bytes(m[offset:offset + size])\n",
      "  File \"/Users/carlovoss/anaconda3/lib/python3.6/multiprocessing/connection.py\", line 404, in _send_bytes\n",
      "    self._send(header + buf)\n",
      "  File \"/Users/carlovoss/anaconda3/lib/python3.6/multiprocessing/connection.py\", line 368, in _send\n",
      "    n = write(self._handle, buf)\n",
      "BrokenPipeError: [Errno 32] Broken pipe\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-36-dfd1d610d55a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparam\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'training'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'n_epochs'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0;31m# Train with batches\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m     \u001b[0mmean_epoch_training_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_loader_training\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0;31m# Evaluate\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/jupyter_notebooks/masterarbeit/src/ai/utils/trainer.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, data_loader_training)\u001b[0m\n\u001b[1;32m     41\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     42\u001b[0m             \u001b[0;31m# Forward propagation\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 43\u001b[0;31m             \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhidden\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     44\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     45\u001b[0m             \u001b[0;31m# Calculate loss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/jupyter_notebooks/masterarbeit/venv_pm/lib/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    491\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    492\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 493\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    494\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    495\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-24-9d555edc6f10>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_data, hidden)\u001b[0m\n\u001b[1;32m     24\u001b[0m         \u001b[0;31m# LSTM in Pytorch return two results: the first one usually called output\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m         \u001b[0;31m# and the second one (hidden_state, cell_state).\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 26\u001b[0;31m         \u001b[0mlstm_out\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mhidden_state\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcell_state\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlstm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhidden\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     27\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m         \u001b[0;31m# LSTM returns as output all the hidden_states for all the timesteps (seq),\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/jupyter_notebooks/masterarbeit/venv_pm/lib/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    491\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    492\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 493\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    494\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    495\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/jupyter_notebooks/masterarbeit/venv_pm/lib/python3.6/site-packages/torch/nn/modules/rnn.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input, hx)\u001b[0m\n\u001b[1;32m    557\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward_packed\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    558\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 559\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    560\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    561\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/jupyter_notebooks/masterarbeit/venv_pm/lib/python3.6/site-packages/torch/nn/modules/rnn.py\u001b[0m in \u001b[0;36mforward_tensor\u001b[0;34m(self, input, hx)\u001b[0m\n\u001b[1;32m    537\u001b[0m         \u001b[0munsorted_indices\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    538\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 539\u001b[0;31m         \u001b[0moutput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhidden\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_sizes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_batch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msorted_indices\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    540\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    541\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0moutput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpermute_hidden\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhidden\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0munsorted_indices\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/jupyter_notebooks/masterarbeit/venv_pm/lib/python3.6/site-packages/torch/nn/modules/rnn.py\u001b[0m in \u001b[0;36mforward_impl\u001b[0;34m(self, input, hx, batch_sizes, max_batch_size, sorted_indices)\u001b[0m\n\u001b[1;32m    520\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mbatch_sizes\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    521\u001b[0m             result = _VF.lstm(input, hx, self._get_flat_weights(), self.bias, self.num_layers,\n\u001b[0;32m--> 522\u001b[0;31m                               self.dropout, self.training, self.bidirectional, self.batch_first)\n\u001b[0m\u001b[1;32m    523\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    524\u001b[0m             result = _VF.lstm(input, batch_sizes, hx, self._get_flat_weights(), self.bias,\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Create lists to save training loss and validation loss of each epoch\n",
    "hist_loss = []\n",
    "torch.manual_seed(0)\n",
    "\n",
    "for epoch in range(param['training']['n_epochs']):\n",
    "    # Train with batches \n",
    "    mean_epoch_training_loss = trainer.train(data_loader_training)\n",
    "\n",
    "    # Evaluate\n",
    "    mean_epoch_validation_loss = trainer.evaluate(data_loader_validation, hist_loss, epoch)\n",
    "\n",
    "    # Cache History\n",
    "    trainer.cache_history_training(hist_loss, epoch, mean_epoch_training_loss, mean_epoch_validation_loss)\n",
    "\n",
    "    # Save model if its the best one since the last change in configuration of hyperparameters\n",
    "    status_ok = trainer.save_model(epoch, mean_epoch_validation_loss, param['model']['input_size'], \n",
    "                                   param['model']['lstm_layer'], param['model']['n_hidden_lstm'], \n",
    "                                   param['model']['n_hidden_fc'], param[\"model\"][\"sequence_size\"])\n",
    "    if not status_ok:\n",
    "        break\n",
    "\n",
    "# Safe results to csv file\n",
    "df = pd.DataFrame(hist_loss)\n",
    "df.to_csv(param[\"filed_location\"][\"history\"], sep=\";\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "masterarbeit",
   "language": "python",
   "name": "masterarbeit"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
