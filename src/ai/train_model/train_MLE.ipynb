{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader\n",
    "import torch.optim as optim\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# own Modules \n",
    "#from models import LstmMle\n",
    "from data_loader import DataPreperator, DataSet\n",
    "#from trainer import Trainer\n",
    "#from loss_module import LossModuleMle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import builtins\n",
    "\n",
    "class Trainer():\n",
    "    def __init__(self, model, optimizer, scheduler, scheduler_active, criterion, patience, location_model, location_stats):\n",
    "        self.model = model\n",
    "        # lr=1. because of scheduler (1*learning_rate_schedular)\n",
    "        self.optimizer = optimizer\n",
    "        self.scheduler = scheduler\n",
    "        self.scheduler_active = scheduler_active\n",
    "        # initialize further variables\n",
    "        self.criterion = criterion\n",
    "        self.epoch_training_loss = []\n",
    "        self.epoch_validation_loss = []\n",
    "        self.lowest_loss = 99\n",
    "        self.trails = 0\n",
    "        self.patience = patience\n",
    "        self.location_model = location_model\n",
    "        self.location_stats = location_stats\n",
    "    \n",
    "    def train(self, data_loader_training):\n",
    "        for batch_number, data in enumerate(data_loader_training):\n",
    "            # The LSTM has to be reinitialised, otherwise the LSTM will treat a new batch \n",
    "            # as a continuation of a sequence. When batches of data are independent sequences, \n",
    "            # then you should reinitialise the hidden state before each batch. \n",
    "            # But if your data is made up of really long sequences and you cut it up into batches \n",
    "            # making sure that each batch follows on from the previous batch, then in that case \n",
    "            # you wouldnâ€™t reinitialise the hidden state before each batch.\n",
    "            # In the current workflow of class DataProvoider independent sequences are returned. \n",
    "            input_data, target_data = data\n",
    "            \n",
    "            self.model.train()\n",
    "            hidden = self.model.init_hidden()\n",
    "\n",
    "            # Zero out gradient, else they will accumulate between minibatches\n",
    "            self.optimizer.zero_grad()\n",
    "\n",
    "            # Forward propagation\n",
    "            y_hat, tau = self.model(input_data, hidden)\n",
    "\n",
    "            # Calculate loss\n",
    "            loss = self.criterion(y_hat, tau, target_data)\n",
    "            \n",
    "            if loss >5:\n",
    "                print(\"Mean loss in batchnumber {}: {}\".format(batch_number, loss))\n",
    "            \n",
    "            self.epoch_training_loss.append(loss.item())\n",
    "\n",
    "            # Backward pass\n",
    "            loss.backward()\n",
    "\n",
    "            # Update parameters\n",
    "            self.optimizer.step()\n",
    "\n",
    "            # Update LR if scheduler is active \n",
    "            if self.scheduler_active:\n",
    "                self.scheduler.step()\n",
    "            \n",
    "        # Return mean of loss over all training iterations\n",
    "        return sum(self.epoch_training_loss) / float(len(self.epoch_training_loss))\n",
    "    \n",
    "    def evaluate(self, data_loader_validation, hist_loss, epoch):\n",
    "        for batch_number, data in enumerate(data_loader_validation):\n",
    "            input_data, target_data = data\n",
    "            self.model.eval()\n",
    "            hidden = self.model.init_hidden()\n",
    "            y_hat, tau = self.model(input_data, hidden)\n",
    "\n",
    "            # Calculate loss\n",
    "            loss = self.criterion(y_hat, tau, target_data)\n",
    "            self.epoch_validation_loss.append(loss.item())\n",
    "            \n",
    "        # Return mean of loss over all validation iterations\n",
    "        return sum(self.epoch_validation_loss) / float(len(self.epoch_validation_loss))\n",
    "            \n",
    "    def cache_history_training(self, hist_loss, epoch, mean_epoch_training_loss, mean_epoch_validation_loss):\n",
    "        # Save training and validation loss to history\n",
    "        print(\"-------- epoch_no. {} finished with eval loss {}--------\".format(epoch, mean_epoch_validation_loss))\n",
    "        return {'epoch': epoch, 'training': mean_epoch_training_loss, 'validation': mean_epoch_validation_loss}\n",
    "            \n",
    "        # Empty list for new epoch \n",
    "        self.epoch_training_loss = []\n",
    "        self.epoch_validation_loss = []\n",
    "        \n",
    "    def save_model(self, epoch, mean_epoch_validation_loss, input_size, n_lstm_layer, n_hidden_lstm, n_hidden_fc, seq_size):\n",
    "        if mean_epoch_validation_loss < self.lowest_loss:\n",
    "            self.trials = 0\n",
    "            self.lowest_loss = mean_epoch_validation_loss\n",
    "            torch.save({\n",
    "                'model_state_dict': self.model.state_dict(),\n",
    "                'optimizer_state_dict': self.optimizer.state_dict(),\n",
    "                'loss': mean_epoch_validation_loss\n",
    "            }, self.location_model+\"_InputSize\"+str(input_size)+\"_LayerLstm\"+\n",
    "                str(n_lstm_layer)+\"_HiddenLstm\"+str(n_hidden_lstm)+\"_HiddenFc\"+str(n_hidden_fc)+\"_Seq\"+str(seq_size)+\".pt\")\n",
    "            print(\"Epoch {}: best model saved with loss: {}\".format(epoch, mean_epoch_validation_loss))\n",
    "            return True\n",
    "    \n",
    "        # Else: Increase trails by one and start new epoch as long as not too many epochs \n",
    "        # were unsuccessful (controlled by patience)\n",
    "        else:\n",
    "            self.trials += 1\n",
    "            if self.trials >= self.patience :\n",
    "                print(\"Early stopping on epoch {}\".format(epoch))\n",
    "                return False\n",
    "            return True\n",
    "    \n",
    "    def save_statistic(self, hist_loss, sequenze_size, n_lstm_layer, n_hidden_lstm, n_hidden_fc, time):\n",
    "        with open(self.location_stats, 'a') as file:\n",
    "            file.write(\"\\n\"+str(round(min(hist_loss),2))+\",\"+str(sequenze_size)+\",\"+str(n_lstm_layer)+\",\"+\n",
    "            str(n_hidden_lstm)+\",\"+str(n_hidden_fc)+\",\"+str(round(time,1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "\n",
    "class LstmMle(nn.Module):\n",
    "    def __init__(self, batch_size, input_dim, n_hidden_lstm, n_layers, dropout_rate, n_hidden_fc):\n",
    "        super(LstmMle, self).__init__()\n",
    "        # Attributes for LSTM Network\n",
    "        self.input_dim = input_dim\n",
    "        self.n_hidden_lstm = n_hidden_lstm\n",
    "        self.n_layers = n_layers\n",
    "        self.batch_size = batch_size\n",
    "        self.dropout_rate = dropout_rate\n",
    "        self.n_hidden_fc = n_hidden_fc\n",
    "        \n",
    "        # Definition of NN layer\n",
    "        # batch_first = True because dataloader creates batches and batch_size is 0. dimension\n",
    "        self.lstm = nn.LSTM(input_size = self.input_dim, hidden_size = self.n_hidden_lstm, num_layers = self.n_layers, batch_first = True, dropout = self.dropout_rate)\n",
    "        self.fc1 = nn.Linear(self.n_hidden_lstm, self.n_hidden_fc)\n",
    "        self.dropout = nn.Dropout(p=self.dropout_rate)\n",
    "        self.fc_y_hat = nn.Linear(self.n_hidden_fc, self.input_dim)\n",
    "        self.fc_tau = nn.Linear(self.n_hidden_fc, self.input_dim)\n",
    "        \n",
    "    def forward(self, input_data, hidden):\n",
    "        # Forward propagate LSTM\n",
    "        # LSTM in Pytorch return two results: the first one usually called output \n",
    "        # and the second one (hidden_state, cell_state). \n",
    "        lstm_out, (hidden_state, cell_state) = self.lstm(input_data, hidden)\n",
    "\n",
    "        # LSTM returns as output all the hidden_states for all the timesteps (seq), \n",
    "        # in other words all of the hidden states throughout\n",
    "        # the sequence.\n",
    "        # Thus we have to select the output from the last sequence (last hidden state of sequence)\n",
    "        # Length of input data can varry \n",
    "        length_seq = input_data.size()[1]\n",
    "        last_out = lstm_out[:,length_seq-1,:]\n",
    "\n",
    "        # Forward path through the subsequent fully connected tanh activation \n",
    "        # neural network with 2q output channels\n",
    "        out = self.fc1(last_out)\n",
    "        out = self.dropout(out)\n",
    "        out = F.tanh(out)\n",
    "        y_hat = self.fc_y_hat(out)\n",
    "        tau = self.fc_tau(out)\n",
    "        \n",
    "        #print(\"y_hat: {}\".format(y_hat))\n",
    "        #print(\"tau: {}\".format(tau))\n",
    "        #print(\"-------------------\")\n",
    "        \n",
    "        return y_hat, tau\n",
    "    \n",
    "    def init_hidden(self):\n",
    "        # This method is for initializing hidden state as well as cell state\n",
    "        # We need to detach the hidden state to prevent exploding/vanishing gradients\n",
    "        h0 = torch.zeros(self.n_layers, self.batch_size, self.n_hidden_lstm, requires_grad=False)\n",
    "        c0 = torch.zeros(self.n_layers, self.batch_size, self.n_hidden_lstm, requires_grad=False)\n",
    "        return [t for t in (h0, c0)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LossModuleMle(torch.nn.Module):\n",
    "    def __init__(self, input_size, batch_size):\n",
    "        \"\"\"\n",
    "        In the constructor we instantiate the module and assign them as\n",
    "        member variables.\n",
    "        \"\"\"\n",
    "        super(LossModuleMle, self).__init__()\n",
    "        self.input_size = input_size\n",
    "        self.batch_size = batch_size\n",
    "\n",
    "    def forward(self, y_hat, tau, target_data):\n",
    "        \"\"\"\n",
    "        In the forward function we accept a Tensor of input data and we must return\n",
    "        a Tensor of output data.\n",
    "        We are minimizing the the negative log likelihood loss function.\n",
    "        We write Ïƒ_t = exp(Ï„_t) to guarantee Ïƒ > 0 and to provide numerical stability in the learning process.\n",
    "        \"\"\"\n",
    "        # Extract elements from output\n",
    "        \n",
    "        #print(\"y_hat: {}\".format(y_hat))\n",
    "        #print(\"tau: {}\".format(tau))\n",
    "        \n",
    "        # Compute loss\n",
    "        term = torch.pow((target_data - y_hat) / torch.exp(tau), 2) + 2 * tau\n",
    "        loss_batches = torch.sum(input=term, dim=1) / self.input_size\n",
    "        mean_loss = torch.sum(loss_batches)/self.batch_size\n",
    "        \n",
    "        #print(\"Term: {}\".format(term))\n",
    "        #print(\"Loss per batch: {}\".format(loss_batches))\n",
    "        #print(\"Mean loss: {}\".format(mean_loss))\n",
    "        #if mean_loss >5 or mean_loss <0:\n",
    "            #print(\"target_data: {}\".format(target_data))\n",
    "            #print(\"y_hat: {}\".format(y_hat))\n",
    "            #print(\"tau: {}\".format(tau))\n",
    "            #print(\"Term: {}\".format(term))\n",
    "            #print(\"Mean loss: {}\".format(mean_loss))\n",
    "        \n",
    "        return mean_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "param = {\n",
    "    \"data\" : {\n",
    "        \"stake_training_data\" : 0.75, \n",
    "        \"path\" : '../../../data/phm_data_challenge/01_M01_DC_preprocessed_grid_search.csv' ,\n",
    "    },\n",
    "    \"preprocessing\" : {\n",
    "        \"first_order_difference\" : False,\n",
    "        \"droped_features\": [\"ID\", \"ongoing time\", \"up time\", \"RLU\", \"runnum\"],\n",
    "        \"features_not_to_scale\": ['FIXTURESHUTTERPOSITION_0.0', 'FIXTURESHUTTERPOSITION_1.0',\n",
    "                                      'FIXTURESHUTTERPOSITION_2.0', 'FIXTURESHUTTERPOSITION_3.0',\n",
    "                                      'FIXTURESHUTTERPOSITION_255.0']\n",
    "    },\n",
    "    \"model\" : {\n",
    "        \"input_size\" : 21,\n",
    "        \"n_hidden_lstm\" : 100,\n",
    "        \"sequence_size\" : 50,\n",
    "        \"batch_size\" : 8,\n",
    "        \"lstm_layer\" : 2,\n",
    "        \"n_hidden_fc\": 50,\n",
    "        \"dropout_rate\": 0.2\n",
    "    },\n",
    "    \"cycling_lr\" : {\n",
    "        \"scheduler_active\" : True, \n",
    "        # step_size is the number of training iterations (total samples/batch_size) per half cycle. \n",
    "        # Authors suggest setting step_size 2-8 x training iterations in epoch.\n",
    "        \"step_size\" : (12500/8)*2, \n",
    "        # Mode can be one of {triangular, triangular2, exp_range}\n",
    "        \"mode\" : \"triangular\", \n",
    "        \"gamma\" : 0.9995,\n",
    "        \"base_lr\" : 0.005,\n",
    "        \"max_lr\" :0.03\n",
    "    },\n",
    "    \"training\": {\n",
    "        \"n_epochs\" : 100,\n",
    "        \"patience\" : 10,\n",
    "    },\n",
    "    \"filed_location\": {\n",
    "        \"trained_model\" : \"../../../models/MLE_model/xxxxxxxx\",\n",
    "        \"history\" : \"../../visualisation/files/history_training/MLExxxxxxxxx.csv\"\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing\n",
    "1. First order difference (if selected)\n",
    "2. Split data into train and validation data\n",
    "3. Scale train and validation data with train's mean and variance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6251\n"
     ]
    }
   ],
   "source": [
    "train_loader = DataPreperator(path=param['data']['path'], \n",
    "                              ignored_features=param['preprocessing']['droped_features'],\n",
    "                              stake_training_data=param['data']['stake_training_data'],\n",
    "                              features_not_to_scale=param['preprocessing']['features_not_to_scale'],\n",
    "                              first_order_difference=param[\"preprocessing\"][\"first_order_difference\"])\n",
    "train_data, validation_data = train_loader.prepare_data()\n",
    "print(len(train_data)+len(validation_data))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mean and variance from scale process (only of continious features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-0.00616403  0.0014136  -0.00775649  0.02678833  0.11496844  0.01260359\n",
      " -0.03450196  0.01716666  0.00792266  0.05113533  0.15137018 -0.15249734\n",
      " -0.06415179 -0.25143178  0.87424645 -0.30155094]\n",
      "[9.54757947e-01 9.65299841e-01 9.79715208e-01 9.95428027e-01\n",
      " 1.20228023e+00 9.84873713e-01 4.43684229e-01 1.01021270e+00\n",
      " 9.70240421e-01 6.98134612e-01 1.44322416e+00 9.29744851e-12\n",
      " 1.95742556e-05 1.38008019e-05 4.47681049e-05 1.81650562e-01]\n"
     ]
    }
   ],
   "source": [
    "mean, var = train_loader.provide_statistics()\n",
    "print(mean)\n",
    "print(var)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialize Dataset \n",
    "Time series data must be transformed into a structure of samples with `input` and `target` components before it can be used to fit a supervised learning model. <br>\n",
    "For a time series interested in one-step predictions, the observations at prior time steps, so-called lag observations, are used as `input` and the `target` is the observation at the current time step.\n",
    "\n",
    "For example, a univariate series can be expressed as a supervised learning problem with three time steps for `input` and one step as `target`, as follows:\n",
    "\n",
    "|input|target|\n",
    "|-----|------|\n",
    "[1, 2, 3]|[4]\n",
    "[2, 3, 4]|[5]\n",
    "[3, 4, 5]|[6]\n",
    "\n",
    "The Keras deep learning library provides the `TimeseriesGenerator` to automatically transform both univariate and multivariate time series data into such a format. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_train = DataSet(train_data, timesteps=param[\"model\"][\"sequence_size\"])\n",
    "dataset_validation = DataSet(validation_data, timesteps=param[\"model\"][\"sequence_size\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialize DataLoader\n",
    "Actually the data has a other size than in the table above because of multivariate time series data and because of using batches. <br>\n",
    "__First dimension__: batch size --> Defines the number of samples that will be propagated through the network simultaneously. <br>\n",
    "__Second dimension:__ timesteps --> Number of sequence which is passed into the LSTM <br>\n",
    "__Third dimension:__ input_dim --> Is the number of features. In this case data from 7 sensors, collected at the same time. <br>\n",
    "\n",
    "![](../../../knowledge/pictures/input_shape.png)\n",
    "\n",
    "Data is shuffled because each mini batch is indipendent from each other, but samples of a minibatch are in chronological order."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_loader_training = DataLoader(dataset_train, \n",
    "                                  batch_size=param[\"model\"][\"batch_size\"], \n",
    "                                  num_workers=4, \n",
    "                                  shuffle=True, \n",
    "                                  drop_last=True\n",
    "                                 )\n",
    "data_loader_validation = DataLoader(dataset_validation, \n",
    "                                    batch_size=param[\"model\"][\"batch_size\"], \n",
    "                                    num_workers=4, \n",
    "                                    shuffle=True, \n",
    "                                    drop_last=True\n",
    "                                   )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data of batch: 0\n",
      "Size of input data: torch.Size([8, 50, 21])\n",
      "Size of target data: torch.Size([8, 21])\n",
      "Data of batch: 1\n",
      "Size of input data: torch.Size([8, 50, 21])\n",
      "Size of target data: torch.Size([8, 21])\n"
     ]
    }
   ],
   "source": [
    "for batch_idx, data in enumerate(data_loader_training):\n",
    "    x,y = data\n",
    "    print('Data of batch: {}'.format(batch_idx))\n",
    "    print(\"Size of input data: {}\".format(x.size()))\n",
    "    print(\"Size of target data: {}\".format(y.size()))\n",
    "    if batch_idx >=1: break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialize Neural Network\n",
    "__Parameters for LSTM Modul:__\n",
    "- input_size : The number of expected features in the input x\n",
    "- hidden_size :The number of features in the hidden state h\n",
    "- num_layers : Number of recurrent layers. E.g., setting num_layers=2 would mean stacking two LSTMs together to form a stacked LSTM, with the second LSTM taking in outputs of the first LSTM and computing the final results.\n",
    "- batch_first : If True, then the input __and output__ tensors are provided as (batch, seq, feature).\n",
    "- dropout â€“ If non-zero, introduces a Dropout layer on the outputs of each LSTM layer except the last layer, with dropout probability equal to dropout. Default: 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(0)\n",
    "model = LstmMle(batch_size=param['model']['batch_size'], \n",
    "                input_dim=param['model']['input_size'], \n",
    "                n_hidden_lstm=param['model']['n_hidden_lstm'], \n",
    "                n_layers=param['model']['lstm_layer'],\n",
    "                dropout_rate= param['model']['dropout_rate'],\n",
    "                n_hidden_fc=param['model']['n_hidden_fc']\n",
    "                )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define MSE Loss function as torch module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = LossModuleMle(param[\"model\"][\"input_size\"], param[\"model\"][\"batch_size\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialize Optimizer and Cyclic Learning Rate Scheduler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.SGD(model.parameters(), lr=1.)  \n",
    "scheduler = torch.optim.lr_scheduler.CyclicLR(optimizer=optimizer, \n",
    "                                              base_lr=param['cycling_lr']['base_lr'], \n",
    "                                              max_lr=param['cycling_lr']['max_lr'], \n",
    "                                              step_size_up=param['cycling_lr']['step_size'], \n",
    "                                              mode=param['cycling_lr']['mode'],\n",
    "                                              gamma=param['cycling_lr']['gamma']\n",
    "                                             )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialize Trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = Trainer(model=model,\n",
    "                  optimizer=optimizer,\n",
    "                  scheduler=scheduler,\n",
    "                  scheduler_active = param[\"cycling_lr\"][\"scheduler_active\"],\n",
    "                  criterion=criterion, \n",
    "                  location_model=param[\"filed_location\"][\"trained_model\"], \n",
    "                  location_stats=param[\"filed_location\"][\"history\"], \n",
    "                  patience=param['training']['patience']\n",
    "                 )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Loop\n",
    "An epoch consists of a learning cycle over all batches of training data and an evaluation of the most recent model with the testing data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/carlovoss/jupyter_notebooks/masterarbeit/venv_pm/lib/python3.6/site-packages/torch/nn/functional.py:1374: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n",
      "  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean loss in batchnumber 347: 30.261371612548828\n",
      "Mean loss in batchnumber 372: 5.352893352508545\n",
      "Mean loss in batchnumber 373: 8.840778350830078\n",
      "Mean loss in batchnumber 374: 7.8138427734375\n",
      "Mean loss in batchnumber 402: 31.865032196044922\n",
      "Mean loss in batchnumber 405: 6.212568283081055\n",
      "Mean loss in batchnumber 418: 1099.4146728515625\n",
      "Mean loss in batchnumber 419: 46.77112579345703\n",
      "-------- epoch_no. 0 finished with eval loss nan--------\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'Trainer' object has no attribute 'trials'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-133-eb0114e168c0>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     17\u001b[0m     status_ok = trainer.save_model(epoch, mean_epoch_validation_loss, param['model']['input_size'], \n\u001b[1;32m     18\u001b[0m                                    \u001b[0mparam\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'model'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'lstm_layer'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparam\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'model'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'n_hidden_lstm'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 19\u001b[0;31m                                    param['model']['n_hidden_fc'], param[\"model\"][\"sequence_size\"])\n\u001b[0m\u001b[1;32m     20\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mstatus_ok\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m         \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-120-f71a48717267>\u001b[0m in \u001b[0;36msave_model\u001b[0;34m(self, epoch, mean_epoch_validation_loss, input_size, n_lstm_layer, n_hidden_lstm, n_hidden_fc, seq_size)\u001b[0m\n\u001b[1;32m    103\u001b[0m         \u001b[0;31m# were unsuccessful (controlled by patience)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    104\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 105\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrials\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    106\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrials\u001b[0m \u001b[0;34m>=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpatience\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    107\u001b[0m                 \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Early stopping on epoch {}\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepoch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'Trainer' object has no attribute 'trials'"
     ]
    }
   ],
   "source": [
    "# Create lists to save training loss and validation loss of each epoch\n",
    "hist_loss = []\n",
    "torch.manual_seed(0)\n",
    "\n",
    "for epoch in range(param['training']['n_epochs']):\n",
    "    # Train with batches \n",
    "    mean_epoch_training_loss = trainer.train(data_loader_training)\n",
    "    \n",
    "    # Evaluate\n",
    "    mean_epoch_validation_loss = trainer.evaluate(data_loader_validation, hist_loss, epoch)\n",
    "\n",
    "    # Cache History\n",
    "    history = trainer.cache_history_training(hist_loss, epoch, mean_epoch_training_loss, mean_epoch_validation_loss)\n",
    "    hist_loss.append(history)\n",
    "\n",
    "    # Save model if its the best one since the last change in configuration of hyperparameters\n",
    "    status_ok = trainer.save_model(epoch, mean_epoch_validation_loss, param['model']['input_size'], \n",
    "                                   param['model']['lstm_layer'], param['model']['n_hidden_lstm'], \n",
    "                                   param['model']['n_hidden_fc'], param[\"model\"][\"sequence_size\"])\n",
    "    if not status_ok:\n",
    "        break\n",
    "\n",
    "# Safe results to csv file\n",
    "df = pd.DataFrame(hist_loss)\n",
    "df.to_csv(param[\"filed_location\"][\"history\"], sep=\";\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "masterarbeit",
   "language": "python",
   "name": "masterarbeit"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
