{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn.utils import clip_grad_norm_\n",
    "from torch.utils.data import DataLoader\n",
    "import torch.optim as optim\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# own Modules \n",
    "from models import LstmMse\n",
    "from data_loader import DataPreperator, DataSet\n",
    "from trainer import Trainer\n",
    "from loss_module import LossModuleMse, LossModuleMle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "hyperparam = {\n",
    "    \"data\" : {\n",
    "        \"stake_training_data\" : 0.75, \n",
    "        \"path\" : '../../../data/vega_shrinkwrapper_original/NewBlade/'\n",
    "    },\n",
    "    \"model\" : {\n",
    "        \"input_size\" : 7,\n",
    "        \"n_hidden\" : [5, 10, 15, 20, 50, 75, 100, 120, 150],\n",
    "        \"sequence_size\" : [5, 10, 15, 20, 25, 30, 40, 55, 50],\n",
    "        \"batch_size\" : 8,\n",
    "        \"lstm_layer\" : [1, 2, 3, 4]\n",
    "    },\n",
    "    \"cycling_lr\" : {\n",
    "        # step_size is the number of training iterations (total samples/batch_size) per half cycle. \n",
    "        # Authors suggest setting step_size 2-8 x training iterations in epoch.\n",
    "        \"step_size\" : [(2048/8)*2, (2048/8)*4, (2048/8)*6],\n",
    "        # Mode can be one of {triangular, triangular2, exp_range}\n",
    "        \"mode\" : [\"triangular\", \"triangular2\", \"exp_range\"],\n",
    "        \"gamma\" : 0.9995,\n",
    "        \"base_lr\" : 0.016, \n",
    "        \"max_lr\" :0.1\n",
    "    },\n",
    "    \"training\": {\n",
    "        \"n_epochs\" : 100,\n",
    "        \"patience\" : 10,\n",
    "    },\n",
    "    \"filed_location\": {\n",
    "        \"trained_model\" : \"../../../models/ML_model/best_model.pt\",\n",
    "        \"history\" : \"../../../visualisation/files/history_ML.csv\"\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "hyperparam = {\n",
    "    \"data\" : {\n",
    "        \"stake_training_data\" : 0.75, \n",
    "        \"path\" : '../../../data/artifical_signals/'\n",
    "    },\n",
    "    \"model\" : {\n",
    "        \"input_size\" : 8,\n",
    "        \"n_hidden\" : 150,\n",
    "        \"sequence_size\" : 50,\n",
    "        \"batch_size\" : 8,\n",
    "        \"lstm_layer\" : 3,\n",
    "        \"gaussian\" : 1\n",
    "    },\n",
    "    \"cycling_lr\" : {\n",
    "        # step_size is the number of training iterations (total samples/batch_size) per half cycle. \n",
    "        # Authors suggest setting step_size 2-8 x training iterations in epoch.\n",
    "        \"step_size\" : (2048/8)*2, \n",
    "        # Mode can be one of {triangular, triangular2, exp_range}\n",
    "        \"mode\" : \"triangular\", \n",
    "        \"gamma\" : 0.9995,\n",
    "        \"base_lr\" : 0.016, \n",
    "        \"max_lr\" :0.1\n",
    "    },\n",
    "    \"training\": {\n",
    "        \"n_epochs\" : 100,\n",
    "        \"patience\" : 10,\n",
    "    },\n",
    "    \"filed_location\": {\n",
    "        \"trained_model\" : \"../../../models/MSE_model/model_artifical_data.pt\",\n",
    "        \"history\" : \"../../visualisation/files/history_MSE_artifical_data.csv\"\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Split raw data into train and validation data and scale it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = DataPreperator(path=hyperparam['data']['path']+'NewBlade_with_sine.csv')\n",
    "train_data, validation_data = train_loader.provide_data(stake_training_data=hyperparam['data']['stake_training_data'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialize Dataset \n",
    "Time series data must be transformed into a structure of samples with `input` and `target` components before it can be used to fit a supervised learning model. <br>\n",
    "For a time series interested in one-step predictions, the observations at prior time steps, so-called lag observations, are used as `input` and the `target` is the observation at the current time step.\n",
    "\n",
    "For example, a univariate series can be expressed as a supervised learning problem with three time steps for `input` and one step as `target`, as follows:\n",
    "\n",
    "|input|target|\n",
    "|-----|------|\n",
    "[1, 2, 3]|[4]\n",
    "[2, 3, 4]|[5]\n",
    "[3, 4, 5]|[6]\n",
    "\n",
    "The Keras deep learning library provides the `TimeseriesGenerator` to automatically transform both univariate and multivariate time series data into such a format. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_train = DataSet(train_data, timesteps=hyperparam[\"model\"][\"sequence_size\"])\n",
    "dataset_validation = DataSet(validation_data, timesteps=hyperparam[\"model\"][\"sequence_size\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialize DataLoader\n",
    "Actually the data has a other size than in the table above because of multivariate time series data and because of using batches. <br>\n",
    "__First dimension__: batch size --> Defines the number of samples that will be propagated through the network simultaneously. <br>\n",
    "__Second dimension:__ timesteps --> Number of sequence which is passed into the LSTM <br>\n",
    "__Third dimension:__ input_dim --> Is the number of features. In this case data from 7 sensors, collected at the same time. <br>\n",
    "\n",
    "![](../../knowledge/pictures/input_shape.png)\n",
    "\n",
    "Data is shuffled because each mini batch is indipendent from each other, but samples of a minibatch are in chronological order."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_loader_training = DataLoader(dataset_train, batch_size=hyperparam[\"model\"][\"batch_size\"], \n",
    "                                  num_workers=1, shuffle=True, drop_last=True)\n",
    "data_loader_validation = DataLoader(dataset_validation, batch_size=hyperparam[\"model\"][\"batch_size\"], \n",
    "                                    num_workers=1, shuffle=True, drop_last=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data of batch: 0\n",
      "Size of input data: torch.Size([8, 50, 7])\n",
      "Size of target data: torch.Size([8, 7])\n",
      "Data of batch: 1\n",
      "Size of input data: torch.Size([8, 50, 7])\n",
      "Size of target data: torch.Size([8, 7])\n"
     ]
    }
   ],
   "source": [
    "for batch_idx, data in enumerate(data_loader_training):\n",
    "    x,y = data\n",
    "    print('Data of batch: {}'.format(batch_idx))\n",
    "    print(\"Size of input data: {}\".format(x.size()))\n",
    "    print(\"Size of target data: {}\".format(y.size()))\n",
    "    if batch_idx >=1: break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define MSE Loss function as torch module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LossModule(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        \"\"\"\n",
    "        In the constructor we instantiate the module and assign them as\n",
    "        member variables.\n",
    "        \"\"\"\n",
    "        super(LossModule, self).__init__()\n",
    "\n",
    "    def forward(self, output, target_data):\n",
    "        \"\"\"\n",
    "        In the forward function we accept a Tensor of input data and we must return\n",
    "        a Tensor of output data. \n",
    "        \"\"\"\n",
    "        # Compute loss\n",
    "        mean_loss = torch.sum((output - target_data)**2, dim=1) / hyperparam[\"model\"][\"input_size\"]\n",
    "        mean_loss = torch.sum(mean_loss) / hyperparam[\"model\"][\"batch_size\"]\n",
    "        return mean_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = LossModuleMse()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialize Neural Network\n",
    "__Parameters for LSTM Modul:__\n",
    "- input_size : The number of expected features in the input x\n",
    "- hidden_size :The number of features in the hidden state h\n",
    "- num_layers : Number of recurrent layers. E.g., setting num_layers=2 would mean stacking two LSTMs together to form a stacked LSTM, with the second LSTM taking in outputs of the first LSTM and computing the final results.\n",
    "- batch_first : If True, then the input __and output__ tensors are provided as (batch, seq, feature).\n",
    "- dropout â€“ If non-zero, introduces a Dropout layer on the outputs of each LSTM layer except the last layer, with dropout probability equal to dropout. Default: 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize model\n",
    "torch.manual_seed(0)\n",
    "model = LstmMse(batch_size=hyperparam['model']['batch_size'], input_dim=hyperparam['model']['input_size'], \n",
    "             n_hidden=hyperparam['model']['n_hidden'], n_layers=hyperparam['model']['lstm_layer'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Loop\n",
    "An epoch consists of a learning cycle over all batches of training data and an evaluation of the most recent model with the testing data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for num_lstm_layer in hyperparam[\"model\"][\"lstm_layer\"]:\n",
    "    for n_hidden in hyperparam[\"model\"][\"n_hidden\"]:\n",
    "        for mode in hyperparam[\"cycling_lr\"][\"mode\"]:\n",
    "            for step_size in hyperparam[\"cycling_lr\"][\"step_size\"]:\n",
    "                print(\"Start model training\")\n",
    "                # Create lists to save training loss and validation loss of each epoch\n",
    "                hist_loss = []\n",
    "                status_ok = True\n",
    "                \n",
    "                for epoch in range(hyperparam['training']['n_epochs']):\n",
    "                    trainer = Trainer(batch_size=hyperparam['model']['batch_size'], input_dim, n_hidden, n_layers, base_lr, max_lr, \n",
    "              step_size_up, mode, gamma, criterion, location_model, location_stats, )\n",
    "                    \n",
    "                    while(status_ok):\n",
    "                        trainer.train()\n",
    "                        mean_epoch_validation_loss = trainer.evaluate()\n",
    "                        status_ok = trainer.save_model(mean_epch_validation_loss)\n",
    "                        \n",
    "                    trainer.save_statistic(hist_loss)         "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start model training\n",
      "-------- epoch_no. 0 finished with eval loss 1.1011186806779159--------\n",
      "Epoch 0: best model saved with loss: 1.1011186806779159\n",
      "   epoch  training  validation\n",
      "0      0  0.721968    1.101119\n"
     ]
    }
   ],
   "source": [
    "# Initialize optimizer and Cyclic Learning Rate\n",
    "# lr=1. because of scheduler (1*learning_rate_schedular)\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=1.)  \n",
    "\n",
    "scheduler = torch.optim.lr_scheduler.CyclicLR(optimizer=optimizer, base_lr=hyperparam['cycling_lr']['base_lr'], \n",
    "                                              max_lr=hyperparam['cycling_lr']['max_lr'], step_size_up=hyperparam['cycling_lr']['step_size'], \n",
    "                                              mode=hyperparam['cycling_lr']['mode'], gamma=hyperparam['cycling_lr']['gamma'])\n",
    "\n",
    "print(\"Start model training\")\n",
    "# Create lists to save training loss and validation loss of each epoch\n",
    "hist_loss = []\n",
    "lr_find_lr = []\n",
    "\n",
    "# Set first comparative value\n",
    "lowest_loss = 99\n",
    "trails = 0\n",
    "\n",
    "for epoch in range(hyperparam['training']['n_epochs']):\n",
    "    # Empty list for recording performance \n",
    "    epoch_training_loss = []\n",
    "    epoch_validation_loss = []\n",
    "    \n",
    "    model.train()\n",
    "    \n",
    "    ##### Training #####\n",
    "    for batch_number, data in enumerate(data_loader_training):\n",
    "        # The LSTM has to be reinitialised, otherwise the LSTM will treat a new batch as a continuation of a sequence.\n",
    "        # When batches of data are independent sequences, then you should reinitialise the hidden state before each batch. \n",
    "        # But if your data is made up of really long sequences and you cut it up into batches making sure that each batch \n",
    "        # follows on from the previous batch, then in that case you wouldnâ€™t reinitialise the hidden state before each batch.\n",
    "        # In the current workflow of class DataProvoider independent sequences are returned. \n",
    "        \n",
    "        input_data, target_data = data\n",
    "        \n",
    "        hidden = model.init_hidden()\n",
    "        \n",
    "        # Zero out gradient, else they will accumulate between minibatches\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # Forward propagation\n",
    "        output = model(input_data, hidden)\n",
    "         \n",
    "        # Calculate loss\n",
    "        criterion = LossModule()\n",
    "        loss = criterion(output, target_data)\n",
    "        epoch_training_loss.append(loss.item())\n",
    "    \n",
    "        # Backward pass\n",
    "        loss.backward()\n",
    "        \n",
    "        # Update parameters\n",
    "        optimizer.step()\n",
    "        \n",
    "        # Update LR\n",
    "        scheduler.step()\n",
    "        lr_step = optimizer.state_dict()[\"param_groups\"][0][\"lr\"]\n",
    "        lr_find_lr.append(lr_step)\n",
    "        \n",
    "    # Save mean of loss over all training iterations\n",
    "    mean_epoch_training_loss = sum(epoch_training_loss) / float(len(epoch_training_loss))\n",
    "    \n",
    "    ##### Evaluation #####\n",
    "    for batch_number, data in enumerate(data_loader_validation):\n",
    "        input_data, target_data = data\n",
    "        model.eval()\n",
    "        hidden = model.init_hidden()\n",
    "        output = model(input_data, hidden)\n",
    "        \n",
    "        # Calculate loss\n",
    "        criterion = LossModule()\n",
    "        loss = criterion(output, target_data)\n",
    "        epoch_validation_loss.append(loss.item())\n",
    "    \n",
    "    # Save mean of loss over all validation iterations\n",
    "    mean_epoch_validation_loss = sum(epoch_validation_loss) / float(len(epoch_validation_loss))\n",
    "    \n",
    "    # Save training and validation loss to history\n",
    "    hist_loss.append({'epoch': epoch, \n",
    "                      'training': mean_epoch_training_loss, \n",
    "                      'validation': mean_epoch_validation_loss})\n",
    "    \n",
    "    print(\"-------- epoch_no. {} finished with eval loss {}--------\".format(epoch, mean_epoch_validation_loss))\n",
    "        \n",
    "    # Check after every evaluation whether the latest model is the best one or not\n",
    "    # If this is the case, set current score to best_score, reset trails and save the model.\n",
    "    if mean_epoch_validation_loss < lowest_loss:\n",
    "        trials = 0\n",
    "        lowest_loss = mean_epoch_validation_loss\n",
    "        torch.save({\n",
    "            'epoch': epoch,\n",
    "            'model_state_dict': model.state_dict(),\n",
    "            'optimizer_state_dict': optimizer.state_dict(),\n",
    "            'loss': mean_epoch_validation_loss\n",
    "        }, hyperparam[\"filed_location\"][\"model\"])\n",
    "        print(\"Epoch {}: best model saved with loss: {}\".format(epoch, mean_epoch_validation_loss))\n",
    "    \n",
    "    # Else: Increase trails by one and start new epoch as long as not too many epochs \n",
    "    # were unsuccessful (controlled by patience)\n",
    "    else:\n",
    "        trials += 1\n",
    "        if trials >= hyperparam['training']['patience'] :\n",
    "            print(f'Early stopping on epoch {epoch}')\n",
    "            break  \n",
    "\n",
    "# Safe results to csv file\n",
    "df = pd.DataFrame(hist_loss)\n",
    "df.to_csv(hyperparam[\"filed_location\"][\"history\"], sep=\";\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualization of cyclic learning rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 504x504 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "x = range(len(lr_find_lr))\n",
    "data = pd.DataFrame(data={'y': lr_find_lr, 'x': x})\n",
    "f, ax = plt.subplots(figsize=(7, 7))\n",
    "sns.lineplot(x=data.x, y=data.y, ax=ax)\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "masterarbeit",
   "language": "python",
   "name": "masterarbeit"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
