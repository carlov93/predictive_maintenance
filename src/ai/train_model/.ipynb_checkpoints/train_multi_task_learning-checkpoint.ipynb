{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader\n",
    "import torch.optim as optim\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# own Modules \n",
    "from models import LstmMultiTaskLearning, AnalysisLayer\n",
    "from data_loader import DataPreperator, DataSet\n",
    "from trainer import TrainerMultiTaskLearning\n",
    "from loss_module import LossMse"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Take care of these things before training:\n",
    "- Select correct path and define droped_features\n",
    "- Change parameter of model\n",
    "- Change step_size in cycling_lr\n",
    "- Change filed_location"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "param = {\n",
    "    \"data\" : {\n",
    "        \"stake_training_data\" : 0.75, \n",
    "        \"path\" : '../../../data/phm_data_challenge/01_M01_DC_preprocessed_multi_task_learning_training.csv' ,\n",
    "    },\n",
    "    \"preprocessing\" : {\n",
    "        \"first_order_difference\" : False,\n",
    "        \"droped_features\": [\"ID\", \"ongoing time\", \"up time\", \"RLU\", \"runnum\", 'FIXTURESHUTTERPOSITION_0.0', \n",
    "                            'FIXTURESHUTTERPOSITION_1.0', 'FIXTURESHUTTERPOSITION_2.0', 'FIXTURESHUTTERPOSITION_3.0',\n",
    "                            'FIXTURESHUTTERPOSITION_255.0', \"ETCHSOURCEUSAGE\", \"ETCHAUXSOURCETIMER\", \n",
    "                            \"ETCHAUX2SOURCETIMER\", \"status\"\n",
    "                           ],\n",
    "        \"features_not_to_scale\": []\n",
    "    },\n",
    "    \"model\" : {\n",
    "        \"input_size\" : 13,\n",
    "        \"n_hidden_lstm\" : 100,\n",
    "        \"sequence_size\" : 100,\n",
    "        \"batch_size\" : 8,\n",
    "        \"lstm_layer\" : 2,\n",
    "        \"n_hidden_fc_pred\": 50,\n",
    "        \"n_hidden_fc_ls\": 5,\n",
    "        \"dropout_rate\": 0.2\n",
    "    },\n",
    "    \"cycling_lr\" : {\n",
    "        \"scheduler_active\" : True, \n",
    "        # step_size is the number of training iterations (total samples/batch_size) per half cycle. \n",
    "        # Authors suggest setting step_size 2-8 x training iterations in epoch.\n",
    "        \"step_size\" : (12500/8)*2, \n",
    "        # Mode can be one of {triangular, triangular2, exp_range}\n",
    "        \"mode\" : \"triangular\", \n",
    "        \"gamma\" : 0.9995,\n",
    "        \"base_lr\" : 0.016, \n",
    "        \"max_lr\" :0.07\n",
    "    },\n",
    "    \"training\": {\n",
    "        \"n_epochs\" : 100,\n",
    "        \"patience\" : 10,\n",
    "    },\n",
    "    \"filed_location\": {\n",
    "        \"trained_model\" : \"../../../models/MSE_multi_learning/phm_data\",\n",
    "        \"history\" : \"../../visualisation/files/history_training/multi_learning.csv\"\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing\n",
    "1. First order difference (if selected)\n",
    "2. Split data into train and validation data\n",
    "3. Scale train and validation data with train's mean and variance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6251\n"
     ]
    }
   ],
   "source": [
    "train_loader = DataPreperator(path=param['data']['path'], \n",
    "                              ignored_features=param['preprocessing']['droped_features'],\n",
    "                              stake_training_data=param['data']['stake_training_data'],\n",
    "                              features_not_to_scale=param['preprocessing']['features_not_to_scale'],\n",
    "                              first_order_difference=param[\"preprocessing\"][\"first_order_difference\"])\n",
    "train_data, validation_data = train_loader.prepare_data()\n",
    "print(len(train_data)+len(validation_data))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mean and variance from scale process (only of continious features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-0.00616403  0.0014136  -0.00775649  0.02678833  0.11496844  0.01260359\n",
      " -0.03450196  0.01716666  0.00792266  0.05113533  0.15137018 -0.15249734\n",
      " -0.06415179 -0.25143178  0.87424645 -0.30155094]\n",
      "[9.54757947e-01 9.65299841e-01 9.79715208e-01 9.95428027e-01\n",
      " 1.20228023e+00 9.84873713e-01 4.43684229e-01 1.01021270e+00\n",
      " 9.70240421e-01 6.98134612e-01 1.44322416e+00 9.29744851e-12\n",
      " 1.95742556e-05 1.38008019e-05 4.47681049e-05 1.81650562e-01]\n"
     ]
    }
   ],
   "source": [
    "mean, var = train_loader.provide_statistics()\n",
    "print(mean)\n",
    "print(var)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialize Dataset \n",
    "Time series data must be transformed into a structure of samples with `input` and `target` components before it can be used to fit a supervised learning model. <br>\n",
    "For a time series interested in one-step predictions, the observations at prior time steps, so-called lag observations, are used as `input` and the `target` is the observation at the current time step.\n",
    "\n",
    "For example, a univariate series can be expressed as a supervised learning problem with three time steps for `input` and one step as `target`, as follows:\n",
    "\n",
    "|input|target|\n",
    "|-----|------|\n",
    "[1, 2, 3]|[4]\n",
    "[2, 3, 4]|[5]\n",
    "[3, 4, 5]|[6]\n",
    "\n",
    "The Keras deep learning library provides the `TimeseriesGenerator` to automatically transform both univariate and multivariate time series data into such a format. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_train = DataSet(train_data, timesteps=param[\"model\"][\"sequence_size\"])\n",
    "dataset_validation = DataSet(validation_data, timesteps=param[\"model\"][\"sequence_size\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialize DataLoader\n",
    "Actually the data has a other size than in the table above because of multivariate time series data and because of using batches. <br>\n",
    "__First dimension__: batch size --> Defines the number of samples that will be propagated through the network simultaneously. <br>\n",
    "__Second dimension:__ timesteps --> Number of sequence which is passed into the LSTM <br>\n",
    "__Third dimension:__ input_dim --> Is the number of features. In this case data from 7 sensors, collected at the same time. <br>\n",
    "\n",
    "![](../../../knowledge/pictures/input_shape.png)\n",
    "\n",
    "Data is shuffled because each mini batch is indipendent from each other, but samples of a minibatch are in chronological order."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_loader_training = DataLoader(dataset_train, \n",
    "                                  batch_size=param[\"model\"][\"batch_size\"], \n",
    "                                  num_workers=4, \n",
    "                                  shuffle=True, \n",
    "                                  drop_last=True\n",
    "                                 )\n",
    "data_loader_validation = DataLoader(dataset_validation, \n",
    "                                    batch_size=param[\"model\"][\"batch_size\"], \n",
    "                                    num_workers=4, \n",
    "                                    shuffle=True, \n",
    "                                    drop_last=True\n",
    "                                   )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data of batch: 0\n",
      "Size of input data: torch.Size([8, 50, 21])\n",
      "Size of target data: torch.Size([8, 21])\n",
      "Data of batch: 1\n",
      "Size of input data: torch.Size([8, 50, 21])\n",
      "Size of target data: torch.Size([8, 21])\n"
     ]
    }
   ],
   "source": [
    "for batch_idx, data in enumerate(data_loader_training):\n",
    "    x,y = data\n",
    "    print('Data of batch: {}'.format(batch_idx))\n",
    "    print(\"Size of input data: {}\".format(x.size()))\n",
    "    print(\"Size of target data: {}\".format(y.size()))\n",
    "    if batch_idx >=1: break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialize Neural Network\n",
    "__Parameters for LSTM Modul:__\n",
    "- input_size : The number of expected features in the input x\n",
    "- hidden_size :The number of features in the hidden state h\n",
    "- num_layers : Number of recurrent layers. E.g., setting num_layers=2 would mean stacking two LSTMs together to form a stacked LSTM, with the second LSTM taking in outputs of the first LSTM and computing the final results.\n",
    "- batch_first : If True, then the input __and output__ tensors are provided as (batch, seq, feature).\n",
    "- dropout â€“ If non-zero, introduces a Dropout layer on the outputs of each LSTM layer except the last layer, with dropout probability equal to dropout. Default: 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(0)\n",
    "model = LstmMultiTaskLearning(batch_size=param['model']['batch_size'], \n",
    "                              input_dim=param['model']['input_size'], \n",
    "                              n_hidden_lstm=param['model']['n_hidden_lstm'], \n",
    "                              n_layers=param['model']['lstm_layer'],\n",
    "                              dropout_rate= param['model']['dropout_rate'],\n",
    "                              n_hidden_fc_prediction=param['model']['n_hidden_fc_pred'], \n",
    "                              n_hidden_fc_ls_analysis=param['model']['n_hidden_fc_ls']      \n",
    "                              )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define MSE Loss function as torch module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = LossMse(param[\"model\"][\"input_size\"], param[\"model\"][\"batch_size\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialize Optimizer and Cyclic Learning Rate Scheduler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.SGD(model.parameters(), lr=1.)  \n",
    "scheduler = torch.optim.lr_scheduler.CyclicLR(optimizer=optimizer, \n",
    "                                              base_lr=param['cycling_lr']['base_lr'], \n",
    "                                              max_lr=param['cycling_lr']['max_lr'], \n",
    "                                              step_size_up=param['cycling_lr']['step_size'], \n",
    "                                              mode=param['cycling_lr']['mode'],\n",
    "                                              gamma=param['cycling_lr']['gamma']\n",
    "                                             )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialize Trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = TrainerMultiTaskLearning(model=model,\n",
    "                                   optimizer=optimizer,\n",
    "                                   scheduler=scheduler,\n",
    "                                   scheduler_active = param[\"cycling_lr\"][\"scheduler_active\"],\n",
    "                                   criterion=criterion, \n",
    "                                   location_model=param[\"filed_location\"][\"trained_model\"], \n",
    "                                   location_stats=param[\"filed_location\"][\"history\"], \n",
    "                                   patience=param['training']['patience']\n",
    "                                   )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Loop\n",
    "An epoch consists of a learning cycle over all batches of training data and an evaluation of the most recent model with the testing data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 0.1002,  0.1196, -0.0131,  0.1405, -0.0000],\n",
      "        [ 0.1552,  0.1354, -0.0871,  0.1141, -0.1304],\n",
      "        [ 0.1631,  0.1788, -0.0000,  0.1350, -0.1396],\n",
      "        [ 0.1008,  0.0000, -0.0000,  0.1090, -0.2126],\n",
      "        [ 0.1420,  0.1547, -0.0291,  0.1348, -0.1434],\n",
      "        [ 0.0685,  0.0983, -0.0287,  0.1297, -0.0000],\n",
      "        [ 0.1123,  0.1329, -0.0579,  0.1191, -0.1446],\n",
      "        [ 0.0937,  0.0900, -0.0404,  0.1008, -0.0000]], grad_fn=<TanhBackward>)\n",
      "----------\n",
      "tensor([[ 0.1002,  0.1196, -0.0131,  0.1405, -0.0000],\n",
      "        [ 0.1552,  0.1354, -0.0871,  0.1141, -0.1304],\n",
      "        [ 0.1631,  0.1788, -0.0000,  0.1350, -0.1396],\n",
      "        [ 0.1008,  0.0000, -0.0000,  0.1090, -0.2126],\n",
      "        [ 0.1420,  0.1547, -0.0291,  0.1348, -0.1434],\n",
      "        [ 0.0685,  0.0983, -0.0287,  0.1297, -0.0000],\n",
      "        [ 0.1123,  0.1329, -0.0579,  0.1191, -0.1446],\n",
      "        [ 0.0937,  0.0900, -0.0404,  0.1008, -0.0000]], grad_fn=<TanhBackward>)\n",
      "tensor([[ 0.0000,  0.1462, -0.0000,  0.1399, -0.1491],\n",
      "        [ 0.0727,  0.0957, -0.0000,  0.0000, -0.2093],\n",
      "        [ 0.0000,  0.1410, -0.0118,  0.1416, -0.1594],\n",
      "        [ 0.0000,  0.1327, -0.0739,  0.1291, -0.1773],\n",
      "        [ 0.0729,  0.0884, -0.0000,  0.0000, -0.0000],\n",
      "        [ 0.1415,  0.1453, -0.0000,  0.1345, -0.1617],\n",
      "        [ 0.0752,  0.1067, -0.0266,  0.1280, -0.2075],\n",
      "        [ 0.1165,  0.1075, -0.0307,  0.1212, -0.0000]], grad_fn=<TanhBackward>)\n",
      "----------\n",
      "tensor([[ 0.0000,  0.1462, -0.0000,  0.1399, -0.1491],\n",
      "        [ 0.0727,  0.0957, -0.0000,  0.0000, -0.2093],\n",
      "        [ 0.0000,  0.1410, -0.0118,  0.1416, -0.1594],\n",
      "        [ 0.0000,  0.1327, -0.0739,  0.1291, -0.1773],\n",
      "        [ 0.0729,  0.0884, -0.0000,  0.0000, -0.0000],\n",
      "        [ 0.1415,  0.1453, -0.0000,  0.1345, -0.1617],\n",
      "        [ 0.0752,  0.1067, -0.0266,  0.1280, -0.2075],\n",
      "        [ 0.1165,  0.1075, -0.0307,  0.1212, -0.0000]], grad_fn=<TanhBackward>)\n",
      "tensor([[ 0.0686,  0.0000, -0.0000,  0.0000, -0.2064],\n",
      "        [ 0.0000,  0.0000, -0.0489,  0.1218, -0.2060],\n",
      "        [ 0.0805,  0.1058, -0.0377,  0.1387, -0.2026],\n",
      "        [ 0.0890,  0.1000, -0.0364,  0.1341, -0.2074],\n",
      "        [ 0.1177,  0.1254, -0.0764,  0.1451, -0.1671],\n",
      "        [ 0.0879,  0.1094, -0.0000,  0.1428, -0.2014],\n",
      "        [ 0.1099,  0.1036, -0.0962,  0.1345, -0.1711],\n",
      "        [ 0.0000,  0.0000, -0.0000,  0.1356, -0.1812]], grad_fn=<TanhBackward>)\n",
      "----------\n",
      "tensor([[ 0.0686,  0.0000, -0.0000,  0.0000, -0.2064],\n",
      "        [ 0.0000,  0.0000, -0.0489,  0.1218, -0.2060],\n",
      "        [ 0.0805,  0.1058, -0.0377,  0.1387, -0.2026],\n",
      "        [ 0.0890,  0.1000, -0.0364,  0.1341, -0.2074],\n",
      "        [ 0.1177,  0.1254, -0.0764,  0.1451, -0.1671],\n",
      "        [ 0.0879,  0.1094, -0.0000,  0.1428, -0.2014],\n",
      "        [ 0.1099,  0.1036, -0.0962,  0.1345, -0.1711],\n",
      "        [ 0.0000,  0.0000, -0.0000,  0.1356, -0.1812]], grad_fn=<TanhBackward>)\n",
      "tensor([[ 0.0858,  0.1062, -0.0452,  0.0000, -0.2058],\n",
      "        [ 0.1030,  0.1069, -0.0430,  0.1312, -0.1987],\n",
      "        [ 0.1271,  0.1442, -0.0410,  0.1160, -0.1532],\n",
      "        [ 0.0851,  0.1097, -0.0368,  0.1313, -0.2003],\n",
      "        [ 0.0000,  0.1375, -0.0032,  0.1494, -0.1975],\n",
      "        [ 0.0000,  0.1311, -0.0000,  0.0000, -0.1506],\n",
      "        [ 0.1021,  0.0926, -0.0567,  0.0960, -0.2017],\n",
      "        [ 0.1220,  0.1374, -0.0368,  0.1199, -0.0000]], grad_fn=<TanhBackward>)\n",
      "----------\n",
      "tensor([[ 0.0858,  0.1062, -0.0452,  0.0000, -0.2058],\n",
      "        [ 0.1030,  0.1069, -0.0430,  0.1312, -0.1987],\n",
      "        [ 0.1271,  0.1442, -0.0410,  0.1160, -0.1532],\n",
      "        [ 0.0851,  0.1097, -0.0368,  0.1313, -0.2003],\n",
      "        [ 0.0000,  0.1375, -0.0032,  0.1494, -0.1975],\n",
      "        [ 0.0000,  0.1311, -0.0000,  0.0000, -0.1506],\n",
      "        [ 0.1021,  0.0926, -0.0567,  0.0960, -0.2017],\n",
      "        [ 0.1220,  0.1374, -0.0368,  0.1199, -0.0000]], grad_fn=<TanhBackward>)\n",
      "tensor([[ 0.0971,  0.0000, -0.0481,  0.0000, -0.0000],\n",
      "        [ 0.1318,  0.0000, -0.0252,  0.1382, -0.1497],\n",
      "        [ 0.0968,  0.0984, -0.0474,  0.0000, -0.2140],\n",
      "        [ 0.0000,  0.1023, -0.0498,  0.1082, -0.0000],\n",
      "        [ 0.0898,  0.1052, -0.0352,  0.1319, -0.0000],\n",
      "        [ 0.0803,  0.0856, -0.0317,  0.0910, -0.2112],\n",
      "        [ 0.1326,  0.1425, -0.0809,  0.1122, -0.1536],\n",
      "        [ 0.0937,  0.1237, -0.0591,  0.1187, -0.0000]], grad_fn=<TanhBackward>)\n",
      "----------\n",
      "tensor([[ 0.0971,  0.0000, -0.0481,  0.0000, -0.0000],\n",
      "        [ 0.1318,  0.0000, -0.0252,  0.1382, -0.1497],\n",
      "        [ 0.0968,  0.0984, -0.0474,  0.0000, -0.2140],\n",
      "        [ 0.0000,  0.1023, -0.0498,  0.1082, -0.0000],\n",
      "        [ 0.0898,  0.1052, -0.0352,  0.1319, -0.0000],\n",
      "        [ 0.0803,  0.0856, -0.0317,  0.0910, -0.2112],\n",
      "        [ 0.1326,  0.1425, -0.0809,  0.1122, -0.1536],\n",
      "        [ 0.0937,  0.1237, -0.0591,  0.1187, -0.0000]], grad_fn=<TanhBackward>)\n",
      "tensor([[ 0.1143,  0.0000, -0.1052,  0.1141, -0.1773],\n",
      "        [ 0.0000,  0.1164, -0.0317,  0.1347, -0.2041],\n",
      "        [ 0.0000,  0.1146, -0.0000,  0.0000, -0.1776],\n",
      "        [ 0.0917,  0.1033, -0.0526,  0.1196, -0.2001],\n",
      "        [ 0.0923,  0.0953, -0.0000,  0.0988, -0.0000],\n",
      "        [ 0.0948,  0.1173, -0.0000,  0.1558, -0.0000],\n",
      "        [ 0.1317,  0.1266, -0.0696,  0.1057, -0.0000],\n",
      "        [ 0.0727,  0.1027, -0.0387,  0.1274, -0.2025]], grad_fn=<TanhBackward>)\n",
      "----------\n",
      "tensor([[ 0.1143,  0.0000, -0.1052,  0.1141, -0.1773],\n",
      "        [ 0.0000,  0.1164, -0.0317,  0.1347, -0.2041],\n",
      "        [ 0.0000,  0.1146, -0.0000,  0.0000, -0.1776],\n",
      "        [ 0.0917,  0.1033, -0.0526,  0.1196, -0.2001],\n",
      "        [ 0.0923,  0.0953, -0.0000,  0.0988, -0.0000],\n",
      "        [ 0.0948,  0.1173, -0.0000,  0.1558, -0.0000],\n",
      "        [ 0.1317,  0.1266, -0.0696,  0.1057, -0.0000],\n",
      "        [ 0.0727,  0.1027, -0.0387,  0.1274, -0.2025]], grad_fn=<TanhBackward>)\n",
      "tensor([[ 0.0883,  0.0944, -0.0659,  0.1169, -0.1842],\n",
      "        [ 0.1036,  0.0903, -0.0000,  0.0966, -0.2116],\n",
      "        [ 0.0861,  0.0000, -0.0510,  0.0000, -0.1854],\n",
      "        [ 0.0882,  0.1206, -0.0434,  0.1287, -0.1872],\n",
      "        [ 0.1050,  0.0899, -0.0374,  0.1362, -0.1811],\n",
      "        [ 0.0834,  0.0998, -0.0000,  0.0898, -0.2113],\n",
      "        [ 0.0000,  0.0931, -0.0456,  0.1227, -0.2125],\n",
      "        [ 0.1539,  0.1478, -0.0632,  0.1157, -0.1417]], grad_fn=<TanhBackward>)\n",
      "----------\n",
      "tensor([[ 0.0883,  0.0944, -0.0659,  0.1169, -0.1842],\n",
      "        [ 0.1036,  0.0903, -0.0000,  0.0966, -0.2116],\n",
      "        [ 0.0861,  0.0000, -0.0510,  0.0000, -0.1854],\n",
      "        [ 0.0882,  0.1206, -0.0434,  0.1287, -0.1872],\n",
      "        [ 0.1050,  0.0899, -0.0374,  0.1362, -0.1811],\n",
      "        [ 0.0834,  0.0998, -0.0000,  0.0898, -0.2113],\n",
      "        [ 0.0000,  0.0931, -0.0456,  0.1227, -0.2125],\n",
      "        [ 0.1539,  0.1478, -0.0632,  0.1157, -0.1417]], grad_fn=<TanhBackward>)\n",
      "tensor([[ 0.1395,  0.1336, -0.0540,  0.1250, -0.1623],\n",
      "        [ 0.1731,  0.0000, -0.1197,  0.1254, -0.0000],\n",
      "        [ 0.0978,  0.1079, -0.0000,  0.1303, -0.1962],\n",
      "        [ 0.0000,  0.0838, -0.0000,  0.0000, -0.1933],\n",
      "        [ 0.1594,  0.1517, -0.0000,  0.0000, -0.0000],\n",
      "        [ 0.0842,  0.0920, -0.0432,  0.1194, -0.2083],\n",
      "        [ 0.1008,  0.1175, -0.0000,  0.1343, -0.2015],\n",
      "        [ 0.1540,  0.1578, -0.0000,  0.0000, -0.1478]], grad_fn=<TanhBackward>)\n",
      "----------\n",
      "tensor([[ 0.1395,  0.1336, -0.0540,  0.1250, -0.1623],\n",
      "        [ 0.1731,  0.0000, -0.1197,  0.1254, -0.0000],\n",
      "        [ 0.0978,  0.1079, -0.0000,  0.1303, -0.1962],\n",
      "        [ 0.0000,  0.0838, -0.0000,  0.0000, -0.1933],\n",
      "        [ 0.1594,  0.1517, -0.0000,  0.0000, -0.0000],\n",
      "        [ 0.0842,  0.0920, -0.0432,  0.1194, -0.2083],\n",
      "        [ 0.1008,  0.1175, -0.0000,  0.1343, -0.2015],\n",
      "        [ 0.1540,  0.1578, -0.0000,  0.0000, -0.1478]], grad_fn=<TanhBackward>)\n",
      "tensor([[ 0.0926,  0.0000, -0.0233,  0.1363, -0.2055],\n",
      "        [ 0.0813,  0.1020, -0.0000,  0.0000, -0.2106],\n",
      "        [ 0.0714,  0.0930, -0.0264,  0.1167, -0.2039],\n",
      "        [ 0.0950,  0.1089, -0.0000,  0.1201, -0.1836],\n",
      "        [ 0.1515,  0.1647, -0.0000,  0.1260, -0.1472],\n",
      "        [ 0.0524,  0.0918, -0.0567,  0.1124, -0.2149],\n",
      "        [ 0.1619,  0.0000, -0.0000,  0.0905, -0.1420],\n",
      "        [ 0.0000,  0.0969, -0.0382,  0.1256, -0.1963]], grad_fn=<TanhBackward>)\n",
      "----------\n",
      "tensor([[ 0.0926,  0.0000, -0.0233,  0.1363, -0.2055],\n",
      "        [ 0.0813,  0.1020, -0.0000,  0.0000, -0.2106],\n",
      "        [ 0.0714,  0.0930, -0.0264,  0.1167, -0.2039],\n",
      "        [ 0.0950,  0.1089, -0.0000,  0.1201, -0.1836],\n",
      "        [ 0.1515,  0.1647, -0.0000,  0.1260, -0.1472],\n",
      "        [ 0.0524,  0.0918, -0.0567,  0.1124, -0.2149],\n",
      "        [ 0.1619,  0.0000, -0.0000,  0.0905, -0.1420],\n",
      "        [ 0.0000,  0.0969, -0.0382,  0.1256, -0.1963]], grad_fn=<TanhBackward>)\n",
      "tensor([[ 0.0696,  0.0882, -0.0450,  0.1213, -0.2136],\n",
      "        [ 0.0971,  0.0804, -0.0337,  0.1220, -0.1826],\n",
      "        [ 0.1313,  0.0000, -0.0372,  0.1396, -0.0000],\n",
      "        [ 0.1510,  0.1550, -0.0478,  0.1244, -0.1650],\n",
      "        [ 0.0000,  0.1520, -0.1296,  0.1026, -0.1213],\n",
      "        [ 0.0861,  0.1060, -0.0000,  0.1239, -0.1876],\n",
      "        [ 0.1316,  0.1387, -0.0561,  0.1065, -0.1294],\n",
      "        [ 0.1104,  0.1123, -0.0000,  0.1344, -0.1929]], grad_fn=<TanhBackward>)\n",
      "----------\n",
      "tensor([[ 0.0696,  0.0882, -0.0450,  0.1213, -0.2136],\n",
      "        [ 0.0971,  0.0804, -0.0337,  0.1220, -0.1826],\n",
      "        [ 0.1313,  0.0000, -0.0372,  0.1396, -0.0000],\n",
      "        [ 0.1510,  0.1550, -0.0478,  0.1244, -0.1650],\n",
      "        [ 0.0000,  0.1520, -0.1296,  0.1026, -0.1213],\n",
      "        [ 0.0861,  0.1060, -0.0000,  0.1239, -0.1876],\n",
      "        [ 0.1316,  0.1387, -0.0561,  0.1065, -0.1294],\n",
      "        [ 0.1104,  0.1123, -0.0000,  0.1344, -0.1929]], grad_fn=<TanhBackward>)\n",
      "tensor([[ 0.0000,  0.0858, -0.0964,  0.0974, -0.1662],\n",
      "        [ 0.0000,  0.0943, -0.0533,  0.0000, -0.2091],\n",
      "        [ 0.0895,  0.0932, -0.0464,  0.0840, -0.2178],\n",
      "        [ 0.0878,  0.0000, -0.0000,  0.1039, -0.1716],\n",
      "        [ 0.1562,  0.1460, -0.0556,  0.1103, -0.1546],\n",
      "        [ 0.0830,  0.0891, -0.0475,  0.0900, -0.0000],\n",
      "        [ 0.0971,  0.1030, -0.0292,  0.1271, -0.2126],\n",
      "        [ 0.1821,  0.1577, -0.1025,  0.0893, -0.1465]], grad_fn=<TanhBackward>)\n",
      "----------\n",
      "tensor([[ 0.0000,  0.0858, -0.0964,  0.0974, -0.1662],\n",
      "        [ 0.0000,  0.0943, -0.0533,  0.0000, -0.2091],\n",
      "        [ 0.0895,  0.0932, -0.0464,  0.0840, -0.2178],\n",
      "        [ 0.0878,  0.0000, -0.0000,  0.1039, -0.1716],\n",
      "        [ 0.1562,  0.1460, -0.0556,  0.1103, -0.1546],\n",
      "        [ 0.0830,  0.0891, -0.0475,  0.0900, -0.0000],\n",
      "        [ 0.0971,  0.1030, -0.0292,  0.1271, -0.2126],\n",
      "        [ 0.1821,  0.1577, -0.1025,  0.0893, -0.1465]], grad_fn=<TanhBackward>)\n",
      "tensor([[ 0.0765,  0.0874, -0.0415,  0.0000, -0.2048],\n",
      "        [ 0.0740,  0.0920, -0.0405,  0.0000, -0.0000],\n",
      "        [ 0.0798,  0.0980, -0.0548,  0.1074, -0.0000],\n",
      "        [ 0.0000,  0.0000, -0.0779,  0.0000, -0.1559],\n",
      "        [ 0.1006,  0.0921, -0.0000,  0.0795, -0.0000],\n",
      "        [ 0.1480,  0.1571, -0.0750,  0.1195, -0.1467],\n",
      "        [ 0.1395,  0.1485, -0.0654,  0.1219, -0.1439],\n",
      "        [ 0.0889,  0.1072, -0.0000,  0.1341, -0.2031]], grad_fn=<TanhBackward>)\n",
      "----------\n",
      "tensor([[ 0.0765,  0.0874, -0.0415,  0.0000, -0.2048],\n",
      "        [ 0.0740,  0.0920, -0.0405,  0.0000, -0.0000],\n",
      "        [ 0.0798,  0.0980, -0.0548,  0.1074, -0.0000],\n",
      "        [ 0.0000,  0.0000, -0.0779,  0.0000, -0.1559],\n",
      "        [ 0.1006,  0.0921, -0.0000,  0.0795, -0.0000],\n",
      "        [ 0.1480,  0.1571, -0.0750,  0.1195, -0.1467],\n",
      "        [ 0.1395,  0.1485, -0.0654,  0.1219, -0.1439],\n",
      "        [ 0.0889,  0.1072, -0.0000,  0.1341, -0.2031]], grad_fn=<TanhBackward>)\n",
      "tensor([[ 0.0760,  0.0956, -0.0428,  0.1218, -0.0000],\n",
      "        [ 0.0747,  0.0964, -0.0000,  0.1235, -0.2179],\n",
      "        [ 0.1028,  0.1088, -0.0359,  0.1359, -0.2086],\n",
      "        [ 0.1330,  0.1340, -0.0900,  0.1090, -0.1660],\n",
      "        [ 0.0817,  0.0985, -0.0471,  0.0000, -0.2076],\n",
      "        [ 0.0959,  0.0000, -0.0000,  0.0812, -0.2305],\n",
      "        [ 0.1454,  0.0000, -0.0882,  0.0837, -0.1500],\n",
      "        [ 0.1698,  0.0000, -0.0558,  0.1252, -0.1544]], grad_fn=<TanhBackward>)\n",
      "----------\n",
      "tensor([[ 0.0760,  0.0956, -0.0428,  0.1218, -0.0000],\n",
      "        [ 0.0747,  0.0964, -0.0000,  0.1235, -0.2179],\n",
      "        [ 0.1028,  0.1088, -0.0359,  0.1359, -0.2086],\n",
      "        [ 0.1330,  0.1340, -0.0900,  0.1090, -0.1660],\n",
      "        [ 0.0817,  0.0985, -0.0471,  0.0000, -0.2076],\n",
      "        [ 0.0959,  0.0000, -0.0000,  0.0812, -0.2305],\n",
      "        [ 0.1454,  0.0000, -0.0882,  0.0837, -0.1500],\n",
      "        [ 0.1698,  0.0000, -0.0558,  0.1252, -0.1544]], grad_fn=<TanhBackward>)\n",
      "tensor([[ 0.0917,  0.1099, -0.0564,  0.1018, -0.1913],\n",
      "        [ 0.0660,  0.0754, -0.0532,  0.1041, -0.0000],\n",
      "        [ 0.0000,  0.0948, -0.0449,  0.1107, -0.2248],\n",
      "        [ 0.0664,  0.0964, -0.0593,  0.1034, -0.2102],\n",
      "        [ 0.0928,  0.1039, -0.0613,  0.0846, -0.2033],\n",
      "        [ 0.0948,  0.1114, -0.0670,  0.1069, -0.1840],\n",
      "        [ 0.0848,  0.0968, -0.0000,  0.1067, -0.2051],\n",
      "        [ 0.0000,  0.1622, -0.0630,  0.1295, -0.1589]], grad_fn=<TanhBackward>)\n",
      "----------\n",
      "tensor([[ 0.0917,  0.1099, -0.0564,  0.1018, -0.1913],\n",
      "        [ 0.0660,  0.0754, -0.0532,  0.1041, -0.0000],\n",
      "        [ 0.0000,  0.0948, -0.0449,  0.1107, -0.2248],\n",
      "        [ 0.0664,  0.0964, -0.0593,  0.1034, -0.2102],\n",
      "        [ 0.0928,  0.1039, -0.0613,  0.0846, -0.2033],\n",
      "        [ 0.0948,  0.1114, -0.0670,  0.1069, -0.1840],\n",
      "        [ 0.0848,  0.0968, -0.0000,  0.1067, -0.2051],\n",
      "        [ 0.0000,  0.1622, -0.0630,  0.1295, -0.1589]], grad_fn=<TanhBackward>)\n",
      "tensor([[ 0.0632,  0.0800, -0.0562,  0.1058, -0.2112],\n",
      "        [ 0.1930,  0.1552, -0.0000,  0.0000, -0.1394],\n",
      "        [ 0.0629,  0.0826, -0.0000,  0.1057, -0.2194],\n",
      "        [ 0.1045,  0.0938, -0.0504,  0.0939, -0.2247],\n",
      "        [ 0.0652,  0.0789, -0.0654,  0.0874, -0.2243],\n",
      "        [ 0.1592,  0.1579, -0.0879,  0.0000, -0.1550],\n",
      "        [ 0.1032,  0.1062, -0.0488,  0.0000, -0.2138],\n",
      "        [ 0.0739,  0.0945, -0.0753,  0.0000, -0.2168]], grad_fn=<TanhBackward>)\n",
      "----------\n",
      "tensor([[ 0.0632,  0.0800, -0.0562,  0.1058, -0.2112],\n",
      "        [ 0.1930,  0.1552, -0.0000,  0.0000, -0.1394],\n",
      "        [ 0.0629,  0.0826, -0.0000,  0.1057, -0.2194],\n",
      "        [ 0.1045,  0.0938, -0.0504,  0.0939, -0.2247],\n",
      "        [ 0.0652,  0.0789, -0.0654,  0.0874, -0.2243],\n",
      "        [ 0.1592,  0.1579, -0.0879,  0.0000, -0.1550],\n",
      "        [ 0.1032,  0.1062, -0.0488,  0.0000, -0.2138],\n",
      "        [ 0.0739,  0.0945, -0.0753,  0.0000, -0.2168]], grad_fn=<TanhBackward>)\n",
      "tensor([[ 0.0805,  0.0895, -0.0751,  0.0000, -0.0000],\n",
      "        [ 0.0842,  0.0661, -0.0736,  0.0666, -0.2292],\n",
      "        [ 0.0861,  0.0799, -0.0701,  0.0000, -0.2283],\n",
      "        [ 0.0673,  0.0963, -0.0674,  0.0921, -0.0000],\n",
      "        [ 0.1505,  0.1483, -0.0789,  0.0874, -0.1519],\n",
      "        [ 0.0000,  0.0755, -0.0620,  0.0696, -0.0000],\n",
      "        [ 0.0000,  0.1589, -0.1073,  0.0929, -0.1603],\n",
      "        [ 0.1585,  0.0000, -0.0952,  0.0985, -0.1550]], grad_fn=<TanhBackward>)\n",
      "----------\n",
      "tensor([[ 0.0805,  0.0895, -0.0751,  0.0000, -0.0000],\n",
      "        [ 0.0842,  0.0661, -0.0736,  0.0666, -0.2292],\n",
      "        [ 0.0861,  0.0799, -0.0701,  0.0000, -0.2283],\n",
      "        [ 0.0673,  0.0963, -0.0674,  0.0921, -0.0000],\n",
      "        [ 0.1505,  0.1483, -0.0789,  0.0874, -0.1519],\n",
      "        [ 0.0000,  0.0755, -0.0620,  0.0696, -0.0000],\n",
      "        [ 0.0000,  0.1589, -0.1073,  0.0929, -0.1603],\n",
      "        [ 0.1585,  0.0000, -0.0952,  0.0985, -0.1550]], grad_fn=<TanhBackward>)\n",
      "tensor([[ 0.1018,  0.1086, -0.0765,  0.0679, -0.1963],\n",
      "        [ 0.0902,  0.1005, -0.0719,  0.0967, -0.2074],\n",
      "        [ 0.0707,  0.0759, -0.0847,  0.0819, -0.0000],\n",
      "        [ 0.0706,  0.0866, -0.0000,  0.0949, -0.2192],\n",
      "        [ 0.1632,  0.1577, -0.0959,  0.0000, -0.1442],\n",
      "        [ 0.0930,  0.0826, -0.0536,  0.0704, -0.2296],\n",
      "        [ 0.1499,  0.1397, -0.0863,  0.0727, -0.1534],\n",
      "        [ 0.0966,  0.0906, -0.0808,  0.1065, -0.2092]], grad_fn=<TanhBackward>)\n",
      "----------\n",
      "tensor([[ 0.1018,  0.1086, -0.0765,  0.0679, -0.1963],\n",
      "        [ 0.0902,  0.1005, -0.0719,  0.0967, -0.2074],\n",
      "        [ 0.0707,  0.0759, -0.0847,  0.0819, -0.0000],\n",
      "        [ 0.0706,  0.0866, -0.0000,  0.0949, -0.2192],\n",
      "        [ 0.1632,  0.1577, -0.0959,  0.0000, -0.1442],\n",
      "        [ 0.0930,  0.0826, -0.0536,  0.0704, -0.2296],\n",
      "        [ 0.1499,  0.1397, -0.0863,  0.0727, -0.1534],\n",
      "        [ 0.0966,  0.0906, -0.0808,  0.1065, -0.2092]], grad_fn=<TanhBackward>)\n",
      "tensor([[ 0.1525,  0.1285, -0.0935,  0.0000, -0.1699],\n",
      "        [ 0.0000,  0.0873, -0.0652,  0.0952, -0.2236],\n",
      "        [ 0.0873,  0.0767, -0.0937,  0.0856, -0.2060],\n",
      "        [ 0.0914,  0.0863, -0.0846,  0.0579, -0.2321],\n",
      "        [ 0.0982,  0.0815, -0.0411,  0.0704, -0.2375],\n",
      "        [ 0.0807,  0.0708, -0.0735,  0.0588, -0.0000],\n",
      "        [ 0.1545,  0.0000, -0.0926,  0.1023, -0.1532],\n",
      "        [ 0.0677,  0.0000, -0.0736,  0.0785, -0.0000]], grad_fn=<TanhBackward>)\n",
      "----------\n",
      "tensor([[ 0.1525,  0.1285, -0.0935,  0.0000, -0.1699],\n",
      "        [ 0.0000,  0.0873, -0.0652,  0.0952, -0.2236],\n",
      "        [ 0.0873,  0.0767, -0.0937,  0.0856, -0.2060],\n",
      "        [ 0.0914,  0.0863, -0.0846,  0.0579, -0.2321],\n",
      "        [ 0.0982,  0.0815, -0.0411,  0.0704, -0.2375],\n",
      "        [ 0.0807,  0.0708, -0.0735,  0.0588, -0.0000],\n",
      "        [ 0.1545,  0.0000, -0.0926,  0.1023, -0.1532],\n",
      "        [ 0.0677,  0.0000, -0.0736,  0.0785, -0.0000]], grad_fn=<TanhBackward>)\n",
      "tensor([[ 0.1032,  0.0000, -0.0895,  0.0461, -0.2205],\n",
      "        [ 0.0984,  0.0972, -0.0839,  0.0862, -0.1968],\n",
      "        [ 0.0732,  0.0764, -0.0850,  0.0000, -0.2114],\n",
      "        [ 0.1317,  0.1268, -0.1009,  0.0799, -0.1993],\n",
      "        [ 0.0613,  0.0595, -0.0651,  0.0920, -0.2202],\n",
      "        [ 0.0000,  0.0772, -0.0778,  0.0000, -0.2282],\n",
      "        [ 0.0000,  0.1050, -0.0577,  0.0952, -0.2280],\n",
      "        [ 0.0961,  0.0796, -0.0849,  0.0478, -0.2252]], grad_fn=<TanhBackward>)\n",
      "----------\n",
      "tensor([[ 0.1032,  0.0000, -0.0895,  0.0461, -0.2205],\n",
      "        [ 0.0984,  0.0972, -0.0839,  0.0862, -0.1968],\n",
      "        [ 0.0732,  0.0764, -0.0850,  0.0000, -0.2114],\n",
      "        [ 0.1317,  0.1268, -0.1009,  0.0799, -0.1993],\n",
      "        [ 0.0613,  0.0595, -0.0651,  0.0920, -0.2202],\n",
      "        [ 0.0000,  0.0772, -0.0778,  0.0000, -0.2282],\n",
      "        [ 0.0000,  0.1050, -0.0577,  0.0952, -0.2280],\n",
      "        [ 0.0961,  0.0796, -0.0849,  0.0478, -0.2252]], grad_fn=<TanhBackward>)\n",
      "tensor([[ 0.0738,  0.0750, -0.0562,  0.0910, -0.0000],\n",
      "        [ 0.0775,  0.0897, -0.0896,  0.0697, -0.2221],\n",
      "        [ 0.1041,  0.0000, -0.0000,  0.0448, -0.0000],\n",
      "        [ 0.1065,  0.0867, -0.0936,  0.0802, -0.2008],\n",
      "        [ 0.1655,  0.0000, -0.0000,  0.0980, -0.1776],\n",
      "        [ 0.0000,  0.0762, -0.0576,  0.0703, -0.2378],\n",
      "        [ 0.0739,  0.0884, -0.0657,  0.0853, -0.2135],\n",
      "        [ 0.0000,  0.1602, -0.0000,  0.0700, -0.1523]], grad_fn=<TanhBackward>)\n",
      "----------\n",
      "tensor([[ 0.0738,  0.0750, -0.0562,  0.0910, -0.0000],\n",
      "        [ 0.0775,  0.0897, -0.0896,  0.0697, -0.2221],\n",
      "        [ 0.1041,  0.0000, -0.0000,  0.0448, -0.0000],\n",
      "        [ 0.1065,  0.0867, -0.0936,  0.0802, -0.2008],\n",
      "        [ 0.1655,  0.0000, -0.0000,  0.0980, -0.1776],\n",
      "        [ 0.0000,  0.0762, -0.0576,  0.0703, -0.2378],\n",
      "        [ 0.0739,  0.0884, -0.0657,  0.0853, -0.2135],\n",
      "        [ 0.0000,  0.1602, -0.0000,  0.0700, -0.1523]], grad_fn=<TanhBackward>)\n",
      "tensor([[ 0.0000,  0.1646, -0.0813,  0.0791, -0.2072],\n",
      "        [ 0.0000,  0.0646, -0.0853,  0.0000, -0.0000],\n",
      "        [ 0.0727,  0.0753, -0.0788,  0.0781, -0.2230],\n",
      "        [ 0.0603,  0.0823, -0.0880,  0.0694, -0.2121],\n",
      "        [ 0.1731,  0.1613, -0.1049,  0.0612, -0.1419],\n",
      "        [ 0.0807,  0.0911, -0.0000,  0.0775, -0.2138],\n",
      "        [ 0.1679,  0.0000, -0.0857,  0.0785, -0.1747],\n",
      "        [ 0.0000,  0.1063, -0.0000,  0.0818, -0.1841]], grad_fn=<TanhBackward>)\n",
      "----------\n",
      "tensor([[ 0.0000,  0.1646, -0.0813,  0.0791, -0.2072],\n",
      "        [ 0.0000,  0.0646, -0.0853,  0.0000, -0.0000],\n",
      "        [ 0.0727,  0.0753, -0.0788,  0.0781, -0.2230],\n",
      "        [ 0.0603,  0.0823, -0.0880,  0.0694, -0.2121],\n",
      "        [ 0.1731,  0.1613, -0.1049,  0.0612, -0.1419],\n",
      "        [ 0.0807,  0.0911, -0.0000,  0.0775, -0.2138],\n",
      "        [ 0.1679,  0.0000, -0.0857,  0.0785, -0.1747],\n",
      "        [ 0.0000,  0.1063, -0.0000,  0.0818, -0.1841]], grad_fn=<TanhBackward>)\n",
      "tensor([[ 0.0638,  0.0567, -0.0763,  0.0780, -0.2268],\n",
      "        [ 0.0682,  0.0771, -0.0713,  0.0645, -0.2238],\n",
      "        [ 0.1716,  0.1711, -0.1044,  0.0870, -0.0000],\n",
      "        [ 0.1083,  0.0000, -0.0967,  0.0777, -0.1953],\n",
      "        [ 0.1650,  0.1513, -0.0895,  0.0743, -0.0000],\n",
      "        [ 0.0928,  0.0000, -0.0918,  0.0344, -0.2316],\n",
      "        [ 0.0740,  0.0793, -0.0000,  0.0585, -0.2137],\n",
      "        [ 0.1064,  0.1021, -0.1283,  0.0586, -0.2194]], grad_fn=<TanhBackward>)\n",
      "----------\n",
      "tensor([[ 0.0638,  0.0567, -0.0763,  0.0780, -0.2268],\n",
      "        [ 0.0682,  0.0771, -0.0713,  0.0645, -0.2238],\n",
      "        [ 0.1716,  0.1711, -0.1044,  0.0870, -0.0000],\n",
      "        [ 0.1083,  0.0000, -0.0967,  0.0777, -0.1953],\n",
      "        [ 0.1650,  0.1513, -0.0895,  0.0743, -0.0000],\n",
      "        [ 0.0928,  0.0000, -0.0918,  0.0344, -0.2316],\n",
      "        [ 0.0740,  0.0793, -0.0000,  0.0585, -0.2137],\n",
      "        [ 0.1064,  0.1021, -0.1283,  0.0586, -0.2194]], grad_fn=<TanhBackward>)\n",
      "tensor([[ 0.0547,  0.0538, -0.0000,  0.0553, -0.2381],\n",
      "        [ 0.0000,  0.0000, -0.0863,  0.0741, -0.1958],\n",
      "        [ 0.0826,  0.0628, -0.0000,  0.0000, -0.2517],\n",
      "        [ 0.1538,  0.1564, -0.0000,  0.0764, -0.1811],\n",
      "        [ 0.0682,  0.0758, -0.0800,  0.0631, -0.2268],\n",
      "        [ 0.1601,  0.1505, -0.0884,  0.0000, -0.1920],\n",
      "        [ 0.1666,  0.1628, -0.0000,  0.0711, -0.1620],\n",
      "        [ 0.0920,  0.0664, -0.0000,  0.0546, -0.2632]], grad_fn=<TanhBackward>)\n",
      "----------\n",
      "tensor([[ 0.0547,  0.0538, -0.0000,  0.0553, -0.2381],\n",
      "        [ 0.0000,  0.0000, -0.0863,  0.0741, -0.1958],\n",
      "        [ 0.0826,  0.0628, -0.0000,  0.0000, -0.2517],\n",
      "        [ 0.1538,  0.1564, -0.0000,  0.0764, -0.1811],\n",
      "        [ 0.0682,  0.0758, -0.0800,  0.0631, -0.2268],\n",
      "        [ 0.1601,  0.1505, -0.0884,  0.0000, -0.1920],\n",
      "        [ 0.1666,  0.1628, -0.0000,  0.0711, -0.1620],\n",
      "        [ 0.0920,  0.0664, -0.0000,  0.0546, -0.2632]], grad_fn=<TanhBackward>)\n",
      "tensor([[ 0.0542,  0.0507, -0.0956,  0.0473, -0.0000],\n",
      "        [ 0.1656,  0.0000, -0.1124,  0.0803, -0.0000],\n",
      "        [ 0.0626,  0.0686, -0.0666,  0.0000, -0.2504],\n",
      "        [ 0.0702,  0.0691, -0.0000,  0.0569, -0.2113],\n",
      "        [ 0.1651,  0.1527, -0.1017,  0.0809, -0.1899],\n",
      "        [ 0.0000,  0.0611, -0.0786,  0.0505, -0.2347],\n",
      "        [ 0.0698,  0.0732, -0.0000,  0.0761, -0.2464],\n",
      "        [ 0.0000,  0.0714, -0.0854,  0.0325, -0.2366]], grad_fn=<TanhBackward>)\n",
      "----------\n",
      "tensor([[ 0.0542,  0.0507, -0.0956,  0.0473, -0.0000],\n",
      "        [ 0.1656,  0.0000, -0.1124,  0.0803, -0.0000],\n",
      "        [ 0.0626,  0.0686, -0.0666,  0.0000, -0.2504],\n",
      "        [ 0.0702,  0.0691, -0.0000,  0.0569, -0.2113],\n",
      "        [ 0.1651,  0.1527, -0.1017,  0.0809, -0.1899],\n",
      "        [ 0.0000,  0.0611, -0.0786,  0.0505, -0.2347],\n",
      "        [ 0.0698,  0.0732, -0.0000,  0.0761, -0.2464],\n",
      "        [ 0.0000,  0.0714, -0.0854,  0.0325, -0.2366]], grad_fn=<TanhBackward>)\n",
      "tensor([[ 0.0934,  0.0720, -0.0000,  0.0000, -0.2507],\n",
      "        [ 0.0735,  0.0553, -0.1012,  0.0579, -0.2324],\n",
      "        [ 0.0866,  0.0659, -0.0000,  0.0327, -0.2273],\n",
      "        [ 0.0000,  0.1627, -0.0000,  0.0582, -0.1282],\n",
      "        [ 0.0000,  0.0000, -0.0697,  0.0705, -0.2453],\n",
      "        [ 0.1534,  0.1442, -0.0879,  0.0902, -0.1856],\n",
      "        [ 0.0000,  0.0000, -0.0893,  0.0448, -0.2339],\n",
      "        [ 0.0966,  0.0570, -0.0772,  0.0354, -0.2306]], grad_fn=<TanhBackward>)\n",
      "----------\n",
      "tensor([[ 0.0934,  0.0720, -0.0000,  0.0000, -0.2507],\n",
      "        [ 0.0735,  0.0553, -0.1012,  0.0579, -0.2324],\n",
      "        [ 0.0866,  0.0659, -0.0000,  0.0327, -0.2273],\n",
      "        [ 0.0000,  0.1627, -0.0000,  0.0582, -0.1282],\n",
      "        [ 0.0000,  0.0000, -0.0697,  0.0705, -0.2453],\n",
      "        [ 0.1534,  0.1442, -0.0879,  0.0902, -0.1856],\n",
      "        [ 0.0000,  0.0000, -0.0893,  0.0448, -0.2339],\n",
      "        [ 0.0966,  0.0570, -0.0772,  0.0354, -0.2306]], grad_fn=<TanhBackward>)\n",
      "tensor([[ 0.0000,  0.1417, -0.1186,  0.0624, -0.1667],\n",
      "        [ 0.0903,  0.0520, -0.0875,  0.0255, -0.2338],\n",
      "        [ 0.1088,  0.0827, -0.1057,  0.0645, -0.2128],\n",
      "        [ 0.0944,  0.0642, -0.0000,  0.0226, -0.2253],\n",
      "        [ 0.0000,  0.0905, -0.0651,  0.0000, -0.2510],\n",
      "        [ 0.0720,  0.0735, -0.0000,  0.0424, -0.0000],\n",
      "        [ 0.0904,  0.0519, -0.0953,  0.0347, -0.2318],\n",
      "        [ 0.0503,  0.0632, -0.0000,  0.0682, -0.2611]], grad_fn=<TanhBackward>)\n",
      "----------\n",
      "tensor([[ 0.0000,  0.1417, -0.1186,  0.0624, -0.1667],\n",
      "        [ 0.0903,  0.0520, -0.0875,  0.0255, -0.2338],\n",
      "        [ 0.1088,  0.0827, -0.1057,  0.0645, -0.2128],\n",
      "        [ 0.0944,  0.0642, -0.0000,  0.0226, -0.2253],\n",
      "        [ 0.0000,  0.0905, -0.0651,  0.0000, -0.2510],\n",
      "        [ 0.0720,  0.0735, -0.0000,  0.0424, -0.0000],\n",
      "        [ 0.0904,  0.0519, -0.0953,  0.0347, -0.2318],\n",
      "        [ 0.0503,  0.0632, -0.0000,  0.0682, -0.2611]], grad_fn=<TanhBackward>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/carlovoss/anaconda3/lib/python3.6/multiprocessing/queues.py\", line 240, in _feed\n",
      "    send_bytes(obj)\n",
      "  File \"/Users/carlovoss/anaconda3/lib/python3.6/multiprocessing/connection.py\", line 200, in send_bytes\n",
      "    self._send_bytes(m[offset:offset + size])\n",
      "  File \"/Users/carlovoss/anaconda3/lib/python3.6/multiprocessing/connection.py\", line 404, in _send_bytes\n",
      "    self._send(header + buf)\n",
      "  File \"/Users/carlovoss/anaconda3/lib/python3.6/multiprocessing/connection.py\", line 368, in _send\n",
      "    n = write(self._handle, buf)\n",
      "BrokenPipeError: [Errno 32] Broken pipe\n",
      "  File \"/Users/carlovoss/anaconda3/lib/python3.6/multiprocessing/queues.py\", line 240, in _feed\n",
      "    send_bytes(obj)\n",
      "  File \"/Users/carlovoss/anaconda3/lib/python3.6/multiprocessing/connection.py\", line 200, in send_bytes\n",
      "    self._send_bytes(m[offset:offset + size])\n",
      "  File \"/Users/carlovoss/anaconda3/lib/python3.6/multiprocessing/connection.py\", line 404, in _send_bytes\n",
      "    self._send(header + buf)\n",
      "  File \"/Users/carlovoss/anaconda3/lib/python3.6/multiprocessing/connection.py\", line 368, in _send\n",
      "    n = write(self._handle, buf)\n",
      "BrokenPipeError: [Errno 32] Broken pipe\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/carlovoss/anaconda3/lib/python3.6/multiprocessing/queues.py\", line 240, in _feed\n",
      "    send_bytes(obj)\n",
      "  File \"/Users/carlovoss/anaconda3/lib/python3.6/multiprocessing/connection.py\", line 200, in send_bytes\n",
      "    self._send_bytes(m[offset:offset + size])\n",
      "  File \"/Users/carlovoss/anaconda3/lib/python3.6/multiprocessing/connection.py\", line 404, in _send_bytes\n",
      "    self._send(header + buf)\n",
      "  File \"/Users/carlovoss/anaconda3/lib/python3.6/multiprocessing/connection.py\", line 368, in _send\n",
      "    n = write(self._handle, buf)\n",
      "BrokenPipeError: [Errno 32] Broken pipe\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-12-15517b83aff3>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparam\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'training'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'n_epochs'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0;31m# Train with batches\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m     \u001b[0mmean_epoch_training_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_loader_training\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_latent_space\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/jupyter_notebooks/masterarbeit/src/ai/utils/trainer.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, data_loader_training)\u001b[0m\n\u001b[1;32m    157\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    158\u001b[0m             \u001b[0;31m# Backward pass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 159\u001b[0;31m             \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    160\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    161\u001b[0m             \u001b[0;31m# Update parameters\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/jupyter_notebooks/masterarbeit/venv_pm/lib/python3.6/site-packages/torch/tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph)\u001b[0m\n\u001b[1;32m    105\u001b[0m                 \u001b[0mproducts\u001b[0m\u001b[0;34m.\u001b[0m \u001b[0mDefaults\u001b[0m \u001b[0mto\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    106\u001b[0m         \"\"\"\n\u001b[0;32m--> 107\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    108\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    109\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/jupyter_notebooks/masterarbeit/venv_pm/lib/python3.6/site-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables)\u001b[0m\n\u001b[1;32m     91\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[1;32m     92\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 93\u001b[0;31m         allow_unreachable=True)  # allow_unreachable flag\n\u001b[0m\u001b[1;32m     94\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     95\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Create lists to save training loss and validation loss of each epoch\n",
    "hist_loss = []\n",
    "torch.manual_seed(0)\n",
    "\n",
    "for epoch in range(param['training']['n_epochs']):\n",
    "    # Train with batches \n",
    "    mean_epoch_training_loss = trainer.train(data_loader_training)\n",
    "\n",
    "    # Evaluate\n",
    "    mean_epoch_validation_loss = trainer.evaluate(data_loader_validation, hist_loss, epoch)\n",
    "\n",
    "    # Cache History\n",
    "    trainer.cache_history_training(hist_loss, epoch, mean_epoch_training_loss, mean_epoch_validation_loss)\n",
    "\n",
    "    # Save model if its the best one since the last change in configuration of hyperparameters\n",
    "    status_ok = trainer.save_model(epoch, mean_epoch_validation_loss, param['model']['input_size'], \n",
    "                                   param['model']['lstm_layer'], param['model']['n_hidden_lstm'], \n",
    "                                   param['model']['n_hidden_fc_pred'], param[\"model\"][\"sequence_size\"],\n",
    "                                   param['model']['n_hidden_fc_ls']\n",
    "                                  )\n",
    "    if not status_ok:\n",
    "        break\n",
    "\n",
    "# Safe results to csv file\n",
    "df = pd.DataFrame(hist_loss)\n",
    "df.to_csv(param[\"filed_location\"][\"history\"], sep=\";\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualization of cyclic learning rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = range(len(lr_find_lr))\n",
    "data = pd.DataFrame(data={'y': lr_find_lr, 'x': x})\n",
    "f, ax = plt.subplots(figsize=(7, 7))\n",
    "sns.lineplot(x=data.x, y=data.y, ax=ax)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "masterarbeit",
   "language": "python",
   "name": "masterarbeit"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
