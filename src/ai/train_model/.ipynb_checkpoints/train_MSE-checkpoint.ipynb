{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.nn.utils import clip_grad_norm_\n",
    "from torch.autograd import Variable\n",
    "from torchsummary import summary\n",
    "import pandas as pd\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import matplotlib.pyplot as plt\n",
    "import torch.optim as optim\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from keras.preprocessing.sequence import TimeseriesGenerator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "hyperparameters = {\n",
    "    \"data\" : {\n",
    "        \"stake_traing_data\" : 0.75, \n",
    "        \"path_data\" : '../../data/vega_shrinkwrapper_original/'\n",
    "    },\n",
    "    \"model\" : {\n",
    "        \"input_size\" : 7,\n",
    "        \"n_hidden\" : 24,\n",
    "        \"sequence_size\" : 50,\n",
    "        \"batch_size\" : 8,\n",
    "        \"lstm_layer\" : 2,\n",
    "    },\n",
    "    \"cycling_lr\" : {\n",
    "        \"step_size\" : 4, \n",
    "        \"mode\" : \"triangular_decay\", \n",
    "        \"gamma\" : 0.95\n",
    "    },\n",
    "    \"training\": {\n",
    "        \"n_epochs\" : 200,\n",
    "        \"patience\" : 50,\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Split train and test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_data(stake_training_data, path):\n",
    "    dataset = pd.read_csv(path)\n",
    "    amount_training_data = round(len(dataset)*stake_training_data)\n",
    "    train_data = dataset.iloc[0:amount_training_data,:]\n",
    "    validation_data = dataset.iloc[amount_training_data:,:]\n",
    "    return train_data, validation_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_test_data(path):\n",
    "    dataset = pd.read_csv(path)\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Timestamp</th>\n",
       "      <th>pCut Motor: Torque</th>\n",
       "      <th>pCut CTRL Position controller: Lag error</th>\n",
       "      <th>pCut CTRL Position controller: Actual position</th>\n",
       "      <th>pCut CTRL Position controller: Actual speed</th>\n",
       "      <th>pSvolFilm CTRL Position controller: Actual position</th>\n",
       "      <th>pSvolFilm CTRL Position controller: Actual speed</th>\n",
       "      <th>pSvolFilm CTRL Position controller: Lag error</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-0.188</td>\n",
       "      <td>-0.112131</td>\n",
       "      <td>-0.002490</td>\n",
       "      <td>-884606</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>11128</td>\n",
       "      <td>2.504289</td>\n",
       "      <td>0.261085</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-0.184</td>\n",
       "      <td>-0.088931</td>\n",
       "      <td>-0.003863</td>\n",
       "      <td>-884606</td>\n",
       "      <td>17.166138</td>\n",
       "      <td>11128</td>\n",
       "      <td>-2.504289</td>\n",
       "      <td>0.260083</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-0.180</td>\n",
       "      <td>-0.115141</td>\n",
       "      <td>0.001630</td>\n",
       "      <td>-884606</td>\n",
       "      <td>-6.866455</td>\n",
       "      <td>11128</td>\n",
       "      <td>7.513016</td>\n",
       "      <td>0.259081</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-0.176</td>\n",
       "      <td>-0.111815</td>\n",
       "      <td>0.003003</td>\n",
       "      <td>-884606</td>\n",
       "      <td>-13.732910</td>\n",
       "      <td>11128</td>\n",
       "      <td>-2.504289</td>\n",
       "      <td>0.260083</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-0.172</td>\n",
       "      <td>-0.130970</td>\n",
       "      <td>0.004376</td>\n",
       "      <td>-884606</td>\n",
       "      <td>-6.866455</td>\n",
       "      <td>11128</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.261085</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Timestamp  pCut Motor: Torque   pCut CTRL Position controller: Lag error  \\\n",
       "0     -0.188           -0.112131                                  -0.002490   \n",
       "1     -0.184           -0.088931                                  -0.003863   \n",
       "2     -0.180           -0.115141                                   0.001630   \n",
       "3     -0.176           -0.111815                                   0.003003   \n",
       "4     -0.172           -0.130970                                   0.004376   \n",
       "\n",
       "    pCut CTRL Position controller: Actual position  \\\n",
       "0                                          -884606   \n",
       "1                                          -884606   \n",
       "2                                          -884606   \n",
       "3                                          -884606   \n",
       "4                                          -884606   \n",
       "\n",
       "    pCut CTRL Position controller: Actual speed  \\\n",
       "0                                      0.000000   \n",
       "1                                     17.166138   \n",
       "2                                     -6.866455   \n",
       "3                                    -13.732910   \n",
       "4                                     -6.866455   \n",
       "\n",
       "    pSvolFilm CTRL Position controller: Actual position  \\\n",
       "0                                              11128      \n",
       "1                                              11128      \n",
       "2                                              11128      \n",
       "3                                              11128      \n",
       "4                                              11128      \n",
       "\n",
       "    pSvolFilm CTRL Position controller: Actual speed  \\\n",
       "0                                           2.504289   \n",
       "1                                          -2.504289   \n",
       "2                                           7.513016   \n",
       "3                                          -2.504289   \n",
       "4                                           0.000000   \n",
       "\n",
       "    pSvolFilm CTRL Position controller: Lag error  \n",
       "0                                        0.261085  \n",
       "1                                        0.260083  \n",
       "2                                        0.259081  \n",
       "3                                        0.260083  \n",
       "4                                        0.261085  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data, validation_data = split_data(stake_training_data=0.75, path='../../data/vega_shrinkwrapper_original/NewBlade001.csv')\n",
    "train_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data = load_test_data(path='../../data/vega_shrinkwrapper_original/WornBlade001.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Datahandler \n",
    "Time series data must be transformed into a structure of samples with `input` and `target` components before it can be used to fit a supervised learning model. <br>\n",
    "For a time series interested in one-step predictions, the observations at prior time steps, so-called lag observations, are used as `input` and the `target` is the observation at the current time step.\n",
    "\n",
    "For example, a univariate series can be expressed as a supervised learning problem with three time steps for `input` and one step as `target`, as follows:\n",
    "\n",
    "|input|target|\n",
    "|-----|------|\n",
    "[1, 2, 3]|[4]\n",
    "[2, 3, 4]|[5]\n",
    "[3, 4, 5]|[6]\n",
    "\n",
    "The Keras deep learning library provides the `TimeseriesGenerator` to automatically transform both univariate and multivariate time series data into such a format. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Size of multivariate time series\n",
    "Actually the data has a other size than in the table above because of multivariate time series data and because of using batches. <br>\n",
    "__First dimension__: batch size --> Defines the number of samples that will be propagated through the network simultaneously. <br>\n",
    "__Second dimension:__ timesteps --> Number of sequence which is passed into the LSTM <br>\n",
    "__Third dimension:__ input_dim --> Is the number of features. In this case data from 7 sensors, collected at the same time. <br>\n",
    "\n",
    "![](../pictures/input_shape.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CSVDataset(Dataset):\n",
    "    def __init__(self, data, timesteps):\n",
    "        # All data are loaded from csv file and converted to an numpy array\n",
    "        self.data = data.values\n",
    "        # Data generator is initialized \n",
    "        self.generator = TimeseriesGenerator(self.data, self.data, length=timesteps, batch_size=1)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        x, y = self.generator[index]\n",
    "        x_torch = torch.from_numpy(x)\n",
    "        # Dimension 0 with size 1 (created by TimeseriesGenerator because of batch_size=1) gets removed \n",
    "        # because DataLoader will add a dimension 0 with size=batch_size as well\n",
    "        x_torch = torch.squeeze(x_torch) # torch.Size([1, timesteps, 7]) --> torch.Size([timesteps, 7])\n",
    "        y_torch = torch.from_numpy(y)\n",
    "        return (x_torch.float(), y_torch.float()) \n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.generator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_train = CSVDataset(train_data, timesteps=3)\n",
    "dataset_validation = CSVDataset(validation_data, timesteps=3)\n",
    "dataset_test = CSVDataset(test_data, timesteps=3)\n",
    "\n",
    "data_loader_training = DataLoader(dataset_train, batch_size=2, num_workers=1, shuffle=False)\n",
    "data_loader_validation = DataLoader(dataset_validation, batch_size=1, num_workers=1, shuffle=False)\n",
    "data_loader_test = DataLoader(dataset_test, batch_size=1, num_workers=1, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data of batch: 0\n",
      "Size of input data: torch.Size([2, 3, 8])\n",
      "Size of target data: torch.Size([2, 1, 8])\n",
      "tensor([[[-1.8800e-01, -1.1213e-01, -2.4900e-03, -8.8461e+05,  0.0000e+00,\n",
      "           1.1128e+04,  2.5043e+00,  2.6108e-01],\n",
      "         [-1.8400e-01, -8.8931e-02, -3.8633e-03, -8.8461e+05,  1.7166e+01,\n",
      "           1.1128e+04, -2.5043e+00,  2.6008e-01],\n",
      "         [-1.8000e-01, -1.1514e-01,  1.6298e-03, -8.8461e+05, -6.8665e+00,\n",
      "           1.1128e+04,  7.5130e+00,  2.5908e-01]],\n",
      "\n",
      "        [[-1.8400e-01, -8.8931e-02, -3.8633e-03, -8.8461e+05,  1.7166e+01,\n",
      "           1.1128e+04, -2.5043e+00,  2.6008e-01],\n",
      "         [-1.8000e-01, -1.1514e-01,  1.6298e-03, -8.8461e+05, -6.8665e+00,\n",
      "           1.1128e+04,  7.5130e+00,  2.5908e-01],\n",
      "         [-1.7600e-01, -1.1182e-01,  3.0031e-03, -8.8461e+05, -1.3733e+01,\n",
      "           1.1128e+04, -2.5043e+00,  2.6008e-01]]], dtype=torch.float64)\n",
      "tensor([[[-1.7600e-01, -1.1182e-01,  3.0031e-03, -8.8461e+05, -1.3733e+01,\n",
      "           1.1128e+04, -2.5043e+00,  2.6008e-01]],\n",
      "\n",
      "        [[-1.7200e-01, -1.3097e-01,  4.3764e-03, -8.8461e+05, -6.8665e+00,\n",
      "           1.1128e+04,  0.0000e+00,  2.6108e-01]]], dtype=torch.float64)\n",
      "----------------------------------\n",
      "Data of batch: 1\n",
      "Size of input data: torch.Size([2, 3, 8])\n",
      "Size of target data: torch.Size([2, 1, 8])\n",
      "tensor([[[-1.8000e-01, -1.1514e-01,  1.6298e-03, -8.8461e+05, -6.8665e+00,\n",
      "           1.1128e+04,  7.5130e+00,  2.5908e-01],\n",
      "         [-1.7600e-01, -1.1182e-01,  3.0031e-03, -8.8461e+05, -1.3733e+01,\n",
      "           1.1128e+04, -2.5043e+00,  2.6008e-01],\n",
      "         [-1.7200e-01, -1.3097e-01,  4.3764e-03, -8.8461e+05, -6.8665e+00,\n",
      "           1.1128e+04,  0.0000e+00,  2.6108e-01]],\n",
      "\n",
      "        [[-1.7600e-01, -1.1182e-01,  3.0031e-03, -8.8461e+05, -1.3733e+01,\n",
      "           1.1128e+04, -2.5043e+00,  2.6008e-01],\n",
      "         [-1.7200e-01, -1.3097e-01,  4.3764e-03, -8.8461e+05, -6.8665e+00,\n",
      "           1.1128e+04,  0.0000e+00,  2.6108e-01],\n",
      "         [-1.6800e-01, -1.3072e-01,  4.3764e-03, -8.8461e+05,  0.0000e+00,\n",
      "           1.1128e+04,  0.0000e+00,  2.6108e-01]]], dtype=torch.float64)\n",
      "tensor([[[-1.6800e-01, -1.3072e-01,  4.3764e-03, -8.8461e+05,  0.0000e+00,\n",
      "           1.1128e+04,  0.0000e+00,  2.6108e-01]],\n",
      "\n",
      "        [[-1.6400e-01, -9.3035e-02, -2.4900e-03, -8.8461e+05,  0.0000e+00,\n",
      "           1.1128e+04, -2.5043e+00,  2.6008e-01]]], dtype=torch.float64)\n",
      "----------------------------------\n"
     ]
    }
   ],
   "source": [
    "for batch_idx, data in enumerate(data_loader_training):\n",
    "    x,y = data\n",
    "    print('Data of batch: {}'.format(batch_idx))\n",
    "    print(\"Size of input data: {}\".format(x.size()))\n",
    "    print(\"Size of target data: {}\".format(y.size()))\n",
    "    print(x)\n",
    "    print(y)\n",
    "    print(\"----------------------------------\")\n",
    "    if batch_idx >=1: break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Simple Datahandler which does not produce target variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\nclass CSVDataset(Dataset):\\n    def __init__(self, path, timesteps, nb_samples):\\n        self.path = path\\n        self.chunksize = timesteps\\n        self.len = nb_samples / self.chunksize\\n\\n    def __getitem__(self, index):\\n        x = next(\\n            pd.read_csv(\\n                self.path,\\n                skiprows=index * self.chunksize + 1,  #+1, since we skip the header\\n                chunksize=self.chunksize))\\n        x = torch.from_numpy(x.values)\\n        return x\\n\\n    def __len__(self):\\n        return round(self.len)\\n\\n\\ndataset = CSVDataset('../../data/vega_shrinkwrapper/NewBlade001.csv', timesteps=2, nb_samples=nb_samples)\\nloader = DataLoader(dataset, batch_size=10, num_workers=1, shuffle=False)\\n\""
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "class CSVDataset(Dataset):\n",
    "    def __init__(self, path, timesteps, nb_samples):\n",
    "        self.path = path\n",
    "        self.chunksize = timesteps\n",
    "        self.len = nb_samples / self.chunksize\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        x = next(\n",
    "            pd.read_csv(\n",
    "                self.path,\n",
    "                skiprows=index * self.chunksize + 1,  #+1, since we skip the header\n",
    "                chunksize=self.chunksize))\n",
    "        x = torch.from_numpy(x.values)\n",
    "        return x\n",
    "\n",
    "    def __len__(self):\n",
    "        return round(self.len)\n",
    "\n",
    "\n",
    "dataset = CSVDataset('../../data/vega_shrinkwrapper/NewBlade001.csv', timesteps=2, nb_samples=nb_samples)\n",
    "loader = DataLoader(dataset, batch_size=10, num_workers=1, shuffle=False)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cyclic Learning Rate\n",
    "CLR is used to enhance the way the learning rate is scheduled during training, to provide better convergence and help in regularizing deep learning models. It eliminates the need to experimentally find the best values for the global learning rate. Allowing the learning rate to cyclically vary between lower and upper boundary values. The idea is to divide the training process into cycles determined by a stepsize parameter, which defines the number of iterations in half a cycle.\n",
    "![](../pictures/scheduler.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Find base_lr and max_lr\n",
    "Calculate the upper bound of the learning rate for your model. The way to do this is to:\n",
    "1. Define an initial learning rate, the lower boundary of the range you want to test (let’s say 1e-7)\n",
    "2. Define an upper boundary of the range (let’s say 0.1)\n",
    "3. Define an exponential scheme to run through this step by step: <br>\n",
    "```python\n",
    "lr_lambda = lambda x: math.exp(x * math.log(end_lr / start_lr) / (lr_find_epochs * len( data_loader[\"train\"])))\n",
    "scheduler = torch.optim.lr_scheduler.LambdaLR(optimizer, lr_lambda)\n",
    "```\n",
    "<br>\n",
    "__Script for this is in: __  <br>\n",
    "`find_base_and_max_lr.ipynb`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- A good upper bound (max_lr) is not on the lowest point, but about a factor of 10 to the left. (In this case 3e-3)\n",
    "- A good lower bound (base_lr), according to the paper and other sources, is the upper bound, divided by a factor 6."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Implement cyclic learning rate with founded parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cyclical_lr(step_size, mode, gamma=1., base_lr=3e-3, max_lr=5e-3):\n",
    "    \"\"\"\n",
    "    step_size: Number of iterations to complete half of a cycle.\n",
    "    gamma: constant in 'exponential_decay' scaling function\n",
    "    \"\"\"\n",
    "    # Maximum learning rate (scaler) and frequence of decay (scale_mode) depends on mode \n",
    "    if mode == 'triangular':\n",
    "        scaler = lambda x: 1.    # no decay\n",
    "        scale_mode = 'cycle'\n",
    "    elif mode == 'triangular_decay':\n",
    "        scaler = lambda x: 1 / (2.**(x - 1))\n",
    "        scale_mode = 'cycle'\n",
    "    elif mode == 'exponential_decay':\n",
    "        scaler = lambda x: gamma ** x\n",
    "        scale_mode = 'iterations'\n",
    "        \n",
    "    def relative(iteration, step_size):\n",
    "        cycle = np.floor(1 + iteration / (2 * step_size))\n",
    "        x = abs(iteration / step_size - 2 * cycle +1)\n",
    "    \n",
    "        if scale_mode == 'cycle':\n",
    "            return max(0, (1 - x)) * scaler(cycle)\n",
    "        else:\n",
    "            return max(0, (1 - x)) * scaler(iteration)\n",
    "    \n",
    "    return lambda iteration: base_lr + (max_lr - base_lr) * relative(iteration, step_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x141086240>]"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "n = 10\n",
    "scheduler = cyclical_lr(step_size=4, mode=\"triangular_decay\", gamma=0.95)\n",
    "lrs = [scheduler(t) for t in range(n*4)]\n",
    "plt.xlabel(\"\")\n",
    "plt.ylabel(\"learning rate\")\n",
    "plt.plot(lrs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize optimizer\n",
    "# lr=1. because of scheduler (1*learning_rate_schedular?????\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=1.)  \n",
    "\n",
    "# step_size is the number of training iterations per\n",
    "# half cycle. Authors suggest setting step_size\n",
    "# 2-8 x training iterations in epoch.\n",
    "step_szie = 4 * len(data_loader)\n",
    "\n",
    "# Initialize function\n",
    "clr = cyclical_lr(step_size, min_lr= , max_lr)\n",
    "\n",
    "# PyTorch has a LambdaLR object which lets us pass the above lambda function as an argument\n",
    "scheduler = torch.optim.lr_scheduler.LambdaLR(optimizer=optimizer, lr_lambda=[clr])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mixture Density Neural Network\n",
    "__Parameters for LSTM Modul:__\n",
    "- input_size : The number of expected features in the input x\n",
    "- hidden_size :The number of features in the hidden state h\n",
    "- num_layers : Number of recurrent layers. E.g., setting num_layers=2 would mean stacking two LSTMs together to form a stacked LSTM, with the second LSTM taking in outputs of the first LSTM and computing the final results.\n",
    "- batch_first : If True, then the input __and output__ tensors are provided as (batch, seq, feature).\n",
    "- dropout – If non-zero, introduces a Dropout layer on the outputs of each LSTM layer except the last layer, with dropout probability equal to dropout. Default: 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTM(nn.Module):\n",
    "    def __init__(self, batch_size, input_dim, n_hidden=256, n_layers=1):\n",
    "        super(LSTM, self).__init__()\n",
    "        # Attributes for LSTM Network\n",
    "        self.input_dim = input_dim\n",
    "        self.n_hidden = n_hidden\n",
    "        self.n_layers = n_layers\n",
    "        self.batch_size = batch_size\n",
    "        \n",
    "        # Attribut for Gaussians\n",
    "        self.n_gaussians = 2\n",
    "        \n",
    "        # Definition of NN layer\n",
    "        # batch_first = True because dataloader creates batches and batch_size is 0. dimension\n",
    "        self.lstm = nn.LSTM(input_size = self.input_dim, hidden_size = self.n_hidden, num_layers = self.n_layers, batch_first = True)\n",
    "        self.fc1 = nn.Linear(input_size = self.n_hidden, output_size = self.n_gaussians * self.input_dim)\n",
    "        \n",
    "    def forward(self, input_data):\n",
    "        # Forward propagate LSTM\n",
    "        # LSTM in Pytorch return two results the first one usually called output and the second one (hidden_state, cell_state). \n",
    "        # As output the LSTM returns all the hidden_states for all the timesteps (seq), in other words all of the hidden states throughout\n",
    "        # the sequence\n",
    "        # As hidden_state the LSTM returns just the most recent hidden state\n",
    "        lstm_out, (hidden_state, cell_state) = self.lstm(input_data)\n",
    "        # Select the output from the last sequence \n",
    "        last_out = lstm_out[:,self.seq_len -1,:]\n",
    "        out = self.fc1(last_out)\n",
    "        # Reshape out to shape torch.Size(batch_size, n_features, 2)\n",
    "        raw_output = out.view(self.batch_size, self.input_dim, 2)\n",
    "        # y_hat and tau alternate, y_hat and tau are next to each other for each feature \n",
    "        y_hat = raw_output[:,:,0]\n",
    "        tau = raw_output[:,:,1]\n",
    "        #σ = exp(τ) guarantees σ > 0 and provides numerical stability in the learning process\n",
    "        sigma = torch.exp(tau)\n",
    "        \n",
    "        return y_hat, sigma\n",
    "    \n",
    "    def init_hidden(self):\n",
    "        # This is what we'll initialise our hidden state as well as our cell state\n",
    "        # The axes semantics are (num_layers, minibatch_size, hidden_dim)\n",
    "        h0 = torch.zeros(self.n_layers, self.batch_size, self.n_hidden)\n",
    "        c0 = torch.zeros(self.n_layers, self.batch_size, self.n_hidden)\n",
    "        return [t for t in (h0, c0)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loss function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- torch.exp(input) --> Returns a new tensor with the exponential of the elements of the input tensor : $y_i = e^{x_i}$\n",
    "- normal_distribution = torch.distributions.Normal(loc=, scale=) --> Create a normal (Gaussian) distribution parameterized by loc (mean) and scale (standard deviation).\n",
    "- normal_distribution.log_prob(value) --> Returns the log of the probability density function (in this case of normal distribution) evaluated at value.\n",
    "- torch.sum(input=, dim=) --> Returns the sum of all elements in the input tensor; dim is the dimension to reduce \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss_function(output, target_data):\n",
    "    y_hat, sigma = output\n",
    "    # target_data.size(batch_size,1,7) because timestep = 1\n",
    "    # new target_data.size(batch_size,7)\n",
    "    target_data = torch.squeeze(target_data)\n",
    "    \n",
    "    term = ((target_data-y_hat)/sigma)**2 + 2*torch.log(sigma)\n",
    "    return torch.sum(input=term, dim=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialize Model and Optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = LSTM(batch_size=2, input_dim=7, n_hidden=50, n_layers=1)\n",
    "optimizer = optim.SGD(netz.parameters(), lr=0.01)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Loop\n",
    "An epoch consists of a learning cycle over all batches of training data and an evaluation of the most recent model with the testing data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Start model training\")\n",
    "# Create lists to save training loss and validation loss of each epoch\n",
    "hist_training_loss = []\n",
    "hist_validation_loss = []\n",
    "\n",
    "for epoch in range(1, n_epochs +1):\n",
    "    # Empty list for recording performance \n",
    "    epoch_training_loss = []\n",
    "    epoch_validation_loss = []\n",
    "    \n",
    "    # Also, we need to clear out the hidden state of the LSTM,\n",
    "    # detaching it from its history on the last instance.\n",
    "    hidden = model.init_hidden()\n",
    "    \n",
    "    ##### Training #####\n",
    "    for batch_number, (input_data, target_data) in enumerate(data_loader_train):\n",
    "        # Set model to training mode before train the neural network.\n",
    "        model.train()\n",
    "        \n",
    "        # Zero out gradient, else they will accumulate between minibatches\n",
    "        optim.zero_grad()\n",
    "        \n",
    "        # We need to detach the hidden state to prevent exploding/vanishing gradients\n",
    "        hidden = [_.detach() for _ in hidden]\n",
    "        \n",
    "        # Forward propagation\n",
    "        output = model(input_data)\n",
    "        \n",
    "        # Calculate loss\n",
    "        iteration_loss = loss_function(output, target_data)\n",
    "        epoch_training_loss.append(iteration_loss.item())\n",
    "    \n",
    "        # Backward pass\n",
    "        loss.backward()\n",
    "        \n",
    "        # Update LR\n",
    "        scheduler.step()\n",
    "        \n",
    "        # Update parameters\n",
    "        optimizer.step()\n",
    "    \n",
    "    # Save mean of loss over all training iterations\n",
    "    mean_epoch_training_loss = sum(epoch_training_loss) / float(len(epoch_training_loss))\n",
    "    hist_training_loss.append(mean_epoch_training_loss)\n",
    "        \n",
    "    ##### Evaluation #######\n",
    "    for input_data, target_data in data_loader_validation:\n",
    "        # Change model to evaluation (prediction) mode\n",
    "        model.eval()\n",
    "        \n",
    "        # Predict \n",
    "        out = model(input_data)\n",
    "        \n",
    "         # Calculate loss\n",
    "        iteration_loss = loss_function(output, target_data)\n",
    "        epoch_validation_loss.append(iteration_loss.item())\n",
    "        \n",
    "    # Save mean of loss over all validation iterations to epoch history  \n",
    "    mean_epoch_validation_loss = sum(epoch_validation_loss) / float(len(epoch_validation_loss))\n",
    "    hist_validation_loss.append(mean_epoch_validation_loss)\n",
    "        \n",
    "    # Check after every evaluation whether the latest model is the best one or not\n",
    "    # If this is the case, set current score to best_score, reset trails and save the model.\n",
    "    if mean_epoch_validation_loss < lowest_loss:\n",
    "        trials = 0\n",
    "        lowest_loss = mean_epoch_validation_loss\n",
    "        torch.save({\n",
    "            'epoch': epoch,\n",
    "            'model_state_dict': model.state_dict(),\n",
    "            'optimizer_state_dict': optimizer.state_dict(),\n",
    "            'loss': mean_epoch_validation_loss\n",
    "        }, \"/best_model.pt\")\n",
    "        print(\"Epoch {}: best model saved with loss: {}\".format(epoch, mean_epoch_validation_loss))\n",
    "    \n",
    "    # Else: Increase trails by one and start new epoch as long as not too many epochs \n",
    "    # were unsuccessful (controlled by patience)\n",
    "    else:\n",
    "        trials += 1\n",
    "        if trials >= patience:\n",
    "            print(f'Early stopping on epoch {epoch}')\n",
    "            break  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prediction with data of worn blades"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.load_state_dict(torch.load('best.pth'))\n",
    "model.eval()\n",
    "test = []\n",
    "print('Prediction of worn dataset')\n",
    "for input_data, target_data in data_loader_test:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "masterarbeit",
   "language": "python",
   "name": "masterarbeit"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
