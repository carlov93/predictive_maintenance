{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import os\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data import ConcatDataset\n",
    "import torch.optim as optim\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import time\n",
    "from random import randint\n",
    "\n",
    "# own Modules \n",
    "from models_mle import LstmMle\n",
    "from data_set import DataSetSensors\n",
    "from cross_validation import CrossValidationProvider\n",
    "from scaler import DataScaler\n",
    "from trainer import Trainer\n",
    "from loss_module import LossMle\n",
    "from tester import Tester\n",
    "from logger import Logger"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Paramerters phm data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "param = {\n",
    "    \"data\" : {\n",
    "        \"path\" : '../../../../data/phm_data_challenge/recipe/dataset_for_each_recipe/training/training_recipe_67.csv' ,\n",
    "    },\n",
    "    \"preprocessing\" : {\n",
    "        \"droped_features\": [\"ID\", \"stage\", \"Lot\", \"runnum\", \"recipe\", \"recipe_step\",\n",
    "                            \"up time\", \"ongoing time\", \n",
    "                            \"ETCHSOURCEUSAGE\", \"ETCHAUXSOURCETIMER\", \n",
    "                            \"ETCHAUX2SOURCETIMER\", \"FIXTURESHUTTERPOSITION\", \"ROTATIONSPEED\"\n",
    "                           ],\n",
    "        \"features_not_to_scale\": []\n",
    "    },\n",
    "    \"model\" : {\n",
    "        \"input_size\" : 12,\n",
    "        \"n_hidden_lstm\" : [8,13,21], \n",
    "        \"sequence_size\" : [8,21,55], \n",
    "        \"batch_size\" : 8,\n",
    "        \"lstm_layer\" : [1],\n",
    "        \"n_hidden_fc_1\": [55, 89],\n",
    "        \"n_hidden_fc_2\": [55, 89],\n",
    "        \"dropout_rate_lstm\": 0.0,\n",
    "        \"dropout_rate_fc\": 0.2\n",
    "    },\n",
    "    \"cycling_lr\" : {\n",
    "        \"scheduler_active\" : True, \n",
    "        # Mode can be one of {triangular, triangular2, exp_range}\n",
    "        \"mode\" : \"triangular\", \n",
    "        \"gamma\" : 0.9995,\n",
    "        \"base_lr\" : 0.0001, # 0.016, \n",
    "        \"max_lr\" :0.0005,  # 0.75\n",
    "    },\n",
    "    \"training\": {\n",
    "        \"stake_training_data\": 0.75,\n",
    "        \"total_number\" : 15000,\n",
    "        \"n_folds_cv\": 4,\n",
    "        \"n_epochs\" : 5,\n",
    "        \"n_epochs_test_model\": 5,\n",
    "        \"patience\" : 4,\n",
    "    },\n",
    "    \"filed_location\": {\n",
    "        \"model_for_test\" : \"../../../../models/cross_validation/MLE/phm67_A2_\",\n",
    "        \"log_file\" : \"../../../../models/cross_validation/MLE/phm67_log_A2_\", \n",
    "        \"history_trainval\" : \"../../../visualisation/files/cross_validation/MLE/phm_trainval_A2.csv\",\n",
    "        \"history_best_configuration\" : \"../../../../models/cross_validation/MLE/phm67_configurations_A2.csv\"\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Paramerters cpps data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "param = {\n",
    "    \"data\" : {\n",
    "        \"path\" : '../../../../data/cpps_degradation_new/data_obs10/train/obs_space_train_sinusiod_preprocessed.csv' ,\n",
    "    },\n",
    "    \"preprocessing\" : {\n",
    "        \"droped_features\": [\"ID\"\n",
    "                           ],\n",
    "        \"features_not_to_scale\": []\n",
    "    },\n",
    "    \"model\" : {\n",
    "        \"input_size\" : 10,\n",
    "        \"n_hidden_lstm\" : [21], \n",
    "        \"sequence_size\" : [8,21], \n",
    "        \"batch_size\" : 8,\n",
    "        \"lstm_layer\" : [1],\n",
    "        \"n_hidden_fc_1\": [55, 89],\n",
    "        \"n_hidden_fc_2\": [55, 89],\n",
    "        \"dropout_rate_lstm\": 0.0,\n",
    "        \"dropout_rate_fc\": 0.2\n",
    "    },\n",
    "    \"cycling_lr\" : {\n",
    "        \"scheduler_active\" : True, \n",
    "        # Mode can be one of {triangular, triangular2, exp_range}\n",
    "        \"mode\" : \"triangular\", \n",
    "        \"gamma\" : 0.9995,\n",
    "        \"base_lr\" : 0.0001, # 0.016, \n",
    "        \"max_lr\" :0.0005,  # 0.75\n",
    "    },\n",
    "    \"training\": {\n",
    "        \"stake_training_data\": 0.75,\n",
    "        \"total_number\" : 15000,\n",
    "        \"n_folds_cv\": 4,\n",
    "        \"n_epochs\" : 5,\n",
    "        \"n_epochs_test_model\": 5,\n",
    "        \"patience\" : 4,\n",
    "    },\n",
    "    \"filed_location\": {\n",
    "        \"model_for_test\" : \"../../../../models/cross_validation/MLE/cpps_A2_\",\n",
    "        \"log_file\" : \"../../../../models/cross_validation/MLE/cpps_log_A2_\", \n",
    "        \"history_trainval\" : \"../../../visualisation/files/cross_validation/MLE/cpps_trainval_A2.csv\",\n",
    "        \"history_best_configuration\" : \"../../../../models/cross_validation/MLE/cpps_configurations_A2.csv\"\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Paramerters artifical data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "param = {\n",
    "    \"data\" : {\n",
    "        \"path\" : '../../../../data/artifical_signals/artifical_2_signals.csv' ,\n",
    "    },\n",
    "    \"preprocessing\" : {\n",
    "        \"droped_features\": [\"ID\"\n",
    "                           ],\n",
    "        \"features_not_to_scale\": []\n",
    "    },\n",
    "    \"model\" : {\n",
    "        \"input_size\" : 2,\n",
    "        \"n_hidden_lstm\" : [1,8], \n",
    "        \"sequence_size\" : [8,21,55], \n",
    "        \"batch_size\" : 8,\n",
    "        \"lstm_layer\" : [1],\n",
    "        \"n_hidden_fc_1\": [55, 89],\n",
    "        \"n_hidden_fc_2\": [55, 89],\n",
    "        \"dropout_rate_lstm\": 0.0,\n",
    "        \"dropout_rate_fc\": 0.2\n",
    "    },\n",
    "    \"cycling_lr\" : {\n",
    "        \"scheduler_active\" : True, \n",
    "        # Mode can be one of {triangular, triangular2, exp_range}\n",
    "        \"mode\" : \"triangular\", \n",
    "        \"gamma\" : 0.9995,\n",
    "        \"base_lr\" : 0.0001, # 0.016, \n",
    "        \"max_lr\" :0.0005,  # 0.75\n",
    "    },\n",
    "    \"training\": {\n",
    "        \"stake_training_data\": 0.75,\n",
    "        \"total_number\" : 12000,\n",
    "        \"n_folds_cv\": 4,\n",
    "        \"n_epochs\" : 5,\n",
    "        \"n_epochs_test_model\": 5,\n",
    "        \"patience\" : 4,\n",
    "    },\n",
    "    \"filed_location\": {\n",
    "        \"model_for_test\" : \"../../../../models/cross_validation/MLE/artifical_data_A2_\",\n",
    "        \"log_file\" : \"../../../../models/cross_validation/MLE/artifical_data_log_A2_\", \n",
    "        \"history_trainval\" : \"../../../visualisation/files/cross_validation/MLE/artifical_data_trainval.csv\",\n",
    "        \"history_best_configuration\" : \"../../../visualisation/files/cross_validation/MLE/artifical_data_configurations_A2.csv\"\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Split Data into folds\n",
    "- ignored features are getting removed\n",
    "- remaining data are split up into folds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv_provider = CrossValidationProvider(path=param[\"data\"][\"path\"], \n",
    "                                      no_folds=param[\"training\"][\"n_folds_cv\"], \n",
    "                                      amount_data=param[\"training\"][\"total_number\"],\n",
    "                                      ignored_features = param['preprocessing']['droped_features'],\n",
    "                                      stake = param[\"training\"][\"stake_training_data\"], \n",
    "                                     )\n",
    "folds, test_set = cv_provider.provide_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cross Validation Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Data : Fold 1\n",
      "- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -\n",
      "Start with new hyperparameters in grid search: \n",
      "Sequence_size: 10\n",
      "Number LSTM Layers: 1\n",
      "LSTM Number Hidden Dimensions: 10\n",
      "FC NN Number Hidden Dimensions: 20\n",
      "Stopped training on this partion at epoch 2\n",
      "Lowest training loss for this configuration: 0.6462837497659383\n",
      "Lowest validation loss for this configuration: 0.48546048411877735\n",
      "\n",
      "# ## ## ## ## ## ## ## ## ## ## ## ## ## ## ## ## ## ## ## #\n",
      "\n",
      "Start with new hyperparameters in grid search: \n",
      "Sequence_size: 10\n",
      "Number LSTM Layers: 1\n",
      "LSTM Number Hidden Dimensions: 10\n",
      "FC NN Number Hidden Dimensions: 40\n",
      "Stopped training on this partion at epoch 2\n",
      "Lowest training loss for this configuration: 0.2808120509676103\n",
      "Lowest validation loss for this configuration: 0.060651717864952624\n",
      "\n",
      "# ## ## ## ## ## ## ## ## ## ## ## ## ## ## ## ## ## ## ## #\n",
      "\n",
      "Start with new hyperparameters in grid search: \n",
      "Sequence_size: 10\n",
      "Number LSTM Layers: 1\n",
      "LSTM Number Hidden Dimensions: 20\n",
      "FC NN Number Hidden Dimensions: 20\n",
      "Stopped training on this partion at epoch 2\n",
      "Lowest training loss for this configuration: 0.5869112762644709\n",
      "Lowest validation loss for this configuration: 0.40516600383312573\n",
      "\n",
      "# ## ## ## ## ## ## ## ## ## ## ## ## ## ## ## ## ## ## ## #\n",
      "\n",
      "Start with new hyperparameters in grid search: \n",
      "Sequence_size: 10\n",
      "Number LSTM Layers: 1\n",
      "LSTM Number Hidden Dimensions: 20\n",
      "FC NN Number Hidden Dimensions: 40\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-8-31ef6ca6cc72>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    115\u001b[0m                         \u001b[0;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparam\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'training'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'n_epochs'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    116\u001b[0m                             \u001b[0;31m# Train\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 117\u001b[0;31m                             \u001b[0mmean_epoch_training_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_loader_training\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    118\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    119\u001b[0m                             \u001b[0;31m# Evaluate\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/jupyter_notebooks/masterarbeit/src/ai/utils/trainer.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, data_loader_training)\u001b[0m\n\u001b[1;32m     51\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     52\u001b[0m             \u001b[0;31m# Update parameters\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 53\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     54\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     55\u001b[0m             \u001b[0;31m# Update LR if scheduler is active\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/jupyter_notebooks/masterarbeit/venv_pm/lib/python3.6/site-packages/torch/optim/sgd.py\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, closure)\u001b[0m\n\u001b[1;32m     98\u001b[0m                     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     99\u001b[0m                         \u001b[0mbuf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mparam_state\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'momentum_buffer'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 100\u001b[0;31m                         \u001b[0mbuf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmul_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmomentum\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mdampening\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0md_p\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    101\u001b[0m                     \u001b[0;32mif\u001b[0m \u001b[0mnesterov\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    102\u001b[0m                         \u001b[0md_p\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0md_p\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmomentum\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbuf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Initialise Logger\n",
    "session_id = str(randint(10000, 99999))\n",
    "logger = Logger(param[\"filed_location\"][\"log_file\"], session_id)\n",
    "\n",
    "# Create file where validation results are stored and add header\n",
    "column_names_validation = [\"validation_fold\",\"validation_loss\", \"training_loss\",\n",
    "                           \"n_hidden_lstm\", \"sequence_length\", \"n_lstm_layer\", \n",
    "                           \"n_hidden_fc\"]\n",
    "with open(param[\"filed_location\"][\"history_best_configuration\"], \"a+\") as file:\n",
    "    [file.write(column+\";\") for column in column_names_validation]\n",
    "    file.write(\"\\n\")\n",
    "         \n",
    "for iteration in range (0, param[\"training\"][\"n_folds_cv\"]):\n",
    "    # Select folds for current iteration\n",
    "    training_folds = [x for i,x in enumerate(folds) if i!=iteration] \n",
    "    validation_fold = folds[iteration]\n",
    "    print(\"\\n\"+\"# #\"*20+\"\\n\")\n",
    "    logger.log_message(\"\\n\"+\"# #\"*20+\"\\n\")\n",
    "    print(\"Validation Data : Fold \"+ str(iteration+1))\n",
    "    logger.log_message(\"Validation Data : Fold \"+ str(iteration+1))\n",
    "    print(\"\\n\"+\"# #\"*20+\"\\n\")\n",
    "    logger.log_message(\"\\n\"+\"# #\"*20+\"\\n\")\n",
    "    \n",
    "    # Get mean and variance of training folds \n",
    "    raw_training_data = pd.concat(training_folds, axis = 0, ignore_index=True)\n",
    "    raw_validation_data = validation_fold\n",
    "    scaler = DataScaler(features_not_to_scale= param['preprocessing']['features_not_to_scale'])\n",
    "    _, _ = scaler.scale_data(raw_training_data, raw_validation_data)\n",
    "    mean_train, val_train = scaler.provide_statistics()\n",
    "  \n",
    "    # Scale all folds\n",
    "    scaled_folds = []\n",
    "    for i in range(len(folds)):\n",
    "        scaled_folds.append(scaler.scale_fold(folds[i], mean_train, val_train))\n",
    "    \n",
    "    # Training model and test hyperparameter on validation set\n",
    "    for n_lstm_layer in param[\"model\"][\"lstm_layer\"]:\n",
    "        for sequence_size  in param[\"model\"][\"sequence_size\"]:\n",
    "            # Creat Dataset of Training Folds\n",
    "            scaled_folds_training = [x for i,x in enumerate(scaled_folds) if i!=iteration]\n",
    "            reformated_folds = []\n",
    "            for i in range(len(scaled_folds_training)):\n",
    "                reformated_folds.append(DataSet(scaled_folds_training[i], timesteps=sequence_size))\n",
    "            \n",
    "            dataset_validation = DataSetSensors(scaled_folds[iteration], timesteps=sequence_size)\n",
    "            dataset_training = ConcatDataset(reformated_folds)\n",
    "            \n",
    "            # Initialize DataLoader\n",
    "            data_loader_training = DataLoader(dataset_training, \n",
    "                                              batch_size=param[\"model\"][\"batch_size\"], \n",
    "                                              num_workers=0, \n",
    "                                              shuffle=True, \n",
    "                                              drop_last=True\n",
    "                                             )\n",
    "            data_loader_validation = DataLoader(dataset_validation, \n",
    "                                                batch_size=param[\"model\"][\"batch_size\"], \n",
    "                                                num_workers=0, \n",
    "                                                shuffle=True, \n",
    "                                                drop_last=True\n",
    "                                               )\n",
    "            \n",
    "            for n_hidden_lstm in param[\"model\"][\"n_hidden_lstm\"]:\n",
    "                for n_hidden_fc_1 in param[\"model\"][\"n_hidden_fc_1\"]:\n",
    "                    print(\"Start with new hyperparameters in grid search: \")\n",
    "                    logger.log_message(\"Start with new hyperparameters in grid search: \")\n",
    "                    print(\"Sequence_size: {}\".format(sequence_size))\n",
    "                    logger.log_message(\"Sequence_size: {}\".format(sequence_size))\n",
    "                    print(\"Number LSTM Layers: {}\".format(n_lstm_layer))\n",
    "                    logger.log_message(\"Number LSTM Layers: {}\".format(n_lstm_layer))\n",
    "                    print(\"LSTM Number Hidden Dimensions: {}\".format(n_hidden_lstm))\n",
    "                    logger.log_message(\"LSTM Number Hidden Dimensions: {}\".format(n_hidden_lstm))\n",
    "                    print(\"FC NN Number Hidden Dimensions: {}\".format(n_hidden_fc_1))\n",
    "                    logger.log_message(\"FC NN Number Hidden Dimensions: {}\".format(n_hidden_fc_1))\n",
    "                    \n",
    "                    # Initialize model\n",
    "                    torch.manual_seed(0)\n",
    "                    model = LstmMle(batch_size=param['model']['batch_size'], \n",
    "                                    input_dim=param['model']['input_size'], \n",
    "                                    n_hidden_lstm=n_hidden_lstm, \n",
    "                                    n_layers=n_lstm_layer,\n",
    "                                    dropout_rate_lstm= param['model']['dropout_rate_lstm'],\n",
    "                                    dropout_rate_fc= param['model']['dropout_rate_fc'],\n",
    "                                    n_hidden_fc_1=n_hidden_fc_1,\n",
    "                                    K = 0,\n",
    "                                    option = 1, \n",
    "                                    )\n",
    "                    \n",
    "                    for phase in range(1, 3):\n",
    "                        if phase == 1:\n",
    "                            model.K = 0\n",
    "                        if phase == 2:\n",
    "                            model.K = 1\n",
    "\n",
    "                        # Define Loss Function\n",
    "                        criterion = LossMle(param[\"model\"][\"input_size\"], param[\"model\"][\"batch_size\"])\n",
    "\n",
    "                        # Initialize Optimizer and Cyclic Learning Rate Scheduler\n",
    "                        optimizer = torch.optim.SGD(model.parameters(), lr=1.)  \n",
    "                        scheduler = torch.optim.lr_scheduler.CyclicLR(optimizer=optimizer, \n",
    "                                                                      base_lr=param['cycling_lr']['base_lr'], \n",
    "                                                                      max_lr=param['cycling_lr']['max_lr'], \n",
    "                                                                      step_size_up=(raw_training_data.shape[0]/8)*2, \n",
    "                                                                      mode=param['cycling_lr']['mode'],\n",
    "                                                                      gamma=param['cycling_lr']['gamma']\n",
    "                                                                      )\n",
    "                        # Initialize Trainer\n",
    "                        trainer = Trainer(model=model,\n",
    "                                          optimizer=optimizer,\n",
    "                                          scheduler=scheduler,\n",
    "                                          scheduler_active = param[\"cycling_lr\"][\"scheduler_active\"],\n",
    "                                          criterion=criterion, \n",
    "                                          location_model=param[\"filed_location\"][\"model_for_test\"], \n",
    "                                          patience=param['training']['patience']\n",
    "                                         )\n",
    "\n",
    "                        # Measure training time for current configuration\n",
    "                        start = time.time()\n",
    "                        for epoch in range(1, param['training']['n_epochs']+1):\n",
    "                            # Train\n",
    "                            mean_epoch_training_loss = trainer.train(data_loader_training)\n",
    "\n",
    "                            # Evaluate\n",
    "                            mean_epoch_validation_loss = trainer.evaluate(data_loader_validation, epoch)\n",
    "\n",
    "                            if mean_epoch_validation_loss < trainer.lowest_val_loss:\n",
    "                                trainer.trials = 0\n",
    "                                trainer.lowest_val_loss = mean_epoch_validation_loss\n",
    "                                trainer.lowest_train_loss = mean_epoch_training_loss\n",
    "                                status_ok = True\n",
    "                                \n",
    "\n",
    "                            else:\n",
    "                                trainer.trials += 1\n",
    "                                if trainer.trials >= trainer.patience:\n",
    "                                    status_ok = False\n",
    "                            \n",
    "                            # Empty list of train und val loss of this epoch \n",
    "                            trainer.epoch_training_loss = []\n",
    "                            trainer.epoch_validation_loss = []\n",
    "\n",
    "                            if not status_ok or epoch == (param['training']['n_epochs']):\n",
    "                                if phase == 1:\n",
    "                                    break\n",
    "                                logger.log_message(\"Stopped training phase 2 on this partion at epoch {}\".format(epoch))\n",
    "                                print(\"Stopped training phase 2 on this partion at epoch {}\".format(epoch))\n",
    "                                logger.log_message(\"Lowest training loss for this configuration: {}\".format(trainer.lowest_train_loss))\n",
    "                                print(\"Lowest training loss for this configuration: {}\".format(trainer.lowest_train_loss))\n",
    "                                logger.log_message(\"Lowest validation loss for this configuration: {}\".format(trainer.lowest_val_loss))\n",
    "                                print(\"Lowest validation loss for this configuration: {}\".format(trainer.lowest_val_loss))\n",
    "\n",
    "                                # Statistics of current fold\n",
    "                                statistics_validation = [\"fold \"+str(iteration+1),\n",
    "                                                         trainer.lowest_val_loss,\n",
    "                                                         trainer.lowest_train_loss,\n",
    "                                                         n_hidden_lstm, \n",
    "                                                         sequence_size,\n",
    "                                                         n_lstm_layer, \n",
    "                                                         n_hidden_fc_1\n",
    "                                                         ]\n",
    "\n",
    "                                # Safe statistics to .csv file\n",
    "                                with open(param[\"filed_location\"][\"history_best_configuration\"], \"a\") as file:\n",
    "                                    for value in statistics_validation:\n",
    "                                        file.write(str(value)+\";\")\n",
    "                                    file.write(\"\\n\")\n",
    "                                break       \n",
    "                    print(\"- -\"*20)\n",
    "                    logger.log_message(\"- -\"*20)\n",
    "                    \n",
    "print(\"\\n\"+\"# #\"*20+\"\\n\")\n",
    "logger.log_message(\"\\n\"+\"# #\"*20+\"\\n\")\n",
    "print(\"Finished Cross-Validation\")\n",
    "logger.log_message(\"Finished Cross-Validation\")\n",
    "print(\"\\n\"+\"# #\"*20+\"\\n\")\n",
    "logger.log_message(\"\\n\"+\"# #\"*20+\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mean of results over all folds for each configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>validation_fold</th>\n",
       "      <th>validation_loss</th>\n",
       "      <th>training_loss</th>\n",
       "      <th>n_hidden_lstm</th>\n",
       "      <th>sequence_length</th>\n",
       "      <th>n_lstm_layer</th>\n",
       "      <th>n_hidden_fc</th>\n",
       "      <th>Unnamed: 7</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>fold 1</td>\n",
       "      <td>0.424857</td>\n",
       "      <td>0.468983</td>\n",
       "      <td>8</td>\n",
       "      <td>8</td>\n",
       "      <td>1</td>\n",
       "      <td>55</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>fold 2</td>\n",
       "      <td>0.415716</td>\n",
       "      <td>0.468323</td>\n",
       "      <td>8</td>\n",
       "      <td>8</td>\n",
       "      <td>1</td>\n",
       "      <td>55</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>fold 3</td>\n",
       "      <td>0.434139</td>\n",
       "      <td>0.467952</td>\n",
       "      <td>8</td>\n",
       "      <td>8</td>\n",
       "      <td>1</td>\n",
       "      <td>55</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>fold 4</td>\n",
       "      <td>0.423833</td>\n",
       "      <td>0.469305</td>\n",
       "      <td>8</td>\n",
       "      <td>8</td>\n",
       "      <td>1</td>\n",
       "      <td>55</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  validation_fold  validation_loss  training_loss n_hidden_lstm  \\\n",
       "0          fold 1         0.424857       0.468983             8   \n",
       "1          fold 2         0.415716       0.468323             8   \n",
       "2          fold 3         0.434139       0.467952             8   \n",
       "3          fold 4         0.423833       0.469305             8   \n",
       "\n",
       "  sequence_length n_lstm_layer n_hidden_fc  Unnamed: 7  \n",
       "0               8            1          55         NaN  \n",
       "1               8            1          55         NaN  \n",
       "2               8            1          55         NaN  \n",
       "3               8            1          55         NaN  "
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results = pd.read_csv(param[\"filed_location\"][\"history_best_configuration\"], sep=\";\")\n",
    "best_configuration = pd.DataFrame(columns=results.columns)\n",
    "for i in range(1, param[\"training\"][\"n_folds_cv\"]+1):\n",
    "    subset_fold = results.loc[results[\"validation_fold\"]==\"fold \"+str(i)]\n",
    "    index_lowest_loss = subset_fold[\"validation_loss\"].idxmin()\n",
    "    row = results.iloc[index_lowest_loss].to_dict()\n",
    "    best_configuration = best_configuration.append(row, ignore_index = True)\n",
    "best_configuration.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train Model with best hyperparameter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Best hyperparameter (look into result csv file and set values accordingly)\n",
    "opt_n_hidden_lstm = 20\n",
    "opt_sequence_size = 20\n",
    "opt_n_lstm_layer = 1\n",
    "opt_n_hidden_fc_1 = 40"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training phase of final model started\n",
      "Epoch 0: best model saved with loss: 0.9616773870374475\n",
      "Epoch 1: best model saved with loss: 0.8808265012023705\n",
      "Epoch 0: best model saved with loss: 0.3898495008370706\n",
      "Epoch 1: best model saved with loss: 0.07342331251794738\n",
      "Training phase 2 is finished with training loss: 0.07342331251794738\n"
     ]
    }
   ],
   "source": [
    "logger.log_message(\"\\n\")\n",
    "print(\"\\n\"+\"# #\"*20+\"\\n\")\n",
    "logger.log_message(\"\\n\"+\"# #\"*20+\"\\n\")\n",
    "print(\"Training phase of final model started\")\n",
    "logger.log_message(\"Training phase of final model started with this configration:\")\n",
    "logger.log_message(\"Number LSTM Hidden Units: {}\".format(opt_n_hidden_lstm))\n",
    "logger.log_message(\"Sequence Length: {}\".format(opt_sequence_size))\n",
    "logger.log_message(\"Number LSTM Layer: {}\".format(opt_n_lstm_layer))\n",
    "logger.log_message(\"Number FC Hidden Units: {}\".format(opt_n_hidden_fc))\n",
    "print(\"\\n\"+\"# #\"*20+\"\\n\")\n",
    "logger.log_message(\"\\n\"+\"# #\"*20+\"\\n\")\n",
    "\n",
    "# Training Data\n",
    "raw_training_data = pd.concat(folds, axis = 0, ignore_index=True)\n",
    "\n",
    "# Scale training data and test data (test data with mean and variance of training data)\n",
    "scaler = DataScaler(features_not_to_scale= param['preprocessing']['features_not_to_scale'])\n",
    "train_data_scaled, test_data_scaled = scaler.scale_data(raw_training_data, test_set)\n",
    "\n",
    "# Initialize DataSet\n",
    "dataset_train = DataSet(train_data_scaled, timesteps=opt_sequence_size)\n",
    "\n",
    "# Initialize DataLoader\n",
    "data_loader_training = DataLoader(dataset_train, \n",
    "                                  batch_size=param[\"model\"][\"batch_size\"], \n",
    "                                  num_workers=0, \n",
    "                                  shuffle=True, \n",
    "                                  drop_last=True\n",
    "                                 )\n",
    "\n",
    "# Initialize model\n",
    "torch.manual_seed(0)\n",
    "model = LstmMle_1(batch_size=param['model']['batch_size'], \n",
    "                  input_dim=param['model']['input_size'], \n",
    "                  n_hidden_lstm=opt_n_hidden_lstm, \n",
    "                  n_layers=opt_n_lstm_layer,\n",
    "                  dropout_rate_lstm= param['model']['dropout_rate_lstm'],\n",
    "                  dropout_rate_fc= param['model']['dropout_rate_fc'],\n",
    "                  n_hidden_fc_1=opt_n_hidden_fc_1,\n",
    "                  K=0,\n",
    "                  )\n",
    "\n",
    "for phase in range(1, 3):\n",
    "    logger.log_message(\"\\n\")\n",
    "    print(\"Training phase {} of final model started\".format(phase))\n",
    "    logger.log_message(\"Training phase {} of final model started\".format(phase))\n",
    "    if phase == 1:\n",
    "        model.K = 0\n",
    "    if phase == 2:\n",
    "        model.K = 1\n",
    "\n",
    "    # Define Loss Function\n",
    "    criterion = LossMle(param[\"model\"][\"input_size\"], param[\"model\"][\"batch_size\"])\n",
    "\n",
    "    # Initialize Optimizer and Trainer\n",
    "    optimizer = torch.optim.SGD(model.parameters(), lr=1.)  \n",
    "    scheduler = torch.optim.lr_scheduler.CyclicLR(optimizer=optimizer, \n",
    "                                                  base_lr=param['cycling_lr']['base_lr'], \n",
    "                                                  max_lr=param['cycling_lr']['max_lr'], \n",
    "                                                  step_size_up=(len(raw_training_data)/param['model']['batch_size'])*2, # Authors of Cyclic LR suggest setting step_size 2-8 x training iterations in epoch.\n",
    "                                                  mode=param['cycling_lr']['mode'],\n",
    "                                                  gamma=param['cycling_lr']['gamma']\n",
    "                                                 )\n",
    "    trainer = Trainer(model=model,\n",
    "                      optimizer=optimizer,\n",
    "                      scheduler=scheduler,\n",
    "                      scheduler_active = param[\"cycling_lr\"][\"scheduler_active\"],\n",
    "                      criterion=criterion, \n",
    "                      location_model=param[\"filed_location\"][\"model_for_test\"], \n",
    "                      patience=param['training']['patience']\n",
    "                     )\n",
    "\n",
    "\n",
    "    for epoch in range(1,param[\"training\"][\"n_epochs_test_model\"]+1):\n",
    "        # Train with batches \n",
    "        mean_epoch_training_loss = trainer.train(data_loader_training)\n",
    "\n",
    "        # Save model if its the best one since the last change in configuration of hyperparameters\n",
    "        status_ok = trainer.save_model(epoch, mean_epoch_training_loss, session_id)\n",
    "\n",
    "        if not status_ok or epoch == param[\"training\"][\"n_epochs_test_model\"]:\n",
    "            if phase == 1:\n",
    "                break\n",
    "            print(\"Training phase 2 is finished with training loss: {}\".format(mean_epoch_training_loss))\n",
    "            logger.log_message(\"Training phase 2 is finished with training loss: {}\".format(mean_epoch_training_loss))\n",
    "            break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test trained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "# ## ## ## ## ## ## ## ## ## ## ## ## ## ## ## ## ## ## ## #\n",
      "\n",
      "Mean loss of test dataset is -0.583025102196513\n",
      "\n",
      "# ## ## ## ## ## ## ## ## ## ## ## ## ## ## ## ## ## ## ## #\n",
      "\n",
      "Cross Validation finished\n"
     ]
    }
   ],
   "source": [
    "model = LstmMle_1(batch_size=param['model']['batch_size'], \n",
    "                  input_dim=param['model']['input_size'], \n",
    "                  n_hidden_lstm=opt_n_hidden_lstm, \n",
    "                  n_layers=opt_n_lstm_layer,\n",
    "                  dropout_rate_lstm= param['model']['dropout_rate_lstm'],\n",
    "                  dropout_rate_fc= param['model']['dropout_rate_fc'],\n",
    "                  n_hidden_fc_1=opt_n_hidden_fc,\n",
    "                  K=1,\n",
    "                  )\n",
    "\n",
    "checkpoint = torch.load(param[\"filed_location\"][\"model_for_test\"]+\"id\"+str(session_id))\n",
    "model.load_state_dict(checkpoint['model_state_dict'])\n",
    "\n",
    "# Initialize DataSet\n",
    "dataset_test = DataSet(test_data_scaled, timesteps=opt_sequence_size)\n",
    "\n",
    "# Initialize DataLoader\n",
    "data_loader_test = DataLoader(dataset_test, \n",
    "                              batch_size=param[\"model\"][\"batch_size\"], \n",
    "                              num_workers=0, \n",
    "                              shuffle=True, \n",
    "                              drop_last=True\n",
    "                             )\n",
    "\n",
    "# Define Loss Function\n",
    "criterion = LossMle(param[\"model\"][\"input_size\"], param[\"model\"][\"batch_size\"])\n",
    "\n",
    "# Initialize Tester\n",
    "tester = Tester(model=model, criterion=criterion)\n",
    "\n",
    "# Evaluate Testset\n",
    "mean_test_loss = tester.evaluate(data_loader_test)\n",
    "print(\"\\n\"+\"# #\"*20+\"\\n\")\n",
    "logger.log_message(\"\\n\"+\"# #\"*20+\"\\n\")\n",
    "print(\"Mean loss of test dataset is {}\".format(mean_test_loss))\n",
    "logger.log_message(\"Mean loss of test dataset is {}\".format(mean_test_loss))\n",
    "print(\"\\n\"+\"# #\"*20+\"\\n\")\n",
    "logger.log_message(\"\\n\"+\"# #\"*20+\"\\n\")\n",
    "print(\"Cross Validation finished\")\n",
    "logger.log_message(\"Cross Validation finished\")\n",
    "# Delet model\n",
    "os.remove(param[\"filed_location\"][\"model_for_test\"]+\"id\"+str(session_id))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "masterarbeit",
   "language": "python",
   "name": "masterarbeit"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
