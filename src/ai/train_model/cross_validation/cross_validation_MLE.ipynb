{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader\n",
    "import torch.optim as optim\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import time\n",
    "\n",
    "# own Modules \n",
    "from models_mle import LstmMle_1\n",
    "from data_set import DataSet\n",
    "from cross_validation import CrossValidationProvider\n",
    "from scaler import DataScaler\n",
    "from trainer import Trainer\n",
    "from loss_module import LossMle\n",
    "from tester import Tester"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Paramerters artifical data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "param = {\n",
    "    \"data\" : {\n",
    "        \"path\" : '../../../../data/artifical_signals/artifical_2_signals.csv' ,\n",
    "    },\n",
    "    \"preprocessing\" : {\n",
    "        \"droped_features\": [\"ID\"\n",
    "                           ],\n",
    "        \"features_not_to_scale\": []\n",
    "    },\n",
    "    \"model\" : {\n",
    "        \"input_size\" : 2,\n",
    "        \"n_hidden_lstm\" : [10,20,50], \n",
    "        \"sequence_size\" : [10,20,30], \n",
    "        \"batch_size\" : 8,\n",
    "        \"lstm_layer\" : [1],\n",
    "        \"n_hidden_fc\": [20,40,60],\n",
    "        \"dropout_rate_lstm\": 0.0,\n",
    "        \"dropout_rate_fc\": 0.2\n",
    "    },\n",
    "    \"cycling_lr\" : {\n",
    "        \"scheduler_active\" : True, \n",
    "        # Mode can be one of {triangular, triangular2, exp_range}\n",
    "        \"mode\" : \"triangular\", \n",
    "        \"gamma\" : 0.9995,\n",
    "        \"base_lr\" : 0.0001, # 0.016, \n",
    "        \"max_lr\" :0.0005,  # 0.75\n",
    "    },\n",
    "    \"training\": {\n",
    "        \"stake_training_data\": 0.75,\n",
    "        \"total_number\" : 12000,\n",
    "        \"n_folds_cv\": 6,\n",
    "        \"n_epochs\" : 20,\n",
    "        \"patience\" : 7,\n",
    "    },\n",
    "    \"filed_location\": {\n",
    "        \"trained_model\" : \"../../../../models/cross_validation/MLE/artifical_data_\",\n",
    "        \"history_validation\" : \"../../../visualisation/files/cross_validation/MLE/validation_artifical_datacsv\",\n",
    "        \"history_test\" : \"../../../visualisation/files/cross_validation/MLE/test_artifical_data.csv\"\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Paramerters cpps data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "param = {\n",
    "    \"data\" : {\n",
    "        \"path\" : '../../../../data/cpps_degradation/large_degeneration/cpps_data_large_degeneration_training.csv' ,\n",
    "    },\n",
    "    \"preprocessing\" : {\n",
    "        \"droped_features\": [\"ID\"\n",
    "                           ],\n",
    "        \"features_not_to_scale\": []\n",
    "    },\n",
    "    \"model\" : {\n",
    "        \"input_size\" : 10,\n",
    "        \"n_hidden_lstm\" : [10,20,50], \n",
    "        \"sequence_size\" : [10,20,30], \n",
    "        \"batch_size\" : 8,\n",
    "        \"lstm_layer\" : [1],\n",
    "        \"n_hidden_fc\": [20,40,60],\n",
    "        \"dropout_rate_lstm\": 0.0,\n",
    "        \"dropout_rate_fc\": 0.2\n",
    "    },\n",
    "    \"cycling_lr\" : {\n",
    "        \"scheduler_active\" : True, \n",
    "        # Mode can be one of {triangular, triangular2, exp_range}\n",
    "        \"mode\" : \"triangular\", \n",
    "        \"gamma\" : 0.9995,\n",
    "        \"base_lr\" : 0.0001, # 0.016, \n",
    "        \"max_lr\" :0.0005,  # 0.75\n",
    "    },\n",
    "    \"training\": {\n",
    "        \"stake_training_data\": 0.75,\n",
    "        \"total_number\" : 50000,\n",
    "        \"n_folds_cv\": 6,\n",
    "        \"n_epochs\" : 20,\n",
    "        \"patience\" : 7,\n",
    "    },\n",
    "    \"filed_location\": {\n",
    "        \"trained_model\" : \"../../../../models/cross_validation/MLE/cpps_data_\",\n",
    "        \"history_validation\" : \"../../../visualisation/files/cross_validation/MLE/validation_cpps_data.csv\",\n",
    "        \"history_test\" : \"../../../visualisation/files/cross_validation/MLE/test_cpps_data.csv\"\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Paramerters phm data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "param = {\n",
    "    \"data\" : {\n",
    "        \"path\" : '../../../../data/phm_data_challenge/recipe/dataset_for_each_recipe/training/training_recipe_67.csv' ,\n",
    "    },\n",
    "    \"preprocessing\" : {\n",
    "        \"droped_features\": [\"ID\", \"stage\", \"Lot\", \"runnum\", \"recipe\", \"recipe_step\",\n",
    "                            \"up time\", \"ongoing time\", \n",
    "                            \"ETCHSOURCEUSAGE\", \"ETCHAUXSOURCETIMER\", \n",
    "                            \"ETCHAUX2SOURCETIMER\", \"FIXTURESHUTTERPOSITION\", \"ROTATIONSPEED\"\n",
    "                           ],\n",
    "        \"features_not_to_scale\": []\n",
    "    },\n",
    "    \"model\" : {\n",
    "        \"input_size\" : 12,\n",
    "        \"n_hidden_lstm\" : [10,20,50], \n",
    "        \"sequence_size\" : [10,20,30], \n",
    "        \"batch_size\" : 8,\n",
    "        \"lstm_layer\" : [1],\n",
    "        \"n_hidden_fc\": [20,40,60],\n",
    "        \"dropout_rate_lstm\": 0.0,\n",
    "        \"dropout_rate_fc\": 0.2\n",
    "    },\n",
    "    \"cycling_lr\" : {\n",
    "        \"scheduler_active\" : True, \n",
    "        # Mode can be one of {triangular, triangular2, exp_range}\n",
    "        \"mode\" : \"triangular\", \n",
    "        \"gamma\" : 0.9995,\n",
    "        \"base_lr\" : 0.0001, # 0.016, \n",
    "        \"max_lr\" :0.0005,  # 0.75\n",
    "    },\n",
    "    \"training\": {\n",
    "        \"total_number\" : 1000,\n",
    "        \"n_folds_cv\": 6,\n",
    "        \"n_epochs\" : 20,\n",
    "        \"patience\" : 7,\n",
    "    },\n",
    "    \"filed_location\": {\n",
    "        \"trained_model\" : \"../../../../models/cross_validation/MLE/phm_data_recipe_66_\",\n",
    "        \"history_validation\" : \"../../../visualisation/files/cross_validation/MLE/validation_phm_data_recipe_66.csv\",\n",
    "        \"history_test\" : \"../../../visualisation/files/cross_validation/MLE/test_phm_data_recipe_66.csv\"\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Nested Cross Validation for Time Series Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](../../../../knowledge/pictures/nested_cv.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Split Data into folds\n",
    "- ignored features are getting removed\n",
    "- remaining data are split up into folds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv_provider = CrossValidationProvider(path=param[\"data\"][\"path\"], \n",
    "                                      no_folds=param[\"training\"][\"n_folds_cv\"], \n",
    "                                      amount_data=param[\"training\"][\"total_number\"],\n",
    "                                      ignored_features = param['preprocessing']['droped_features']\n",
    "                                     )\n",
    "folds = cv_provider.provide_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cross Validation Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save scaler metrics for test dataset\n",
    "scaler_metrics = {}\n",
    "# Path, where model from phase 1 is stored\n",
    "current_path_model_phase_1 = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Data : Fold 1-2\n",
      "Validation Data : Fold 3\n",
      "Test Data : Fold 4\n",
      "Amount Training Data: 334\n",
      "Amount Validation Data: 167\n",
      "Amount Test Data: 167\n",
      "- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -\n",
      "Start with new hyperparameters in grid search: \n",
      "Sequence_size: 20\n",
      "Number LSTM Layers: 1\n",
      "LSTM Number Hidden Dimensions: 10\n",
      "FC NN Number Hidden Dimensions: 60\n",
      "\n",
      "Start with phase 1:\n",
      "-------- epoch_no. 0 finished with eval loss 0.7073911925156912--------\n",
      "Epoch 0: best model saved with loss: 0.7073911925156912\n",
      "-------- epoch_no. 1 finished with eval loss 0.6998292439513736--------\n",
      "Epoch 1: best model saved with loss: 0.6998292439513736\n",
      "-------- epoch_no. 2 finished with eval loss 0.6910380356841617--------\n",
      "Epoch 2: best model saved with loss: 0.6910380356841617\n",
      "-------- epoch_no. 3 finished with eval loss 0.6853121022383372--------\n",
      "Epoch 3: best model saved with loss: 0.6853121022383372\n",
      "-------- epoch_no. 4 finished with eval loss 0.6810592214266459--------\n",
      "Epoch 4: best model saved with loss: 0.6810592214266459\n",
      "-------- epoch_no. 5 finished with eval loss 0.6736778683132596--------\n",
      "Epoch 5: best model saved with loss: 0.6736778683132596\n",
      "-------- epoch_no. 6 finished with eval loss 0.6665345033009847--------\n",
      "Epoch 6: best model saved with loss: 0.6665345033009847\n",
      "-------- epoch_no. 7 finished with eval loss 0.6608464552296532--------\n",
      "Epoch 7: best model saved with loss: 0.6608464552296532\n",
      "-------- epoch_no. 8 finished with eval loss 0.6565772493680319--------\n",
      "Epoch 8: best model saved with loss: 0.6565772493680319\n",
      "-------- epoch_no. 9 finished with eval loss 0.650932639837265--------\n",
      "Epoch 9: best model saved with loss: 0.650932639837265\n",
      "-------- epoch_no. 10 finished with eval loss 0.6442115306854248--------\n",
      "Epoch 10: best model saved with loss: 0.6442115306854248\n",
      "-------- epoch_no. 11 finished with eval loss 0.638352609342999--------\n",
      "Epoch 11: best model saved with loss: 0.638352609342999\n",
      "-------- epoch_no. 12 finished with eval loss 0.6351436575253805--------\n",
      "Epoch 12: best model saved with loss: 0.6351436575253805\n",
      "-------- epoch_no. 13 finished with eval loss 0.6295944054921468--------\n",
      "Epoch 13: best model saved with loss: 0.6295944054921468\n",
      "-------- epoch_no. 14 finished with eval loss 0.6244062185287476--------\n",
      "Epoch 14: best model saved with loss: 0.6244062185287476\n",
      "-------- epoch_no. 15 finished with eval loss 0.617762996090783--------\n",
      "Epoch 15: best model saved with loss: 0.617762996090783\n",
      "-------- epoch_no. 16 finished with eval loss 0.6150074766741859--------\n",
      "Epoch 16: best model saved with loss: 0.6150074766741859\n",
      "-------- epoch_no. 17 finished with eval loss 0.6108477314313253--------\n",
      "Epoch 17: best model saved with loss: 0.6108477314313253\n",
      "-------- epoch_no. 18 finished with eval loss 0.6059684322939979--------\n",
      "Epoch 18: best model saved with loss: 0.6059684322939979\n",
      "-------- epoch_no. 19 finished with eval loss 0.6014036436875662--------\n",
      "Epoch 19: best model saved with loss: 0.6014036436875662\n",
      "\n",
      "Start with phase 2:\n",
      "Load model for phase 2\n",
      "-------- epoch_no. 0 finished with eval loss 0.6538723674085405--------\n",
      "Epoch 0: best model saved with loss: 0.6538723674085405\n",
      "-------- epoch_no. 1 finished with eval loss 0.625559624698427--------\n",
      "Epoch 1: best model saved with loss: 0.625559624698427\n",
      "-------- epoch_no. 2 finished with eval loss 0.5938816732830472--------\n",
      "Epoch 2: best model saved with loss: 0.5938816732830472\n",
      "-------- epoch_no. 3 finished with eval loss 0.5725892583529154--------\n",
      "Epoch 3: best model saved with loss: 0.5725892583529154\n",
      "-------- epoch_no. 4 finished with eval loss 0.5548359354337057--------\n",
      "Epoch 4: best model saved with loss: 0.5548359354337057\n",
      "-------- epoch_no. 5 finished with eval loss 0.526230298810535--------\n",
      "Epoch 5: best model saved with loss: 0.526230298810535\n",
      "-------- epoch_no. 6 finished with eval loss 0.49582020110554165--------\n",
      "Epoch 6: best model saved with loss: 0.49582020110554165\n",
      "-------- epoch_no. 7 finished with eval loss 0.4705227530664868--------\n",
      "Epoch 7: best model saved with loss: 0.4705227530664868\n",
      "-------- epoch_no. 8 finished with eval loss 0.45394124421808457--------\n",
      "Epoch 8: best model saved with loss: 0.45394124421808457\n",
      "-------- epoch_no. 9 finished with eval loss 0.42729396290249294--------\n",
      "Epoch 9: best model saved with loss: 0.42729396290249294\n",
      "-------- epoch_no. 10 finished with eval loss 0.39518367250760394--------\n",
      "Epoch 10: best model saved with loss: 0.39518367250760394\n",
      "-------- epoch_no. 11 finished with eval loss 0.3668527520365185--------\n",
      "Epoch 11: best model saved with loss: 0.3668527520365185\n",
      "-------- epoch_no. 12 finished with eval loss 0.34911127554045784--------\n",
      "Epoch 12: best model saved with loss: 0.34911127554045784\n",
      "-------- epoch_no. 13 finished with eval loss 0.32382481296857196--------\n",
      "Epoch 13: best model saved with loss: 0.32382481296857196\n",
      "-------- epoch_no. 14 finished with eval loss 0.2918676386276881--------\n",
      "Epoch 14: best model saved with loss: 0.2918676386276881\n",
      "-------- epoch_no. 15 finished with eval loss 0.25794971403148437--------\n",
      "Epoch 15: best model saved with loss: 0.25794971403148437\n",
      "-------- epoch_no. 16 finished with eval loss 0.23769868248038822--------\n",
      "Epoch 16: best model saved with loss: 0.23769868248038822\n",
      "-------- epoch_no. 17 finished with eval loss 0.2155890232986874--------\n",
      "Epoch 17: best model saved with loss: 0.2155890232986874\n",
      "-------- epoch_no. 18 finished with eval loss 0.18305397613181007--------\n",
      "Epoch 18: best model saved with loss: 0.18305397613181007\n",
      "-------- epoch_no. 19 finished with eval loss 0.1512441328830189--------\n",
      "Epoch 19: best model saved with loss: 0.1512441328830189\n",
      "\n",
      "# ## ## ## ## ## ## ## ## ## ## ## ## ## ## ## ## ## ## ## ## ## ## ## ## ## ## ## ## ## #\n",
      "\n",
      "Start with new hyperparameters in grid search: \n",
      "Sequence_size: 20\n",
      "Number LSTM Layers: 1\n",
      "LSTM Number Hidden Dimensions: 20\n",
      "FC NN Number Hidden Dimensions: 60\n",
      "\n",
      "Start with phase 1:\n",
      "-------- epoch_no. 0 finished with eval loss 0.6757429076565636--------\n",
      "Epoch 0: best model saved with loss: 0.6757429076565636\n",
      "-------- epoch_no. 1 finished with eval loss 0.6678687764538659--------\n",
      "Epoch 1: best model saved with loss: 0.6678687764538659\n",
      "-------- epoch_no. 2 finished with eval loss 0.6613561312357584--------\n",
      "Epoch 2: best model saved with loss: 0.6613561312357584\n",
      "-------- epoch_no. 3 finished with eval loss 0.6566310591167874--------\n",
      "Epoch 3: best model saved with loss: 0.6566310591167874\n",
      "-------- epoch_no. 4 finished with eval loss 0.6534800893730588--------\n",
      "Epoch 4: best model saved with loss: 0.6534800893730588\n",
      "-------- epoch_no. 5 finished with eval loss 0.6458038588364919--------\n",
      "Epoch 5: best model saved with loss: 0.6458038588364919\n",
      "-------- epoch_no. 6 finished with eval loss 0.6388661000463698--------\n",
      "Epoch 6: best model saved with loss: 0.6388661000463698\n",
      "-------- epoch_no. 7 finished with eval loss 0.6333584686120352--------\n",
      "Epoch 7: best model saved with loss: 0.6333584686120352\n",
      "-------- epoch_no. 8 finished with eval loss 0.6317017475763956--------\n",
      "Epoch 8: best model saved with loss: 0.6317017475763956\n",
      "-------- epoch_no. 9 finished with eval loss 0.6252348290549384--------\n",
      "Epoch 9: best model saved with loss: 0.6252348290549384\n",
      "-------- epoch_no. 10 finished with eval loss 0.6194247702757517--------\n",
      "Epoch 10: best model saved with loss: 0.6194247702757517\n",
      "-------- epoch_no. 11 finished with eval loss 0.6159195535712771--------\n",
      "Epoch 11: best model saved with loss: 0.6159195535712771\n",
      "-------- epoch_no. 12 finished with eval loss 0.6106446782747904--------\n",
      "Epoch 12: best model saved with loss: 0.6106446782747904\n",
      "-------- epoch_no. 13 finished with eval loss 0.6081453098191155--------\n",
      "Epoch 13: best model saved with loss: 0.6081453098191155\n",
      "-------- epoch_no. 14 finished with eval loss 0.6029035415914323--------\n",
      "Epoch 14: best model saved with loss: 0.6029035415914323\n",
      "-------- epoch_no. 15 finished with eval loss 0.5978851119677225--------\n",
      "Epoch 15: best model saved with loss: 0.5978851119677225\n",
      "-------- epoch_no. 16 finished with eval loss 0.5927983754211001--------\n",
      "Epoch 16: best model saved with loss: 0.5927983754211001\n",
      "-------- epoch_no. 17 finished with eval loss 0.5914389226171706--------\n",
      "Epoch 17: best model saved with loss: 0.5914389226171706\n",
      "-------- epoch_no. 18 finished with eval loss 0.5854412118593851--------\n",
      "Epoch 18: best model saved with loss: 0.5854412118593851\n",
      "-------- epoch_no. 19 finished with eval loss 0.5814546015527513--------\n",
      "Epoch 19: best model saved with loss: 0.5814546015527513\n",
      "\n",
      "Start with phase 2:\n",
      "Load model for phase 2\n",
      "-------- epoch_no. 0 finished with eval loss 0.7042369312710233--------\n",
      "Epoch 0: best model saved with loss: 0.7042369312710233\n",
      "-------- epoch_no. 1 finished with eval loss 0.669124616516961--------\n",
      "Epoch 1: best model saved with loss: 0.669124616516961\n",
      "-------- epoch_no. 2 finished with eval loss 0.6365834375222524--------\n",
      "Epoch 2: best model saved with loss: 0.6365834375222524\n",
      "-------- epoch_no. 3 finished with eval loss 0.6130572987927331--------\n",
      "Epoch 3: best model saved with loss: 0.6130572987927331\n",
      "-------- epoch_no. 4 finished with eval loss 0.5945957005023956--------\n",
      "Epoch 4: best model saved with loss: 0.5945957005023956\n",
      "-------- epoch_no. 5 finished with eval loss 0.5612889031569163--------\n",
      "Epoch 5: best model saved with loss: 0.5612889031569163\n",
      "-------- epoch_no. 6 finished with eval loss 0.5262699772914251--------\n",
      "Epoch 6: best model saved with loss: 0.5262699772914251\n",
      "-------- epoch_no. 7 finished with eval loss 0.5001602619886398--------\n",
      "Epoch 7: best model saved with loss: 0.5001602619886398\n",
      "-------- epoch_no. 8 finished with eval loss 0.4840897023677826--------\n",
      "Epoch 8: best model saved with loss: 0.4840897023677826\n",
      "-------- epoch_no. 9 finished with eval loss 0.45543739861912197--------\n",
      "Epoch 9: best model saved with loss: 0.45543739861912197\n",
      "-------- epoch_no. 10 finished with eval loss 0.42030586964554256--------\n",
      "Epoch 10: best model saved with loss: 0.42030586964554256\n",
      "-------- epoch_no. 11 finished with eval loss 0.39326929218239254--------\n",
      "Epoch 11: best model saved with loss: 0.39326929218239254\n",
      "-------- epoch_no. 12 finished with eval loss 0.3715962486134635--------\n",
      "Epoch 12: best model saved with loss: 0.3715962486134635\n",
      "-------- epoch_no. 13 finished with eval loss 0.34937475621700287--------\n",
      "Epoch 13: best model saved with loss: 0.34937475621700287\n",
      "-------- epoch_no. 14 finished with eval loss 0.3166812062263489--------\n",
      "Epoch 14: best model saved with loss: 0.3166812062263489\n",
      "-------- epoch_no. 15 finished with eval loss 0.28392298767964047--------\n",
      "Epoch 15: best model saved with loss: 0.28392298767964047\n",
      "-------- epoch_no. 16 finished with eval loss 0.2581601407792833--------\n",
      "Epoch 16: best model saved with loss: 0.2581601407792833\n",
      "-------- epoch_no. 17 finished with eval loss 0.2394821908738878--------\n",
      "Epoch 17: best model saved with loss: 0.2394821908738878\n",
      "-------- epoch_no. 18 finished with eval loss 0.2030948665406969--------\n",
      "Epoch 18: best model saved with loss: 0.2030948665406969\n",
      "-------- epoch_no. 19 finished with eval loss 0.16768125858571795--------\n",
      "Epoch 19: best model saved with loss: 0.16768125858571795\n",
      "\n",
      "# ## ## ## ## ## ## ## ## ## ## ## ## ## ## ## ## ## ## ## ## ## ## ## ## ## ## ## ## ## #\n",
      "\n",
      "Start with new hyperparameters in grid search: \n",
      "Sequence_size: 20\n",
      "Number LSTM Layers: 1\n",
      "LSTM Number Hidden Dimensions: 50\n",
      "FC NN Number Hidden Dimensions: 60\n",
      "\n",
      "Start with phase 1:\n",
      "-------- epoch_no. 0 finished with eval loss 0.6813856561978658--------\n",
      "Epoch 0: best model saved with loss: 0.6813856561978658\n",
      "-------- epoch_no. 1 finished with eval loss 0.6738810307449765--------\n",
      "Epoch 1: best model saved with loss: 0.6738810307449765\n",
      "-------- epoch_no. 2 finished with eval loss 0.6670858065287272--------\n",
      "Epoch 2: best model saved with loss: 0.6670858065287272\n",
      "-------- epoch_no. 3 finished with eval loss 0.6621730923652649--------\n",
      "Epoch 3: best model saved with loss: 0.6621730923652649\n",
      "-------- epoch_no. 4 finished with eval loss 0.6586194634437561--------\n",
      "Epoch 4: best model saved with loss: 0.6586194634437561\n",
      "-------- epoch_no. 5 finished with eval loss 0.6530276437600454--------\n",
      "Epoch 5: best model saved with loss: 0.6530276437600454\n",
      "-------- epoch_no. 6 finished with eval loss 0.6463202602333493--------\n",
      "Epoch 6: best model saved with loss: 0.6463202602333493\n",
      "-------- epoch_no. 7 finished with eval loss 0.6416525443394979--------\n",
      "Epoch 7: best model saved with loss: 0.6416525443394979\n",
      "-------- epoch_no. 8 finished with eval loss 0.6379441420237223--------\n",
      "Epoch 8: best model saved with loss: 0.6379441420237223\n",
      "-------- epoch_no. 9 finished with eval loss 0.6328523688846164--------\n",
      "Epoch 9: best model saved with loss: 0.6328523688846164\n",
      "-------- epoch_no. 10 finished with eval loss 0.6274851759274801--------\n",
      "Epoch 10: best model saved with loss: 0.6274851759274801\n",
      "-------- epoch_no. 11 finished with eval loss 0.6230914625856612--------\n",
      "Epoch 11: best model saved with loss: 0.6230914625856612\n",
      "-------- epoch_no. 12 finished with eval loss 0.6194212469789717--------\n",
      "Epoch 12: best model saved with loss: 0.6194212469789717\n",
      "-------- epoch_no. 13 finished with eval loss 0.6159209609031677--------\n",
      "Epoch 13: best model saved with loss: 0.6159209609031677\n",
      "-------- epoch_no. 14 finished with eval loss 0.6106163197093539--------\n",
      "Epoch 14: best model saved with loss: 0.6106163197093539\n",
      "-------- epoch_no. 15 finished with eval loss 0.6048765679200491--------\n",
      "Epoch 15: best model saved with loss: 0.6048765679200491\n",
      "-------- epoch_no. 16 finished with eval loss 0.6010658707883623--------\n",
      "Epoch 16: best model saved with loss: 0.6010658707883623\n",
      "-------- epoch_no. 17 finished with eval loss 0.5984980993800693--------\n",
      "Epoch 17: best model saved with loss: 0.5984980993800693\n",
      "-------- epoch_no. 18 finished with eval loss 0.594330347246594--------\n",
      "Epoch 18: best model saved with loss: 0.594330347246594\n",
      "-------- epoch_no. 19 finished with eval loss 0.5885680814584097--------\n",
      "Epoch 19: best model saved with loss: 0.5885680814584097\n",
      "\n",
      "Start with phase 2:\n",
      "Load model for phase 2\n",
      "-------- epoch_no. 0 finished with eval loss 0.5364257958200243--------\n",
      "Epoch 0: best model saved with loss: 0.5364257958200243\n",
      "-------- epoch_no. 1 finished with eval loss 0.5131701793935564--------\n",
      "Epoch 1: best model saved with loss: 0.5131701793935564\n",
      "-------- epoch_no. 2 finished with eval loss 0.49018626080618966--------\n",
      "Epoch 2: best model saved with loss: 0.49018626080618966\n",
      "-------- epoch_no. 3 finished with eval loss 0.4733319663339191--------\n",
      "Epoch 3: best model saved with loss: 0.4733319663339191\n",
      "-------- epoch_no. 4 finished with eval loss 0.4599818206495709--------\n",
      "Epoch 4: best model saved with loss: 0.4599818206495709\n",
      "-------- epoch_no. 5 finished with eval loss 0.43865832686424255--------\n",
      "Epoch 5: best model saved with loss: 0.43865832686424255\n",
      "-------- epoch_no. 6 finished with eval loss 0.41353361474143135--------\n",
      "Epoch 6: best model saved with loss: 0.41353361474143135\n",
      "-------- epoch_no. 7 finished with eval loss 0.39455031520790523--------\n",
      "Epoch 7: best model saved with loss: 0.39455031520790523\n",
      "-------- epoch_no. 8 finished with eval loss 0.38055556846989524--------\n",
      "Epoch 8: best model saved with loss: 0.38055556846989524\n",
      "-------- epoch_no. 9 finished with eval loss 0.3595251225762897--------\n",
      "Epoch 9: best model saved with loss: 0.3595251225762897\n",
      "-------- epoch_no. 10 finished with eval loss 0.33413488003942704--------\n",
      "Epoch 10: best model saved with loss: 0.33413488003942704\n",
      "-------- epoch_no. 11 finished with eval loss 0.312501793106397--------\n",
      "Epoch 11: best model saved with loss: 0.312501793106397\n",
      "-------- epoch_no. 12 finished with eval loss 0.2967207100656297--------\n",
      "Epoch 12: best model saved with loss: 0.2967207100656297\n",
      "-------- epoch_no. 13 finished with eval loss 0.2771584805515077--------\n",
      "Epoch 13: best model saved with loss: 0.2771584805515077\n",
      "-------- epoch_no. 14 finished with eval loss 0.2500062982241313--------\n",
      "Epoch 14: best model saved with loss: 0.2500062982241313\n",
      "-------- epoch_no. 15 finished with eval loss 0.22170968270964092--------\n",
      "Epoch 15: best model saved with loss: 0.22170968270964092\n",
      "-------- epoch_no. 16 finished with eval loss 0.20196031365129682--------\n",
      "Epoch 16: best model saved with loss: 0.20196031365129682\n",
      "-------- epoch_no. 17 finished with eval loss 0.1851152992910809--------\n",
      "Epoch 17: best model saved with loss: 0.1851152992910809\n",
      "-------- epoch_no. 18 finished with eval loss 0.15841106780701214--------\n",
      "Epoch 18: best model saved with loss: 0.15841106780701214\n",
      "-------- epoch_no. 19 finished with eval loss 0.1243156637582514--------\n",
      "Epoch 19: best model saved with loss: 0.1243156637582514\n",
      "\n",
      "# ## ## ## ## ## ## ## ## ## ## ## ## ## ## ## ## ## ## ## ## ## ## ## ## ## ## ## ## ## #\n",
      "\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'statistics_folds' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-6-2a857ad27e06>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    142\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    143\u001b[0m     \u001b[0;31m# Test model with best hyperparameters of current fold\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 144\u001b[0;31m     \u001b[0mdf_hyperparameters\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstatistics_folds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    145\u001b[0m     \u001b[0mcurrent_fold\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf_hyperparameters\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mdf_hyperparameters\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miteration\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0miteration\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    146\u001b[0m     \u001b[0;31m# Get index of row with lowest validation loss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'statistics_folds' is not defined"
     ]
    }
   ],
   "source": [
    "# Create file where validation results are stored and add header\n",
    "column_names_validation = [\"iteration\",\"validation_fold\",\"validation_loss\", \n",
    "                           \"n_hidden_lstm\", \"sequence_size\", \"n_lstm_layer\", \n",
    "                           \"n_hidden_fc\", \"training phase\"]\n",
    "with open(param[\"filed_location\"][\"history_validation\"], \"a+\") as file:\n",
    "    [file.write(column+\";\") for column in column_names_validation]\n",
    "    file.write(\"\\n\")\n",
    "            \n",
    "# Create file where test results are stored and add header\n",
    "column_names_test = [\"test_fold\", \"test_loss\", \"n_hidden_lstm\", \"sequence_size\",\n",
    "                     \"n_lstm_layer\", \"n_hidden_fc\"]\n",
    "with open(param[\"filed_location\"][\"history_test\"], \"a+\") as file:\n",
    "    [file.write(column+\";\") for column in column_names_test]\n",
    "    file.write(\"\\n\")\n",
    "         \n",
    "for iteration in range (2, param[\"training\"][\"n_folds_cv\"]):\n",
    "    # Select folds for current iteration\n",
    "    training_folds = folds[:iteration]\n",
    "    validation_fold = folds[iteration:iteration+1]\n",
    "    test_fold = folds[iteration:iteration+2]\n",
    "    print(\"Training Data : Fold 1-\" + str(iteration))\n",
    "    print(\"Validation Data : Fold \"+ str(iteration+1))\n",
    "    print(\"Test Data : Fold \"+ str(iteration+2))\n",
    "    \n",
    "    # Concate data of training folds and unpack validation and testdata\n",
    "    raw_training_data = pd.concat(training_folds, axis = 0, ignore_index=True)\n",
    "    raw_validation_data = validation_fold[0]\n",
    "    raw_test_data = test_fold[0]\n",
    "    print(\"Amount Training Data: {}\".format(raw_training_data.shape[0]))\n",
    "    print(\"Amount Validation Data: {}\".format(raw_validation_data.shape[0]))\n",
    "    print(\"Amount Test Data: {}\".format(raw_test_data.shape[0]))\n",
    "    print(\"- -\"*30)\n",
    "    \n",
    "    # Scale training data and validation data (validation data with mean and variance of training data)\n",
    "    scaler = DataScaler(features_not_to_scale= param['preprocessing']['features_not_to_scale'])\n",
    "    train_data_scaled, validation_data_scaled, test_data_scaled = scaler.scale_data(raw_training_data, raw_validation_data, raw_test_data)\n",
    "    \n",
    "    # Training model and test hyperparameter on validation set\n",
    "    for n_lstm_layer in param[\"model\"][\"lstm_layer\"]:\n",
    "        for sequence_size  in param[\"model\"][\"sequence_size\"]:\n",
    "            # Initialize DataSet\n",
    "            dataset_train = DataSet(train_data_scaled, timesteps=sequence_size)\n",
    "            dataset_validation = DataSet(validation_data_scaled, timesteps=sequence_size)\n",
    "            \n",
    "            # Initialize DataLoader\n",
    "            data_loader_training = DataLoader(dataset_train, \n",
    "                                              batch_size=param[\"model\"][\"batch_size\"], \n",
    "                                              num_workers=0, \n",
    "                                              shuffle=True, \n",
    "                                              drop_last=True\n",
    "                                             )\n",
    "            data_loader_validation = DataLoader(dataset_validation, \n",
    "                                                batch_size=param[\"model\"][\"batch_size\"], \n",
    "                                                num_workers=0, \n",
    "                                                shuffle=True, \n",
    "                                                drop_last=True\n",
    "                                               )\n",
    "            \n",
    "            for n_hidden_lstm in param[\"model\"][\"n_hidden_lstm\"]:\n",
    "                for n_hidden_fc in param[\"model\"][\"n_hidden_fc\"]:\n",
    "                    print(\"Start with new hyperparameters in grid search: \")\n",
    "                    print(\"Sequence_size: {}\".format(sequence_size))\n",
    "                    print(\"Number LSTM Layers: {}\".format(n_lstm_layer))\n",
    "                    print(\"LSTM Number Hidden Dimensions: {}\".format(n_hidden_lstm))\n",
    "                    print(\"FC NN Number Hidden Dimensions: {}\".format(n_hidden_fc))\n",
    "                    \n",
    "                    for phase in range(1, 3):\n",
    "                        print(\"\\nStart with phase {}:\".format(phase))\n",
    "                        # Create lists to save training loss and validation loss of each epoch\n",
    "                        hist_loss = []\n",
    "                        torch.manual_seed(0)\n",
    "                        model = LstmMle_1(batch_size=param['model']['batch_size'], \n",
    "                                          input_dim=param['model']['input_size'], \n",
    "                                          n_hidden_lstm=n_hidden_lstm, \n",
    "                                          n_layers=n_lstm_layer,\n",
    "                                          dropout_rate_lstm= param['model']['dropout_rate_lstm'],\n",
    "                                          dropout_rate_fc= param['model']['dropout_rate_fc'],\n",
    "                                          n_hidden_fc=n_hidden_fc,\n",
    "                                          K=phase-1,\n",
    "                                          )\n",
    "                        \n",
    "                        if phase == 2:\n",
    "                            checkpoint = torch.load(current_path_model_phase_1)\n",
    "                            model.load_state_dict(checkpoint['model_state_dict'])\n",
    "                            print(\"Load model for phase 2\")\n",
    "\n",
    "                        # Define Loss Function\n",
    "                        criterion = LossMle(param[\"model\"][\"input_size\"], param[\"model\"][\"batch_size\"])\n",
    "\n",
    "                        # Initialize Optimizer and Cyclic Learning Rate Scheduler\n",
    "                        optimizer = torch.optim.SGD(model.parameters(), lr=1.)  \n",
    "                        scheduler = torch.optim.lr_scheduler.CyclicLR(optimizer=optimizer, \n",
    "                                                                      base_lr=param['cycling_lr']['base_lr'], \n",
    "                                                                      max_lr=param['cycling_lr']['max_lr'], \n",
    "                                                                      step_size_up=(raw_training_data.shape[0]/8)*2, \n",
    "                                                                      mode=param['cycling_lr']['mode'],\n",
    "                                                                      gamma=param['cycling_lr']['gamma']\n",
    "                                                                      )\n",
    "                        # Initialize Trainer\n",
    "                        trainer = Trainer(model=model,\n",
    "                                          optimizer=optimizer,\n",
    "                                          scheduler=scheduler,\n",
    "                                          scheduler_active = param[\"cycling_lr\"][\"scheduler_active\"],\n",
    "                                          criterion=criterion, \n",
    "                                          location_model=param[\"filed_location\"][\"trained_model\"], \n",
    "                                          location_stats=param[\"filed_location\"][\"history_validation\"], \n",
    "                                          patience=param['training']['patience']\n",
    "                                         )\n",
    "\n",
    "                        # Measure training time for current configuration\n",
    "                        start = time.time()\n",
    "\n",
    "                        for epoch in range(param['training']['n_epochs']):\n",
    "                            # Train\n",
    "                            mean_epoch_training_loss = trainer.train(data_loader_training)\n",
    "\n",
    "                            # Evaluate\n",
    "                            mean_epoch_validation_loss = trainer.evaluate(data_loader_validation, hist_loss, epoch)\n",
    "\n",
    "                            # Cache History\n",
    "                            trainer.cache_history_training(hist_loss, epoch, mean_epoch_training_loss, mean_epoch_validation_loss)\n",
    "\n",
    "                            # Save model if its the best one since the last change in configuration of hyperparameters\n",
    "                            trainer.fold = \"Fold_1-\"+str(iteration)\n",
    "                            status_ok, path_model = trainer.save_model(epoch, mean_epoch_validation_loss, param['model']['input_size'], \n",
    "                                                           n_lstm_layer, n_hidden_lstm, n_hidden_fc, sequence_size)\n",
    "                            current_path_model_phase_1 = path_model\n",
    "                            \n",
    "                            if not status_ok or epoch == (param['training']['n_epochs'])-1:\n",
    "                                # Statistics of current fold\n",
    "                                statistics_validation = [iteration,\"fold \"+str(iteration+1),\n",
    "                                                         trainer.lowest_loss, n_hidden_lstm, sequence_size,\n",
    "                                                         n_lstm_layer, n_hidden_fc, phase]\n",
    "                                \n",
    "                                # Safe statistics to .csv file\n",
    "                                with open(param[\"filed_location\"][\"history_validation\"], \"a\") as file:\n",
    "                                    for value in statistics_validation:\n",
    "                                        file.write(str(value)+\";\")\n",
    "                                    file.write(\"\\n\")\n",
    "                                break       \n",
    "                    print(\"\\n\"+\"# #\"*30+\"\\n\")\n",
    "                    \n",
    "    # Test model with best hyperparameters of current fold\n",
    "    df_hyperparameters = pd.read_csv(param[\"filed_location\"][\"history_validation\"])\n",
    "    current_fold = df_hyperparameters.loc[df_hyperparameters.iteration == iteration]\n",
    "    # Get index of row with lowest validation loss\n",
    "    idx_best_hyperparm = current_fold.lowest_validation_loss.idxmin()\n",
    "    opt_n_hidden_lstm = current_fold.iloc[idx_best_hyperparm, 4]\n",
    "    opt_sequence_size = current_fold.iloc[idx_best_hyperparm, 5]\n",
    "    opt_n_lstm_layer = current_fold.iloc[idx_best_hyperparm, 6]\n",
    "    opt_n_didden_fc = current_fold.iloc[idx_best_hyperparm, 7]\n",
    "    \n",
    "    # Load best model\n",
    "    torch.manual_seed(0)\n",
    "    model = LstmMle(batch_size=param[\"model\"][\"batch_size\"], \n",
    "                    input_dim=param[\"model\"][\"input_size\"], \n",
    "                    n_hidden_lstm=opt_n_hidden_lstm, \n",
    "                    n_layers=opt_n_lstm_layer,\n",
    "                    dropout_rate_lstm=param[\"model\"][\"dropout_rate_lstm\"],\n",
    "                    dropout_rate_fc=param[\"model\"][\"dropout_rate_fc\"],\n",
    "                    n_hidden_fc=opt_n_didden_fc\n",
    "                    )\n",
    "    checkpoint = torch.load(param_test[\"model_for_testset\"][\"path\"])\n",
    "    model.load_state_dict(checkpoint['model_state_dict'])\n",
    "\n",
    "    # Initialize DataSet\n",
    "    dataset_test = DataSet(test_data_scaled, timesteps=opt_sequence_size)\n",
    "\n",
    "    # Initialize DataLoader\n",
    "    data_loader_test = DataLoader(dataset_test, \n",
    "                                  batch_size=param[\"model\"][\"batch_size\"], \n",
    "                                  num_workers=0, \n",
    "                                  shuffle=True, \n",
    "                                  drop_last=True\n",
    "                                 )\n",
    "\n",
    "    # Define Loss Function\n",
    "    criterion = LossMle(param[\"model\"][\"input_size\"], param[\"model\"][\"batch_size\"])\n",
    "\n",
    "    # Initialize Tester\n",
    "    tester = Tester(model=model, criterion=criterion)\n",
    "                                                     \n",
    "    # Evaluate Testset\n",
    "    mean_test_loss = tester.evaluate(data_loader_test)\n",
    "    print(\"\\n\"+\"# #\"*30+\"\\n\")\n",
    "    print(\"Mean loss of test dataset is {}\".format(mean_test_loss))\n",
    "    print(\"\\n\"+\"# #\"*30+\"\\n\")\n",
    "    # Statistics of current fold\n",
    "    statistics_test = [\"fold \"+str(iteration+2),\n",
    "                 mean_test_loss, n_hidden_lstm, sequence_size,\n",
    "                 n_lstm_layer, n_hidden_fc]\n",
    "\n",
    "    # Safe statistics to .csv file\n",
    "    with open(param[\"filed_location\"][\"history_test\"], \"a\") as file:\n",
    "        for value in statistics_test:\n",
    "            file.write(str(value)+\";\")\n",
    "        file.write(\"\\n\")\n",
    "\n",
    "print(\"Cross Validation finished\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataframe = pd.read_csv(param[\"filed_location\"][\"history_test\"], sep=\";\")\n",
    "dataframe.head(20)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "masterarbeit",
   "language": "python",
   "name": "masterarbeit"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
