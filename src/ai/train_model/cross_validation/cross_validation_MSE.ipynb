{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader\n",
    "import torch.optim as optim\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import time\n",
    "\n",
    "# own Modules \n",
    "from models_mse import LstmMse\n",
    "from data_set import DataSet\n",
    "from cross_validation import CrossValidationProvider\n",
    "from scaler import DataScaler\n",
    "from trainer import Trainer\n",
    "from loss_module import LossMse\n",
    "from tester import Tester"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Paramerters artifical data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "param = {\n",
    "    \"data\" : {\n",
    "        \"path\" : '../../../../data/artifical_signals/artifical_2_signals.csv' ,\n",
    "    },\n",
    "    \"preprocessing\" : {\n",
    "        \"droped_features\": [\"ID\"\n",
    "                           ],\n",
    "        \"features_not_to_scale\": []\n",
    "    },\n",
    "    \"model\" : {\n",
    "        \"input_size\" : 2,\n",
    "        \"n_hidden_lstm\" : [10,20,50], \n",
    "        \"sequence_size\" : [10,20,30], \n",
    "        \"batch_size\" : 8,\n",
    "        \"lstm_layer\" : [1],\n",
    "        \"n_hidden_fc\": [20,40,60],\n",
    "        \"dropout_rate_lstm\": 0.0,\n",
    "        \"dropout_rate_fc\": 0.2\n",
    "    },\n",
    "    \"cycling_lr\" : {\n",
    "        \"scheduler_active\" : True, \n",
    "        # Mode can be one of {triangular, triangular2, exp_range}\n",
    "        \"mode\" : \"triangular\", \n",
    "        \"gamma\" : 0.9995,\n",
    "        \"base_lr\" : 0.0001, # 0.016, \n",
    "        \"max_lr\" :0.0005,  # 0.75\n",
    "    },\n",
    "    \"training\": {\n",
    "        \"stake_training_data\": 0.75,\n",
    "        \"total_number\" : 12000,\n",
    "        \"n_folds_cv\": 6,\n",
    "        \"n_epochs\" : 20,\n",
    "        \"patience\" : 7,\n",
    "    },\n",
    "    \"filed_location\": {\n",
    "        \"trained_model\" : \"../../../../models/cross_validation/MSE/artifical_data_\",\n",
    "        \"history_validation\" : \"../../../visualisation/files/cross_validation/MSE/validation_artifical_datacsv\",\n",
    "        \"history_test\" : \"../../../visualisation/files/cross_validation/MSE/test_artifical_data.csv\"\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Paramerters cpps data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "param = {\n",
    "    \"data\" : {\n",
    "        \"path\" : '../../../../data/cpps_degradation/large_degeneration/cpps_data_large_degeneration_training.csv' ,\n",
    "    },\n",
    "    \"preprocessing\" : {\n",
    "        \"droped_features\": [\"ID\"\n",
    "                           ],\n",
    "        \"features_not_to_scale\": []\n",
    "    },\n",
    "    \"model\" : {\n",
    "        \"input_size\" : 10,\n",
    "        \"n_hidden_lstm\" : [10,20,50], \n",
    "        \"sequence_size\" : [10,20,30], \n",
    "        \"batch_size\" : 8,\n",
    "        \"lstm_layer\" : [1],\n",
    "        \"n_hidden_fc\": [20,40,60],\n",
    "        \"dropout_rate_lstm\": 0.0,\n",
    "        \"dropout_rate_fc\": 0.2\n",
    "    },\n",
    "    \"cycling_lr\" : {\n",
    "        \"scheduler_active\" : True, \n",
    "        # Mode can be one of {triangular, triangular2, exp_range}\n",
    "        \"mode\" : \"triangular\", \n",
    "        \"gamma\" : 0.9995,\n",
    "        \"base_lr\" : 0.0001, # 0.016, \n",
    "        \"max_lr\" :0.0005,  # 0.75\n",
    "    },\n",
    "    \"training\": {\n",
    "        \"stake_training_data\": 0.75,\n",
    "        \"total_number\" : 50000,\n",
    "        \"n_folds_cv\": 6,\n",
    "        \"n_epochs\" : 20,\n",
    "        \"patience\" : 7,\n",
    "    },\n",
    "    \"filed_location\": {\n",
    "        \"trained_model\" : \"../../../../models/cross_validation/MSE/cpps_data_\",\n",
    "        \"history_validation\" : \"../../../visualisation/files/cross_validation/MSE/validation_cpps_data.csv\",\n",
    "        \"history_test\" : \"../../../visualisation/files/cross_validation/MSE/test_cpps_data.csv\"\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Paramerters phm data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "param = {\n",
    "    \"data\" : {\n",
    "        \"path\" : '../../../../data/phm_data_challenge/recipe/dataset_for_each_recipe/training/training_recipe_67.csv' ,\n",
    "    },\n",
    "    \"preprocessing\" : {\n",
    "        \"droped_features\": [\"ID\", \"stage\", \"Lot\", \"runnum\", \"recipe\", \"recipe_step\",\n",
    "                            \"up time\", \"ongoing time\", \n",
    "                            \"ETCHSOURCEUSAGE\", \"ETCHAUXSOURCETIMER\", \n",
    "                            \"ETCHAUX2SOURCETIMER\", \"FIXTURESHUTTERPOSITION\", \"ROTATIONSPEED\"\n",
    "                           ],\n",
    "        \"features_not_to_scale\": []\n",
    "    },\n",
    "    \"model\" : {\n",
    "        \"input_size\" : 12,\n",
    "        \"n_hidden_lstm\" : [10,20,50], \n",
    "        \"sequence_size\" : [10,20,30], \n",
    "        \"batch_size\" : 8,\n",
    "        \"lstm_layer\" : [1],\n",
    "        \"n_hidden_fc\": [20,40,60],\n",
    "        \"dropout_rate_lstm\": 0.0,\n",
    "        \"dropout_rate_fc\": 0.2\n",
    "    },\n",
    "    \"cycling_lr\" : {\n",
    "        \"scheduler_active\" : True, \n",
    "        # Mode can be one of {triangular, triangular2, exp_range}\n",
    "        \"mode\" : \"triangular\", \n",
    "        \"gamma\" : 0.9995,\n",
    "        \"base_lr\" : 0.0001, # 0.016, \n",
    "        \"max_lr\" :0.0005,  # 0.75\n",
    "    },\n",
    "    \"training\": {\n",
    "        \"total_number\" : 1000,\n",
    "        \"n_folds_cv\": 6,\n",
    "        \"n_epochs\" : 20,\n",
    "        \"patience\" : 7,\n",
    "    },\n",
    "    \"filed_location\": {\n",
    "        \"trained_model\" : \"../../../../models/cross_validation/MSE/phm_data_recipe_67_\",\n",
    "        \"history_validation\" : \"../../../visualisation/files/cross_validation/MSE/validation_phm_data_recipe_67.csv\",\n",
    "        \"history_test\" : \"../../../visualisation/files/cross_validation/MSE/test_phm_data_recipe_67.csv\"\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Nested Cross Validation for Time Series Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](../../../../knowledge/pictures/nested_cv.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Split Data into folds\n",
    "- ignored features are getting removed\n",
    "- remaining data are split up into folds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv_provider = CrossValidationProvider(path=param[\"data\"][\"path\"], \n",
    "                                      no_folds=param[\"training\"][\"n_folds_cv\"], \n",
    "                                      amount_data=param[\"training\"][\"total_number\"],\n",
    "                                      ignored_features = param['preprocessing']['droped_features']\n",
    "                                     )\n",
    "folds = cv_provider.provide_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cross Validation Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save scaler metrics for test dataset\n",
    "scaler_metrics = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Data : Fold 1-2\n",
      "Validation Data : Fold 3\n",
      "Test Data : Fold 4\n",
      "Amount Training Data: 334\n",
      "Amount Validation Data: 167\n",
      "Amount Test Data: 167\n",
      "- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -\n",
      "Start with new hyperparameters in grid search: \n",
      "Sequence_size: 10\n",
      "Number LSTM Layers: 1\n",
      "LSTM Number Hidden Dimensions: 10\n",
      "FC NN Number Hidden Dimensions: 20\n",
      "-------- epoch_no. 0 finished with eval loss 0.8281663750347338--------\n",
      "Epoch 0: best model saved with loss: 0.8281663750347338\n",
      "-------- epoch_no. 1 finished with eval loss 0.8173060417175293--------\n",
      "Epoch 1: best model saved with loss: 0.8173060417175293\n",
      "-------- epoch_no. 2 finished with eval loss 0.8082898259162903--------\n",
      "Epoch 2: best model saved with loss: 0.8082898259162903\n",
      "-------- epoch_no. 3 finished with eval loss 0.8006766344371595--------\n",
      "Epoch 3: best model saved with loss: 0.8006766344371595\n",
      "-------- epoch_no. 4 finished with eval loss 0.7955544151757893--------\n",
      "Epoch 4: best model saved with loss: 0.7955544151757893\n",
      "-------- epoch_no. 5 finished with eval loss 0.786872503004576--------\n",
      "Epoch 5: best model saved with loss: 0.786872503004576\n",
      "-------- epoch_no. 6 finished with eval loss 0.7770572994884691--------\n",
      "Epoch 6: best model saved with loss: 0.7770572994884691\n",
      "-------- epoch_no. 7 finished with eval loss 0.7720555035691512--------\n",
      "Epoch 7: best model saved with loss: 0.7720555035691512\n",
      "-------- epoch_no. 8 finished with eval loss 0.7651600335773668--------\n",
      "Epoch 8: best model saved with loss: 0.7651600335773668\n",
      "-------- epoch_no. 9 finished with eval loss 0.7587852823106866--------\n",
      "Epoch 9: best model saved with loss: 0.7587852823106866\n",
      "-------- epoch_no. 10 finished with eval loss 0.7506374904983922--------\n",
      "Epoch 10: best model saved with loss: 0.7506374904983922\n",
      "-------- epoch_no. 11 finished with eval loss 0.7437954388166729--------\n",
      "Epoch 11: best model saved with loss: 0.7437954388166729\n",
      "-------- epoch_no. 12 finished with eval loss 0.7399128079414368--------\n",
      "Epoch 12: best model saved with loss: 0.7399128079414368\n",
      "-------- epoch_no. 13 finished with eval loss 0.7316949775344447--------\n",
      "Epoch 13: best model saved with loss: 0.7316949775344447\n",
      "-------- epoch_no. 14 finished with eval loss 0.7251511216163635--------\n",
      "Epoch 14: best model saved with loss: 0.7251511216163635\n",
      "-------- epoch_no. 15 finished with eval loss 0.7191844488445082--------\n",
      "Epoch 15: best model saved with loss: 0.7191844488445082\n",
      "-------- epoch_no. 16 finished with eval loss 0.7143931514338443--------\n",
      "Epoch 16: best model saved with loss: 0.7143931514338443\n",
      "-------- epoch_no. 17 finished with eval loss 0.7087233223413166--------\n",
      "Epoch 17: best model saved with loss: 0.7087233223413166\n",
      "-------- epoch_no. 18 finished with eval loss 0.7022593021392822--------\n",
      "Epoch 18: best model saved with loss: 0.7022593021392822\n",
      "-------- epoch_no. 19 finished with eval loss 0.6952317357063293--------\n",
      "Epoch 19: best model saved with loss: 0.6952317357063293\n",
      "\n",
      "# ## ## ## ## ## ## ## ## ## ## ## ## ## ## ## ## ## ## ## ## ## ## ## ## ## ## ## ## ## #\n",
      "\n",
      "Start with new hyperparameters in grid search: \n",
      "Sequence_size: 10\n",
      "Number LSTM Layers: 1\n",
      "LSTM Number Hidden Dimensions: 10\n",
      "FC NN Number Hidden Dimensions: 40\n",
      "-------- epoch_no. 0 finished with eval loss 0.6928614942651046--------\n",
      "Epoch 0: best model saved with loss: 0.6928614942651046\n",
      "-------- epoch_no. 1 finished with eval loss 0.6865336612651223--------\n",
      "Epoch 1: best model saved with loss: 0.6865336612651223\n",
      "-------- epoch_no. 2 finished with eval loss 0.6802522069529483--------\n",
      "Epoch 2: best model saved with loss: 0.6802522069529483\n",
      "-------- epoch_no. 3 finished with eval loss 0.6743318971834684--------\n",
      "Epoch 3: best model saved with loss: 0.6743318971834684\n",
      "-------- epoch_no. 4 finished with eval loss 0.6718262935939588--------\n",
      "Epoch 4: best model saved with loss: 0.6718262935939588\n",
      "-------- epoch_no. 5 finished with eval loss 0.6647262635983919--------\n",
      "Epoch 5: best model saved with loss: 0.6647262635983919\n",
      "-------- epoch_no. 6 finished with eval loss 0.6609352952555606--------\n",
      "Epoch 6: best model saved with loss: 0.6609352952555606\n",
      "-------- epoch_no. 7 finished with eval loss 0.6571177087332073--------\n",
      "Epoch 7: best model saved with loss: 0.6571177087332073\n",
      "-------- epoch_no. 8 finished with eval loss 0.6543205819631878--------\n",
      "Epoch 8: best model saved with loss: 0.6543205819631878\n",
      "-------- epoch_no. 9 finished with eval loss 0.6496263993413824--------\n",
      "Epoch 9: best model saved with loss: 0.6496263993413824\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-8-334b8d46dd00>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    104\u001b[0m                     \u001b[0;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparam\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'training'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'n_epochs'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    105\u001b[0m                         \u001b[0;31m# Train\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 106\u001b[0;31m                         \u001b[0mmean_epoch_training_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_loader_training\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    107\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    108\u001b[0m                         \u001b[0;31m# Evaluate\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/jupyter_notebooks/masterarbeit/src/ai/utils/trainer.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, data_loader_training)\u001b[0m\n\u001b[1;32m     48\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     49\u001b[0m             \u001b[0;31m# Backward pass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 50\u001b[0;31m             \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     51\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     52\u001b[0m             \u001b[0;31m# Update parameters\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/jupyter_notebooks/masterarbeit/venv_pm/lib/python3.6/site-packages/torch/tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph)\u001b[0m\n\u001b[1;32m    105\u001b[0m                 \u001b[0mproducts\u001b[0m\u001b[0;34m.\u001b[0m \u001b[0mDefaults\u001b[0m \u001b[0mto\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    106\u001b[0m         \"\"\"\n\u001b[0;32m--> 107\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    108\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    109\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/jupyter_notebooks/masterarbeit/venv_pm/lib/python3.6/site-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables)\u001b[0m\n\u001b[1;32m     91\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[1;32m     92\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 93\u001b[0;31m         allow_unreachable=True)  # allow_unreachable flag\n\u001b[0m\u001b[1;32m     94\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     95\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Create file where validation results are stored and add header\n",
    "column_names_validation = [\"iteration\",\"validation_fold\",\"validation_loss\", \n",
    "                           \"n_hidden_lstm\", \"sequence_size\", \"n_lstm_layer\", \n",
    "                           \"n_hidden_fc\", \"training phase\"]\n",
    "with open(param[\"filed_location\"][\"history_validation\"], \"a+\") as file:\n",
    "    [file.write(column+\";\") for column in column_names_validation]\n",
    "    file.write(\"\\n\")\n",
    "            \n",
    "# Create file where test results are stored and add header\n",
    "column_names_test = [\"test_fold\", \"test_loss\", \"n_hidden_lstm\", \"sequence_size\",\n",
    "                     \"n_lstm_layer\", \"n_hidden_fc\"]\n",
    "with open(param[\"filed_location\"][\"history_test\"], \"a+\") as file:\n",
    "    [file.write(column+\";\") for column in column_names_test]\n",
    "    file.write(\"\\n\")\n",
    "         \n",
    "for iteration in range (2, param[\"training\"][\"n_folds_cv\"]):\n",
    "    # Select folds for current iteration\n",
    "    training_folds = folds[:iteration]\n",
    "    validation_fold = folds[iteration:iteration+1]\n",
    "    test_fold = folds[iteration:iteration+2]\n",
    "    print(\"Training Data : Fold 1-\" + str(iteration))\n",
    "    print(\"Validation Data : Fold \"+ str(iteration+1))\n",
    "    print(\"Test Data : Fold \"+ str(iteration+2))\n",
    "    \n",
    "    # Concate data of training folds and unpack validation and testdata\n",
    "    raw_training_data = pd.concat(training_folds, axis = 0, ignore_index=True)\n",
    "    raw_validation_data = validation_fold[0]\n",
    "    raw_test_data = test_fold[0]\n",
    "    print(\"Amount Training Data: {}\".format(raw_training_data.shape[0]))\n",
    "    print(\"Amount Validation Data: {}\".format(raw_validation_data.shape[0]))\n",
    "    print(\"Amount Test Data: {}\".format(raw_test_data.shape[0]))\n",
    "    print(\"- -\"*30)\n",
    "    \n",
    "    # Scale training data and validation data (validation data with mean and variance of training data)\n",
    "    scaler = DataScaler(features_not_to_scale= param['preprocessing']['features_not_to_scale'])\n",
    "    train_data_scaled, validation_data_scaled, test_data_scaled = scaler.scale_data(raw_training_data, raw_validation_data, raw_test_data)\n",
    "    \n",
    "    # Training model and test hyperparameter on validation set\n",
    "    for n_lstm_layer in param[\"model\"][\"lstm_layer\"]:\n",
    "        for sequence_size  in param[\"model\"][\"sequence_size\"]:\n",
    "            # Initialize DataSet\n",
    "            dataset_train = DataSet(train_data_scaled, timesteps=sequence_size)\n",
    "            dataset_validation = DataSet(validation_data_scaled, timesteps=sequence_size)\n",
    "            \n",
    "            # Initialize DataLoader\n",
    "            data_loader_training = DataLoader(dataset_train, \n",
    "                                              batch_size=param[\"model\"][\"batch_size\"], \n",
    "                                              num_workers=0, \n",
    "                                              shuffle=True, \n",
    "                                              drop_last=True\n",
    "                                             )\n",
    "            data_loader_validation = DataLoader(dataset_validation, \n",
    "                                                batch_size=param[\"model\"][\"batch_size\"], \n",
    "                                                num_workers=0, \n",
    "                                                shuffle=True, \n",
    "                                                drop_last=True\n",
    "                                               )\n",
    "            \n",
    "            for n_hidden_lstm in param[\"model\"][\"n_hidden_lstm\"]:\n",
    "                for n_hidden_fc in param[\"model\"][\"n_hidden_fc\"]:\n",
    "                    print(\"Start with new hyperparameters in grid search: \")\n",
    "                    print(\"Sequence_size: {}\".format(sequence_size))\n",
    "                    print(\"Number LSTM Layers: {}\".format(n_lstm_layer))\n",
    "                    print(\"LSTM Number Hidden Dimensions: {}\".format(n_hidden_lstm))\n",
    "                    print(\"FC NN Number Hidden Dimensions: {}\".format(n_hidden_fc))\n",
    "                    \n",
    "                    hist_loss = []\n",
    "                    torch.manual_seed(0)\n",
    "                    model = LstmMse(batch_size=param['model']['batch_size'], \n",
    "                                    input_dim=param['model']['input_size'], \n",
    "                                    n_hidden_lstm=n_hidden_lstm, \n",
    "                                    n_layers=n_lstm_layer,\n",
    "                                    dropout_rate_lstm= param['model']['dropout_rate_lstm'],\n",
    "                                    dropout_rate_fc= param['model']['dropout_rate_fc'],\n",
    "                                    n_hidden_fc=n_hidden_fc\n",
    "                                    )\n",
    "\n",
    "                    # Define Loss Function\n",
    "                    criterion = LossMse(param[\"model\"][\"input_size\"], param[\"model\"][\"batch_size\"])\n",
    "\n",
    "                    # Initialize Optimizer and Cyclic Learning Rate Scheduler\n",
    "                    optimizer = torch.optim.SGD(model.parameters(), lr=1.)  \n",
    "                    scheduler = torch.optim.lr_scheduler.CyclicLR(optimizer=optimizer, \n",
    "                                                                  base_lr=param['cycling_lr']['base_lr'], \n",
    "                                                                  max_lr=param['cycling_lr']['max_lr'], \n",
    "                                                                  step_size_up=(raw_training_data.shape[0]/8)*2, \n",
    "                                                                  mode=param['cycling_lr']['mode'],\n",
    "                                                                  gamma=param['cycling_lr']['gamma']\n",
    "                                                                  )\n",
    "                    # Initialize Trainer\n",
    "                    trainer = Trainer(model=model,\n",
    "                                      optimizer=optimizer,\n",
    "                                      scheduler=scheduler,\n",
    "                                      scheduler_active = param[\"cycling_lr\"][\"scheduler_active\"],\n",
    "                                      criterion=criterion, \n",
    "                                      location_model=param[\"filed_location\"][\"trained_model\"], \n",
    "                                      location_stats=param[\"filed_location\"][\"history_validation\"], \n",
    "                                      patience=param['training']['patience']\n",
    "                                     )\n",
    "\n",
    "                    # Measure training time for current configuration\n",
    "                    start = time.time()\n",
    "\n",
    "                    for epoch in range(param['training']['n_epochs']):\n",
    "                        # Train\n",
    "                        mean_epoch_training_loss = trainer.train(data_loader_training)\n",
    "\n",
    "                        # Evaluate\n",
    "                        mean_epoch_validation_loss = trainer.evaluate(data_loader_validation, hist_loss, epoch)\n",
    "\n",
    "                        # Cache History\n",
    "                        trainer.cache_history_training(hist_loss, epoch, mean_epoch_training_loss, mean_epoch_validation_loss)\n",
    "\n",
    "                        # Save model if its the best one since the last change in configuration of hyperparameters\n",
    "                        trainer.fold = \"Fold_1-\"+str(iteration)\n",
    "                        status_ok, path_model = trainer.save_model(epoch, mean_epoch_validation_loss, param['model']['input_size'], \n",
    "                                                       n_lstm_layer, n_hidden_lstm, n_hidden_fc, sequence_size)\n",
    "                        current_path_model_phase_1 = path_model\n",
    "\n",
    "                        if not status_ok or epoch == (param['training']['n_epochs'])-1:\n",
    "                            # Statistics of current fold\n",
    "                            statistics_validation = [iteration,\"fold \"+str(iteration+1),\n",
    "                                                     trainer.lowest_loss, n_hidden_lstm, sequence_size,\n",
    "                                                     n_lstm_layer, n_hidden_fc]\n",
    "\n",
    "                            # Safe statistics to .csv file\n",
    "                            with open(param[\"filed_location\"][\"history_validation\"], \"a\") as file:\n",
    "                                for value in statistics_validation:\n",
    "                                    file.write(str(value)+\";\")\n",
    "                                file.write(\"\\n\")\n",
    "                            break       \n",
    "                    print(\"\\n\"+\"# #\"*30+\"\\n\")\n",
    "                    \n",
    "    # Test model with best hyperparameters of current fold\n",
    "    df_hyperparameters = pd.read_csv(param[\"filed_location\"][\"history_validation\"])\n",
    "    current_fold = df_hyperparameters.loc[df_hyperparameters.iteration == iteration]\n",
    "    # Get index of row with lowest validation loss\n",
    "    idx_best_hyperparm = current_fold.lowest_validation_loss.idxmin()\n",
    "    opt_n_hidden_lstm = current_fold.iloc[idx_best_hyperparm, 4]\n",
    "    opt_sequence_size = current_fold.iloc[idx_best_hyperparm, 5]\n",
    "    opt_n_lstm_layer = current_fold.iloc[idx_best_hyperparm, 6]\n",
    "    opt_n_didden_fc = current_fold.iloc[idx_best_hyperparm, 7]\n",
    "    \n",
    "    # Load best model\n",
    "    torch.manual_seed(0)\n",
    "    model = LstmMse(batch_size=param['model']['batch_size'], \n",
    "                    input_dim=param['model']['input_size'], \n",
    "                    n_hidden_lstm=n_hidden_lstm, \n",
    "                    n_layers=n_lstm_layer,\n",
    "                    dropout_rate_lstm= param['model']['dropout_rate_lstm'],\n",
    "                    dropout_rate_fc= param['model']['dropout_rate_fc'],\n",
    "                    n_hidden_fc=n_hidden_fc\n",
    "                    )\n",
    "    checkpoint = torch.load(param_test[\"model_for_testset\"][\"path\"])\n",
    "    model.load_state_dict(checkpoint['model_state_dict'])\n",
    "\n",
    "    # Initialize DataSet\n",
    "    dataset_test = DataSet(test_data_scaled, timesteps=opt_sequence_size)\n",
    "\n",
    "    # Initialize DataLoader\n",
    "    data_loader_test = DataLoader(dataset_test, \n",
    "                                  batch_size=param[\"model\"][\"batch_size\"], \n",
    "                                  num_workers=0, \n",
    "                                  shuffle=True, \n",
    "                                  drop_last=True\n",
    "                                 )\n",
    "\n",
    "    # Define Loss Function\n",
    "    criterion = LossMse(param[\"model\"][\"input_size\"], param[\"model\"][\"batch_size\"])\n",
    "\n",
    "    # Initialize Tester\n",
    "    tester = Tester(model=model, criterion=criterion)\n",
    "                                                     \n",
    "    # Evaluate Testset\n",
    "    mean_test_loss = tester.evaluate(data_loader_test)\n",
    "    print(\"\\n\"+\"# #\"*30+\"\\n\")\n",
    "    print(\"Mean loss of test dataset is {}\".format(mean_test_loss))\n",
    "    print(\"\\n\"+\"# #\"*30+\"\\n\")\n",
    "    # Statistics of current fold\n",
    "    statistics_test = [\"fold \"+str(iteration+2),\n",
    "                 mean_test_loss, n_hidden_lstm, sequence_size,\n",
    "                 n_lstm_layer, n_hidden_fc]\n",
    "\n",
    "    # Safe statistics to .csv file\n",
    "    with open(param[\"filed_location\"][\"history_test\"], \"a\") as file:\n",
    "        for value in statistics_test:\n",
    "            file.write(str(value)+\";\")\n",
    "        file.write(\"\\n\")\n",
    "\n",
    "print(\"Cross Validation finished\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataframe = pd.read_csv(param[\"filed_location\"][\"history_test\"], sep=\";\")\n",
    "dataframe.head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "masterarbeit",
   "language": "python",
   "name": "masterarbeit"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
