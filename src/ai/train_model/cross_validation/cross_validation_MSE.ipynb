{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader\n",
    "import torch.optim as optim\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import time\n",
    "\n",
    "# own Modules \n",
    "from models_mse import LstmMse\n",
    "from data_set import DataSet\n",
    "from cross_validation import CrossValidationProvider\n",
    "from scaler import DataScaler\n",
    "from trainer import Trainer\n",
    "from loss_module import LossMse\n",
    "from tester import Tester"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Take care of these things before training:\n",
    "- Select correct path and define droped_features\n",
    "- Change parameter of model\n",
    "- Change step_size in cycling_lr\n",
    "- Change filed_location"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "param = {\n",
    "    \"data\" : {\n",
    "        \"path\" : '../../../../data/phm_data_challenge/01_M01_DC_prediction_1.csv' ,\n",
    "    },\n",
    "    \"preprocessing\" : {\n",
    "        \"droped_features\": [\"ID\", \"stage\", \"Lot\", \"runnum\", \"recipe\", \"recipe_step\",\n",
    "                            \"up time\", \"ongoing time\", \n",
    "                            \"ETCHSOURCEUSAGE\", \"ETCHAUXSOURCETIMER\", \n",
    "                            \"ETCHAUX2SOURCETIMER\", \"FIXTURESHUTTERPOSITION\"\n",
    "                           ],\n",
    "        \"features_not_to_scale\": []\n",
    "    },\n",
    "    \"model\" : {\n",
    "        \"input_size\" : 13,\n",
    "        \"n_hidden_lstm\" : [100],\n",
    "        \"sequence_size\" : [100],\n",
    "        \"batch_size\" : 8,\n",
    "        \"lstm_layer\" : [2],\n",
    "        \"n_hidden_fc\": [50],\n",
    "        \"dropout_rate_lstm\": 0.0,\n",
    "        \"dropout_rate_fc\": 0.2\n",
    "    },\n",
    "    \"cycling_lr\" : {\n",
    "        \"scheduler_active\" : True, \n",
    "        # step_size is the number of training iterations (total samples/batch_size) per half cycle. \n",
    "        # Authors suggest setting step_size 2-8 x training iterations in epoch.\n",
    "        \"step_size\" : (12500/8)*2, \n",
    "        # Mode can be one of {triangular, triangular2, exp_range}\n",
    "        \"mode\" : \"triangular\", \n",
    "        \"gamma\" : 0.9995,\n",
    "        \"base_lr\" : 0.016, \n",
    "        \"max_lr\" :0.75\n",
    "    },\n",
    "    \"training\": {\n",
    "        \"stake_training_data\": 0.75,\n",
    "        \"total_number\" : 50000,\n",
    "        \"n_folds_cv\": 5,\n",
    "        \"n_epochs\" : 100,\n",
    "        \"patience\" : 10,\n",
    "    },\n",
    "    \"filed_location\": {\n",
    "        \"trained_model\" : \"../../../../models/MSE_model/xxxx\",\n",
    "        \"history\" : \"../../../visualisation/files/cross_validation/MSE/xxxx.csv\"\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](../../../knowledge/pictures/nested_cv.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Split Data into folds\n",
    "- ignored features are getting removed\n",
    "- remaining data are split up into folds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'CrossValidationProvider' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-faff2df0a39a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m cv_provider = CrossValidationProvider(path=param[\"data\"][\"path\"], \n\u001b[0m\u001b[1;32m      2\u001b[0m                                       \u001b[0mno_folds\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mparam\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"training\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"n_folds_cv\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m                                       \u001b[0mamount_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mparam\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"training\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"total_number\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m                                       \u001b[0mstake_training_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mparam\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"training\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"stake_training_data\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m                                       \u001b[0mignored_features\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mparam\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'preprocessing'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'droped_features'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'CrossValidationProvider' is not defined"
     ]
    }
   ],
   "source": [
    "cv_provider = CrossValidationProvider(path=param[\"data\"][\"path\"], \n",
    "                                      no_folds=param[\"training\"][\"n_folds_cv\"], \n",
    "                                      amount_data=param[\"training\"][\"total_number\"],\n",
    "                                      stake_training_data = param[\"training\"][\"stake_training_data\"],\n",
    "                                      ignored_features = param['preprocessing']['droped_features']\n",
    "                                     )\n",
    "test_data, folds = cv_provider.provide_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cross Validation Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Data : Fold 1-2\n",
      "Validation Data : Fold 3\n",
      "Amount Training Data: 15000\n",
      "Amount Validation Data: 7500\n",
      "- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -\n",
      "Start with new hyperparameters in grid search: \n",
      "Sequence_size: 100\n",
      "Number LSTM Layers: 2\n",
      "LSTM Number Hidden Dimensions: 100\n",
      "FC NN Number Hidden Dimensions: 50\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-4-b3e6cbf4b762>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     86\u001b[0m                     \u001b[0;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparam\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'training'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'n_epochs'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     87\u001b[0m                         \u001b[0;31m# Train\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 88\u001b[0;31m                         \u001b[0mmean_epoch_training_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_loader_training\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     89\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     90\u001b[0m                         \u001b[0;31m# Evaluate\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/jupyter_notebooks/masterarbeit/src/ai/utils/trainer.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, data_loader_training)\u001b[0m\n\u001b[1;32m     49\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     50\u001b[0m             \u001b[0;31m# Backward pass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 51\u001b[0;31m             \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     52\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m             \u001b[0;31m# Update parameters\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/jupyter_notebooks/masterarbeit/venv_pm/lib/python3.6/site-packages/torch/tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph)\u001b[0m\n\u001b[1;32m    105\u001b[0m                 \u001b[0mproducts\u001b[0m\u001b[0;34m.\u001b[0m \u001b[0mDefaults\u001b[0m \u001b[0mto\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    106\u001b[0m         \"\"\"\n\u001b[0;32m--> 107\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    108\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    109\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/jupyter_notebooks/masterarbeit/venv_pm/lib/python3.6/site-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables)\u001b[0m\n\u001b[1;32m     91\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[1;32m     92\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 93\u001b[0;31m         allow_unreachable=True)  # allow_unreachable flag\n\u001b[0m\u001b[1;32m     94\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     95\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "statistics_folds = []\n",
    "for iteration in range (2, param[\"training\"][\"n_folds_cv\"]-1):\n",
    "    # Select folds for current iteration\n",
    "    training_folds = folds[:iteration]\n",
    "    validation_fold = folds[iteration:iteration+1]\n",
    "    print(\"Training Data : Fold 1-\" + str(iteration))\n",
    "    print(\"Validation Data : Fold \"+ str(iteration+1))\n",
    "    \n",
    "    # Concate data of training folds and unpack validation data\n",
    "    raw_training_data = pd.concat(training_folds, axis = 0, ignore_index=True)\n",
    "    raw_validation_data = validation_fold[0]\n",
    "    print(\"Amount Training Data: {}\".format(raw_training_data.shape[0]))\n",
    "    print(\"Amount Validation Data: {}\".format(raw_validation_data.shape[0]))\n",
    "    print(\"- -\"*30)\n",
    "    \n",
    "    # Scale training data and validation data (validation data with mean and variance of training data)\n",
    "    scaler = DataScaler(features_not_to_scale= param['preprocessing']['features_not_to_scale'])\n",
    "    train_data_scaled, validation_data_scaled = scaler.scale_data(raw_training_data, raw_validation_data)\n",
    "    \n",
    "    # Start Training\n",
    "    for n_lstm_layer in param[\"model\"][\"lstm_layer\"]:\n",
    "        for sequence_size  in param[\"model\"][\"sequence_size\"]:\n",
    "            # Initialize DataSet\n",
    "            dataset_train = DataSet(train_data_scaled, timesteps=sequence_size)\n",
    "            dataset_validation = DataSet(validation_data_scaled, timesteps=sequence_size)\n",
    "            \n",
    "            # Initialize DataLoader\n",
    "            data_loader_training = DataLoader(dataset_train, \n",
    "                                              batch_size=param[\"model\"][\"batch_size\"], \n",
    "                                              num_workers=0, \n",
    "                                              shuffle=True, \n",
    "                                              drop_last=True\n",
    "                                             )\n",
    "            data_loader_validation = DataLoader(dataset_validation, \n",
    "                                                batch_size=param[\"model\"][\"batch_size\"], \n",
    "                                                num_workers=0, \n",
    "                                                shuffle=True, \n",
    "                                                drop_last=True\n",
    "                                               )\n",
    "            \n",
    "            for n_hidden_lstm in param[\"model\"][\"n_hidden_lstm\"]:\n",
    "                for n_hidden_fc in param[\"model\"][\"n_hidden_fc\"]:\n",
    "                    print(\"Start with new hyperparameters in grid search: \")\n",
    "                    print(\"Sequence_size: {}\".format(sequence_size))\n",
    "                    print(\"Number LSTM Layers: {}\".format(n_lstm_layer))\n",
    "                    print(\"LSTM Number Hidden Dimensions: {}\".format(n_hidden_lstm))\n",
    "                    print(\"FC NN Number Hidden Dimensions: {}\".format(n_hidden_fc))\n",
    "\n",
    "                    # Create lists to save training loss and validation loss of each epoch\n",
    "                    hist_loss = []\n",
    "                    torch.manual_seed(0)\n",
    "                    model = LstmMse(batch_size=param['model']['batch_size'], \n",
    "                                    input_dim=param['model']['input_size'], \n",
    "                                    n_hidden_lstm=n_hidden_lstm, \n",
    "                                    n_layers=n_lstm_layer,\n",
    "                                    dropout_rate_lstm= param['model']['dropout_rate_lstm'],\n",
    "                                    dropout_rate_fc= param['model']['dropout_rate_fc'],\n",
    "                                    n_hidden_fc=n_hidden_fc\n",
    "                                    )\n",
    "\n",
    "                    # Define Loss Function\n",
    "                    criterion = LossMse(param[\"model\"][\"input_size\"], param[\"model\"][\"batch_size\"])\n",
    "\n",
    "                    # Initialize Optimizer and Cyclic Learning Rate Scheduler\n",
    "                    optimizer = torch.optim.SGD(model.parameters(), lr=1.)  \n",
    "                    scheduler = torch.optim.lr_scheduler.CyclicLR(optimizer=optimizer, \n",
    "                                                                  base_lr=param['cycling_lr']['base_lr'], \n",
    "                                                                  max_lr=param['cycling_lr']['max_lr'], \n",
    "                                                                  step_size_up=param['cycling_lr']['step_size'], \n",
    "                                                                  mode=param['cycling_lr']['mode'],\n",
    "                                                                  gamma=param['cycling_lr']['gamma']\n",
    "                                                                  )\n",
    "                    # Initialize Trainer\n",
    "                    trainer = Trainer(model=model,\n",
    "                                      optimizer=optimizer,\n",
    "                                      scheduler=scheduler,\n",
    "                                      scheduler_active = param[\"cycling_lr\"][\"scheduler_active\"],\n",
    "                                      criterion=criterion, \n",
    "                                      location_model=param[\"filed_location\"][\"trained_model\"], \n",
    "                                      location_stats=param[\"filed_location\"][\"history\"], \n",
    "                                      patience=param['training']['patience']\n",
    "                                     )\n",
    "                    \n",
    "                    # Measure training time for current configuration\n",
    "                    start = time.time()\n",
    "\n",
    "                    for epoch in range(param['training']['n_epochs']):\n",
    "                        # Train\n",
    "                        mean_epoch_training_loss = trainer.train(data_loader_training)\n",
    "\n",
    "                        # Evaluate\n",
    "                        mean_epoch_validation_loss = trainer.evaluate(data_loader_validation, hist_loss, epoch)\n",
    "\n",
    "                        # Save model if its the best one since the last change in configuration of hyperparameters\n",
    "                        trainer.fold = \"Fold 1-\"+str(iteration)\n",
    "                        status_ok = trainer.save_model(epoch, mean_epoch_validation_loss, param['model']['input_size'], \n",
    "                                                       n_lstm_layer, n_hidden_lstm, n_hidden_fc, sequence_size)\n",
    "                        if not status_ok:\n",
    "                            statistics = {\"training_folds\": \"fold 1-\"+str(iteration),\n",
    "                                          \"validation_folds\":\"fold \"+str(iteration+1),\n",
    "                                          \"lowest_loss\": trainer.lowest_loss, \n",
    "                                          \"n_hidden_lstm\" : n_hidden_lstm,\n",
    "                                          \"sequence_size\" : sequence_size,\n",
    "                                          \"lstm_layer\" : n_lstm_layer,\n",
    "                                          \"n_hidden_fc\": n_hidden_fc\n",
    "                                         }\n",
    "                            statistics_folds.append(statistics)\n",
    "                            break\n",
    "\n",
    "                    # Time in minutes\n",
    "                    execution_time = (time.time() - start)/60\n",
    "\n",
    "                    # Save training statistics \n",
    "                    trainer.save_statistic(hist_loss, sequence_size, n_lstm_layer, n_hidden_lstm, n_hidden_fc, execution_time)\n",
    "                    \n",
    "                    # Safe results to csv file\n",
    "                    df = pd.DataFrame(statistics_folds)\n",
    "                    df.to_csv(param[\"filed_location\"][\"history\"], sep=\";\", index=False)\n",
    "                    \n",
    "                    print(\"# #\"*30) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test best Model with test data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Select best model of training phase and change parameter accordingly:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_test = {\n",
    "    \"model_for_testset\" : {\n",
    "        \"path\" : \"../../../models/MSE_model/cross_validation/phm_data_large_InputSize13_LayerLstm2_HiddenLstm100_HiddenFc50_Seq150.pt\",\n",
    "        \"input_size\" : 13,\n",
    "        \"n_hidden_lstm\" : 100,\n",
    "        \"sequence_size\" : 150,\n",
    "        \"batch_size\" : 8,\n",
    "        \"lstm_layer\" : 2,\n",
    "        \"n_hidden_fc\": 50,\n",
    "        \"dropout_rate\": 0.2\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Scale values of best model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_training_data =[ 0.0632522,0.10388593, 0.09563544, 0.0777276, 0.22081628, 0.08311531, 0.01382531,\n",
    "                     0.09862897, 0.07814727, -0.0185826, 0.1000127, -0.0161782, -0.22541928]\n",
    "var_training_data =[0.90316232, 0.97237671, 0.98547017, 0.92090347, 1.18086523, 0.92393987,\n",
    "                    0.41744699, 0.97142703, 0.92604794, 0.68786855, 1.25019607, 0.50023143, 0.69425608]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0547518821002231\n"
     ]
    }
   ],
   "source": [
    "# Scale test data with mean and variance of training data\n",
    "scaler = DataScaler(features_not_to_scale= param['preprocessing']['features_not_to_scale'])\n",
    "test_data_scaled = scaler.scale_data_test_dataset(test_data, mean_training_data, var_training_data)\n",
    "\n",
    "# Initialize DataSet\n",
    "dataset_test = DataSet(test_data_scaled, timesteps=param_test[\"model_for_testset\"][\"sequence_size\"])\n",
    "\n",
    "# Initialize DataLoader\n",
    "data_loader_test = DataLoader(dataset_test, \n",
    "                              batch_size=param_test[\"model_for_testset\"][\"batch_size\"], \n",
    "                              num_workers=0, \n",
    "                              shuffle=True, \n",
    "                              drop_last=True\n",
    "                             )\n",
    "    \n",
    "# Create lists to save training loss and validation loss of each epoch\n",
    "torch.manual_seed(0)\n",
    "model = LstmMse(batch_size=param_test['model_for_testset']['batch_size'], \n",
    "                input_dim=param_test['model_for_testset']['input_size'], \n",
    "                n_hidden_lstm=param_test['model_for_testset']['n_hidden_lstm'], \n",
    "                n_layers=param_test['model_for_testset']['lstm_layer'],\n",
    "                dropout_rate= param_test['model_for_testset']['dropout_rate'],\n",
    "                n_hidden_fc=param_test['model_for_testset']['n_hidden_fc']\n",
    "                )\n",
    "\n",
    "checkpoint = torch.load(param_test[\"model_for_testset\"][\"path\"])\n",
    "model.load_state_dict(checkpoint['model_state_dict'])\n",
    "\n",
    "# Define Loss Function\n",
    "criterion = LossMse(param[\"model\"][\"input_size\"], param[\"model\"][\"batch_size\"])\n",
    "\n",
    "# Initialize Tester\n",
    "tester = Tester(model=model,\n",
    "                criterion=criterion\n",
    "                )\n",
    "\n",
    "# Evaluate Testset\n",
    "mean_test_loss = tester.evaluate(data_loader_test)\n",
    "print(mean_test_loss)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "masterarbeit",
   "language": "python",
   "name": "masterarbeit"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
