{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data import ConcatDataset\n",
    "import torch.optim as optim\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import time\n",
    "from random import randint\n",
    "\n",
    "# own Modules \n",
    "from models_mse import LstmMse\n",
    "from data_set import DataSet\n",
    "from cross_validation import CrossValidationProvider\n",
    "from scaler import DataScaler\n",
    "from trainer import Trainer\n",
    "from loss_module import LossMse\n",
    "from tester import Tester\n",
    "from logger import Logger"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Paramerters artifical data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "param = {\n",
    "    \"data\" : {\n",
    "        \"path\" : '../../../../data/artifical_signals/artifical_2_signals.csv' ,\n",
    "    },\n",
    "    \"preprocessing\" : {\n",
    "        \"droped_features\": [\"ID\"\n",
    "                           ],\n",
    "        \"features_not_to_scale\": []\n",
    "    },\n",
    "    \"model\" : {\n",
    "        \"input_size\" : 2,\n",
    "        \"n_hidden_lstm\" : [20], \n",
    "        \"sequence_size\" : [10,20], \n",
    "        \"batch_size\" : 8,\n",
    "        \"lstm_layer\" : [1],\n",
    "        \"n_hidden_fc\": [20,40],\n",
    "        \"dropout_rate_lstm\": 0.0,\n",
    "        \"dropout_rate_fc\": 0.2\n",
    "    },\n",
    "    \"cycling_lr\" : {\n",
    "        \"scheduler_active\" : True, \n",
    "        # Mode can be one of {triangular, triangular2, exp_range}\n",
    "        \"mode\" : \"triangular\", \n",
    "        \"gamma\" : 0.9995,\n",
    "        \"base_lr\" : 0.0001, # 0.016, \n",
    "        \"max_lr\" :0.0005,  # 0.75\n",
    "    },\n",
    "    \"training\": {\n",
    "        \"stake_training_data\": 0.75,\n",
    "        \"total_number\" : 6000,\n",
    "        \"n_folds_cv\": 2,\n",
    "        \"n_epochs\" : 2,\n",
    "        \"patience\" : 7,\n",
    "    },\n",
    "    \"filed_location\": {\n",
    "        \"model_for_test\" : \"../../../../models/cross_validation/MSE/artifical_data_\",\n",
    "        \"log_file\" : \"../../../../models/cross_validation/MSE/artifical_data_log_\", \n",
    "        \"history_trainval\" : \"../../../visualisation/files/cross_validation/MSE/artifical_data_trainval.csv\",\n",
    "        \"history_best_configuration\" : \"../../../visualisation/files/cross_validation/MSE/artifical_data_configurations.csv\"\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Paramerters cpps data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "param = {\n",
    "    \"data\" : {\n",
    "        \"path\" : '../../../../data/cpps_degradation_new/data_obs10/train/obs_space_train_sinusiod_preprocessed.csv',\n",
    "    },\n",
    "    \"preprocessing\" : {\n",
    "        \"droped_features\": [\"ID\"\n",
    "                           ],\n",
    "        \"features_not_to_scale\": []\n",
    "    },\n",
    "    \"model\" : {\n",
    "        \"input_size\" : 10,\n",
    "        \"n_hidden_lstm\" : [10,50], \n",
    "        \"sequence_size\" : [10,50], \n",
    "        \"batch_size\" : 8,\n",
    "        \"lstm_layer\" : [1],\n",
    "        \"n_hidden_fc\": [60],\n",
    "        \"dropout_rate_lstm\": 0.0,\n",
    "        \"dropout_rate_fc\": 0.2\n",
    "    },\n",
    "    \"cycling_lr\" : {\n",
    "        \"scheduler_active\" : True, \n",
    "        # Mode can be one of {triangular, triangular2, exp_range}\n",
    "        \"mode\" : \"triangular\", \n",
    "        \"gamma\" : 0.9995,\n",
    "        \"base_lr\" : 0.0001, # 0.016, \n",
    "        \"max_lr\" :0.0005,  # 0.75\n",
    "    },\n",
    "    \"training\": {\n",
    "        \"stake_training_data\": 0.75,\n",
    "        \"total_number\" : 6000,\n",
    "        \"n_folds_cv\": 2,\n",
    "        \"n_epochs\" : 4,\n",
    "        \"patience\" : 7,\n",
    "    },\n",
    "    \"filed_location\": {\n",
    "        \"model_for_test\" : \"../../../../models/cross_validation/MSE/cpps_\",\n",
    "        \"log_file\" : \"../../../../models/cross_validation/MSE/cpps_log_\", \n",
    "        \"history_trainval\" : \"../../../visualisation/files/cross_validation/MSE/cpps_trainval.csv\",\n",
    "        \"history_best_configuration\" : \"../../../visualisation/files/cross_validation/MSE/cpps_configurations.csv\"\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Paramerters phm data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "param = {\n",
    "    \"data\" : {\n",
    "        \"path\" : '../../../../data/phm_data_challenge/recipe/dataset_for_each_recipe/training/training_recipe_67.csv' ,\n",
    "    },\n",
    "    \"preprocessing\" : {\n",
    "        \"droped_features\": [\"ID\", \"stage\", \"Lot\", \"runnum\", \"recipe\", \"recipe_step\",\n",
    "                            \"up time\", \"ongoing time\", \n",
    "                            \"ETCHSOURCEUSAGE\", \"ETCHAUXSOURCETIMER\", \n",
    "                            \"ETCHAUX2SOURCETIMER\", \"FIXTURESHUTTERPOSITION\", \"ROTATIONSPEED\"\n",
    "                           ],\n",
    "        \"features_not_to_scale\": []\n",
    "    },\n",
    "    \"model\" : {\n",
    "        \"input_size\" : 12,\n",
    "        \"n_hidden_lstm\" : [10,20], \n",
    "        \"sequence_size\" : [10,30], \n",
    "        \"batch_size\" : 8,\n",
    "        \"lstm_layer\" : [1],\n",
    "        \"n_hidden_fc\": [20,60],\n",
    "        \"dropout_rate_lstm\": 0.0,\n",
    "        \"dropout_rate_fc\": 0.2\n",
    "    },\n",
    "    \"cycling_lr\" : {\n",
    "        \"scheduler_active\" : True, \n",
    "        # Mode can be one of {triangular, triangular2, exp_range}\n",
    "        \"mode\" : \"triangular\", \n",
    "        \"gamma\" : 0.9995,\n",
    "        \"base_lr\" : 0.0001, # 0.016, \n",
    "        \"max_lr\" :0.0005,  # 0.75\n",
    "    },\n",
    "    \"training\": {\n",
    "        \"stake_training_data\": 0.75,\n",
    "        \"total_number\" : 1000,\n",
    "        \"n_folds_cv\": 6,\n",
    "        \"n_epochs\" : 20,\n",
    "        \"patience\" : 7,\n",
    "    },\n",
    "    \"filed_location\": {\n",
    "        \"model_for_test\" : \"../../../../models/cross_validation/MSE/phm67_\",\n",
    "        \"log_file\" : \"../../../../models/cross_validation/MSE/phm67_log_\", \n",
    "        \"history_trainval\" : \"../../../visualisation/files/cross_validation/MSE/phm_trainval.csv\",\n",
    "        \"history_best_configuration\" : \"../../../visualisation/files/cross_validation/MSE/phm67_configurations.csv\"\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Split Data into folds\n",
    "- ignored features are getting removed\n",
    "- remaining data are split up into folds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv_provider = CrossValidationProvider(path=param[\"data\"][\"path\"], \n",
    "                                      no_folds=param[\"training\"][\"n_folds_cv\"], \n",
    "                                      amount_data=param[\"training\"][\"total_number\"],\n",
    "                                      ignored_features = param['preprocessing']['droped_features'],\n",
    "                                      stake = param[\"training\"][\"stake_training_data\"], \n",
    "                                     )\n",
    "folds, test_set = cv_provider.provide_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cross Validation Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Data : Fold 1\n",
      "- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -\n",
      "Start with new hyperparameters in grid search: \n",
      "Sequence_size: 10\n",
      "Number LSTM Layers: 1\n",
      "LSTM Number Hidden Dimensions: 10\n",
      "FC NN Number Hidden Dimensions: 60\n",
      "Stopped training on this partion at epoch 3\n",
      "Lowest training loss for this configuration: 0.9318259647116065\n",
      "Lowest validation loss for this configuration: 1.3952604443899224\n",
      "\n",
      "# ## ## ## ## ## ## ## ## ## ## ## ## ## ## ## ## ## ## ## #\n",
      "\n",
      "Start with new hyperparameters in grid search: \n",
      "Sequence_size: 10\n",
      "Number LSTM Layers: 1\n",
      "LSTM Number Hidden Dimensions: 50\n",
      "FC NN Number Hidden Dimensions: 60\n",
      "Stopped training on this partion at epoch 3\n",
      "Lowest training loss for this configuration: 0.9414016139560513\n",
      "Lowest validation loss for this configuration: 1.4271113180156265\n",
      "\n",
      "# ## ## ## ## ## ## ## ## ## ## ## ## ## ## ## ## ## ## ## #\n",
      "\n",
      "Start with new hyperparameters in grid search: \n",
      "Sequence_size: 50\n",
      "Number LSTM Layers: 1\n",
      "LSTM Number Hidden Dimensions: 10\n",
      "FC NN Number Hidden Dimensions: 60\n",
      "Stopped training on this partion at epoch 3\n",
      "Lowest training loss for this configuration: 0.9355279383334246\n",
      "Lowest validation loss for this configuration: 1.3901355923847718\n",
      "\n",
      "# ## ## ## ## ## ## ## ## ## ## ## ## ## ## ## ## ## ## ## #\n",
      "\n",
      "Start with new hyperparameters in grid search: \n",
      "Sequence_size: 50\n",
      "Number LSTM Layers: 1\n",
      "LSTM Number Hidden Dimensions: 50\n",
      "FC NN Number Hidden Dimensions: 60\n",
      "Stopped training on this partion at epoch 3\n",
      "Lowest training loss for this configuration: 0.9459953407536853\n",
      "Lowest validation loss for this configuration: 1.4214696568250655\n",
      "\n",
      "# ## ## ## ## ## ## ## ## ## ## ## ## ## ## ## ## ## ## ## #\n",
      "\n",
      "Validation Data : Fold 2\n",
      "- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -\n",
      "Start with new hyperparameters in grid search: \n",
      "Sequence_size: 10\n",
      "Number LSTM Layers: 1\n",
      "LSTM Number Hidden Dimensions: 10\n",
      "FC NN Number Hidden Dimensions: 60\n",
      "Stopped training on this partion at epoch 3\n",
      "Lowest training loss for this configuration: 0.9318013803767307\n",
      "Lowest validation loss for this configuration: 0.8276021260235991\n",
      "\n",
      "# ## ## ## ## ## ## ## ## ## ## ## ## ## ## ## ## ## ## ## #\n",
      "\n",
      "Start with new hyperparameters in grid search: \n",
      "Sequence_size: 10\n",
      "Number LSTM Layers: 1\n",
      "LSTM Number Hidden Dimensions: 50\n",
      "FC NN Number Hidden Dimensions: 60\n",
      "Stopped training on this partion at epoch 3\n",
      "Lowest training loss for this configuration: 0.9351823516722236\n",
      "Lowest validation loss for this configuration: 0.8295636938618762\n",
      "\n",
      "# ## ## ## ## ## ## ## ## ## ## ## ## ## ## ## ## ## ## ## #\n",
      "\n",
      "Start with new hyperparameters in grid search: \n",
      "Sequence_size: 50\n",
      "Number LSTM Layers: 1\n",
      "LSTM Number Hidden Dimensions: 10\n",
      "FC NN Number Hidden Dimensions: 60\n",
      "Stopped training on this partion at epoch 3\n",
      "Lowest training loss for this configuration: 0.9356718464873054\n",
      "Lowest validation loss for this configuration: 0.825079651935534\n",
      "\n",
      "# ## ## ## ## ## ## ## ## ## ## ## ## ## ## ## ## ## ## ## #\n",
      "\n",
      "Start with new hyperparameters in grid search: \n",
      "Sequence_size: 50\n",
      "Number LSTM Layers: 1\n",
      "LSTM Number Hidden Dimensions: 50\n",
      "FC NN Number Hidden Dimensions: 60\n",
      "Stopped training on this partion at epoch 3\n",
      "Lowest training loss for this configuration: 0.9399520083720034\n",
      "Lowest validation loss for this configuration: 0.8270350577343594\n",
      "\n",
      "# ## ## ## ## ## ## ## ## ## ## ## ## ## ## ## ## ## ## ## #\n",
      "\n",
      "Finished Cross-Validation\n"
     ]
    }
   ],
   "source": [
    "# Initialise Logger\n",
    "session_id = str(randint(10000, 99999))\n",
    "logger = Logger(param[\"filed_location\"][\"log_file\"], session_id)\n",
    "\n",
    "# Create file where validation results are stored and add header\n",
    "column_names_validation = [\"validation_fold\",\"validation_loss\", \"training_loss\",\n",
    "                           \"n_hidden_lstm\", \"sequence_length\", \"n_lstm_layer\", \n",
    "                           \"n_hidden_fc\"]\n",
    "with open(param[\"filed_location\"][\"history_best_configuration\"], \"a+\") as file:\n",
    "    [file.write(column+\";\") for column in column_names_validation]\n",
    "    file.write(\"\\n\")\n",
    "         \n",
    "for iteration in range (0, param[\"training\"][\"n_folds_cv\"]):\n",
    "    # Select folds for current iteration\n",
    "    training_folds = [x for i,x in enumerate(folds) if i!=iteration] \n",
    "    validation_fold = folds[iteration]\n",
    "    print(\"Validation Data : Fold \"+ str(iteration+1))\n",
    "    logger.log_message(\"Validation Data : Fold \"+ str(iteration+1))\n",
    "    print(\"- -\"*20)\n",
    "    logger.log_message(\"- -\"*20)\n",
    "    \n",
    "    # Get mean and variance of training folds \n",
    "    raw_training_data = pd.concat(training_folds, axis = 0, ignore_index=True)\n",
    "    raw_validation_data = validation_fold\n",
    "    scaler = DataScaler(features_not_to_scale= param['preprocessing']['features_not_to_scale'])\n",
    "    _, _ = scaler.scale_data(raw_training_data, raw_validation_data)\n",
    "    mean_train, val_train = scaler.provide_statistics()\n",
    "  \n",
    "    # Scale all folds\n",
    "    scaled_folds = []\n",
    "    for i in range(len(folds)):\n",
    "        scaled_folds.append(scaler.scale_fold(folds[i], mean_train, val_train))\n",
    "\n",
    "    # Training model and test hyperparameter on validation set\n",
    "    for n_lstm_layer in param[\"model\"][\"lstm_layer\"]:\n",
    "        for sequence_size  in param[\"model\"][\"sequence_size\"]:\n",
    "            # Creat Dataset of Training Folds\n",
    "            scaled_folds_training = [x for i,x in enumerate(scaled_folds) if i!=iteration]\n",
    "            reformated_folds = []\n",
    "            for i in range(len(scaled_folds_training)):\n",
    "                reformated_folds.append(DataSet(scaled_folds_training[i], timesteps=sequence_size))\n",
    "            \n",
    "            dataset_validation = DataSet(scaled_folds[iteration], timesteps=sequence_size)\n",
    "            dataset_training = ConcatDataset(reformated_folds)\n",
    "            \n",
    "            # Initialize DataLoader\n",
    "            data_loader_training = DataLoader(dataset_training, \n",
    "                                              batch_size=param[\"model\"][\"batch_size\"], \n",
    "                                              num_workers=0, \n",
    "                                              shuffle=True, \n",
    "                                              drop_last=True\n",
    "                                             )\n",
    "            data_loader_validation = DataLoader(dataset_validation, \n",
    "                                                batch_size=param[\"model\"][\"batch_size\"], \n",
    "                                                num_workers=0, \n",
    "                                                shuffle=True, \n",
    "                                                drop_last=True\n",
    "                                               )\n",
    "            \n",
    "            for n_hidden_lstm in param[\"model\"][\"n_hidden_lstm\"]:\n",
    "                for n_hidden_fc in param[\"model\"][\"n_hidden_fc\"]:\n",
    "                    print(\"Start with new hyperparameters in grid search: \")\n",
    "                    logger.log_message(\"Start with new hyperparameters in grid search: \")\n",
    "                    print(\"Sequence_size: {}\".format(sequence_size))\n",
    "                    logger.log_message(\"Sequence_size: {}\".format(sequence_size))\n",
    "                    print(\"Number LSTM Layers: {}\".format(n_lstm_layer))\n",
    "                    logger.log_message(\"Number LSTM Layers: {}\".format(n_lstm_layer))\n",
    "                    print(\"LSTM Number Hidden Dimensions: {}\".format(n_hidden_lstm))\n",
    "                    logger.log_message(\"LSTM Number Hidden Dimensions: {}\".format(n_hidden_lstm))\n",
    "                    print(\"FC NN Number Hidden Dimensions: {}\".format(n_hidden_fc))\n",
    "                    logger.log_message(\"FC NN Number Hidden Dimensions: {}\".format(n_hidden_fc))\n",
    "                    \n",
    "                    torch.manual_seed(0)\n",
    "                    model = LstmMse(batch_size=param['model']['batch_size'], \n",
    "                                    input_dim=param['model']['input_size'], \n",
    "                                    n_hidden_lstm=n_hidden_lstm, \n",
    "                                    n_layers=n_lstm_layer,\n",
    "                                    dropout_rate_lstm= param['model']['dropout_rate_lstm'],\n",
    "                                    dropout_rate_fc= param['model']['dropout_rate_fc'],\n",
    "                                    n_hidden_fc=n_hidden_fc\n",
    "                                    )\n",
    "\n",
    "                    # Define Loss Function\n",
    "                    criterion = LossMse(param[\"model\"][\"input_size\"], param[\"model\"][\"batch_size\"])\n",
    "\n",
    "                    # Initialize Optimizer and Cyclic Learning Rate Scheduler\n",
    "                    optimizer = torch.optim.SGD(model.parameters(), lr=1.)  \n",
    "                    scheduler = torch.optim.lr_scheduler.CyclicLR(optimizer=optimizer, \n",
    "                                                                  base_lr=param['cycling_lr']['base_lr'], \n",
    "                                                                  max_lr=param['cycling_lr']['max_lr'], \n",
    "                                                                  step_size_up=(raw_training_data.shape[0]/8)*2, \n",
    "                                                                  mode=param['cycling_lr']['mode'],\n",
    "                                                                  gamma=param['cycling_lr']['gamma']\n",
    "                                                                  )\n",
    "                    # Initialize Trainer\n",
    "                    trainer = Trainer(model=model,\n",
    "                                      optimizer=optimizer,\n",
    "                                      scheduler=scheduler,\n",
    "                                      scheduler_active = param[\"cycling_lr\"][\"scheduler_active\"],\n",
    "                                      criterion=criterion, \n",
    "                                      location_model=param[\"filed_location\"][\"model_for_test\"], \n",
    "                                      patience=param['training']['patience']\n",
    "                                     )\n",
    "\n",
    "                    # Measure training time for current configuration\n",
    "                    start = time.time()\n",
    "                    for epoch in range(param['training']['n_epochs']):\n",
    "                        # Train\n",
    "                        mean_epoch_training_loss = trainer.train(data_loader_training)\n",
    "\n",
    "                        # Evaluate\n",
    "                        mean_epoch_validation_loss = trainer.evaluate(data_loader_validation, epoch)\n",
    "                        \n",
    "                        if mean_epoch_validation_loss < trainer.lowest_val_loss:\n",
    "                            trainer.trials = 0\n",
    "                            trainer.lowest_val_loss = mean_epoch_validation_loss\n",
    "                            trainer.lowest_train_loss = mean_epoch_training_loss\n",
    "                            status_ok = True\n",
    "                            \n",
    "                        else:\n",
    "                            trainer.trials += 1\n",
    "                            if trainer.trials >= trainer.patience:\n",
    "                                status_ok = False       \n",
    "\n",
    "                        if not status_ok or epoch == (param['training']['n_epochs'])-1:\n",
    "                            logger.log_message(\"Stopped training on this partion at epoch {}\".format(epoch))\n",
    "                            print(\"Stopped training on this partion at epoch {}\".format(epoch))\n",
    "                            logger.log_message(\"Lowest training loss for this configuration: {}\".format(trainer.lowest_train_loss))\n",
    "                            print(\"Lowest training loss for this configuration: {}\".format(trainer.lowest_train_loss))\n",
    "                            logger.log_message(\"Lowest validation loss for this configuration: {}\".format(trainer.lowest_val_loss))\n",
    "                            print(\"Lowest validation loss for this configuration: {}\".format(trainer.lowest_val_loss))\n",
    "                            \n",
    "                            # Statistics of current fold\n",
    "                            statistics_validation = [\"fold \"+str(iteration+1),\n",
    "                                                     trainer.lowest_val_loss,\n",
    "                                                     trainer.lowest_train_loss,\n",
    "                                                     n_hidden_lstm, \n",
    "                                                     sequence_size,\n",
    "                                                     n_lstm_layer, \n",
    "                                                     n_hidden_fc\n",
    "                                                     ]\n",
    "\n",
    "                            # Safe statistics to .csv file\n",
    "                            with open(param[\"filed_location\"][\"history_best_configuration\"], \"a\") as file:\n",
    "                                for value in statistics_validation:\n",
    "                                    file.write(str(value)+\";\")\n",
    "                                file.write(\"\\n\")\n",
    "                            break       \n",
    "                    print(\"\\n\"+\"# #\"*20+\"\\n\")\n",
    "                    logger.log_message(\"\\n\"+\"# #\"*20+\"\\n\")\n",
    "print(\"Finished Cross-Validation\")\n",
    "logger.log_message(\"Finished Cross-Validation\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train Model with best hyperparameter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Best hyperparameter (look into result csv file and set values accordingly)\n",
    "opt_n_hidden_lstm = 20\n",
    "opt_sequence_size = 20\n",
    "opt_n_lstm_layer = 1\n",
    "opt_n_hidden_fc = 40\n",
    "epochs_for_final_train = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training phase of final model started\n",
      "Epoch 0: best model saved with loss: 0.9624240054083723\n",
      "Epoch 1: best model saved with loss: 0.8821538669722421\n",
      "Training phase is finished with training loss: 0.8821538669722421\n"
     ]
    }
   ],
   "source": [
    "# Training Data\n",
    "raw_training_data = pd.concat(folds, axis = 0, ignore_index=True)\n",
    "\n",
    "# Scale training data and test data (test data with mean and variance of training data)\n",
    "scaler = DataScaler(features_not_to_scale= param['preprocessing']['features_not_to_scale'])\n",
    "train_data_scaled, test_data_scaled = scaler.scale_data(raw_training_data, test_set)\n",
    "\n",
    "# Initialize DataSet\n",
    "dataset_train = DataSet(train_data_scaled, timesteps=opt_sequence_size)\n",
    "\n",
    "# Initialize DataLoader\n",
    "data_loader_training = DataLoader(dataset_train, \n",
    "                                  batch_size=param[\"model\"][\"batch_size\"], \n",
    "                                  num_workers=0, \n",
    "                                  shuffle=True, \n",
    "                                  drop_last=True\n",
    "                                 )\n",
    "\n",
    "# Initialise Model with best hyperparameter\n",
    "torch.manual_seed(0)\n",
    "model = LstmMse(batch_size=param['model']['batch_size'], \n",
    "                input_dim=param['model']['input_size'], \n",
    "                n_hidden_lstm=opt_n_hidden_lstm, \n",
    "                n_layers=opt_n_lstm_layer,\n",
    "                dropout_rate_lstm= param['model']['dropout_rate_lstm'],\n",
    "                dropout_rate_fc= param['model']['dropout_rate_fc'],\n",
    "                n_hidden_fc=opt_n_hidden_fc,\n",
    "                )\n",
    "\n",
    "# Define Loss Function\n",
    "criterion = LossMse(param[\"model\"][\"input_size\"], param[\"model\"][\"batch_size\"])\n",
    "\n",
    "# Initialize Optimizer and Trainer\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=1.)  \n",
    "scheduler = torch.optim.lr_scheduler.CyclicLR(optimizer=optimizer, \n",
    "                                              base_lr=param['cycling_lr']['base_lr'], \n",
    "                                              max_lr=param['cycling_lr']['max_lr'], \n",
    "                                              step_size_up=(len(raw_training_data)/param['model']['batch_size'])*2, # Authors of Cyclic LR suggest setting step_size 2-8 x training iterations in epoch.\n",
    "                                              mode=param['cycling_lr']['mode'],\n",
    "                                              gamma=param['cycling_lr']['gamma']\n",
    "                                             )\n",
    "trainer = Trainer(model=model,\n",
    "                  optimizer=optimizer,\n",
    "                  scheduler=scheduler,\n",
    "                  scheduler_active = param[\"cycling_lr\"][\"scheduler_active\"],\n",
    "                  criterion=criterion, \n",
    "                  location_model=param[\"filed_location\"][\"model_for_test\"], \n",
    "                  patience=param['training']['patience']\n",
    "                 )\n",
    "\n",
    "logger.log_message(\"\\n\")\n",
    "print(\"Training phase of final model started\")\n",
    "logger.log_message(\"Training phase of final model started\")\n",
    "\n",
    "for epoch in range(epochs_for_final_train):\n",
    "    # Train with batches \n",
    "    mean_epoch_training_loss = trainer.train(data_loader_training)\n",
    "\n",
    "    # Save model if its the best one since the last change in configuration of hyperparameters\n",
    "    status_ok = trainer.save_model(epoch, mean_epoch_training_loss, session_id)\n",
    "    \n",
    "    if not status_ok or epoch == epochs_for_final_train-1:\n",
    "        print(\"Training phase is finished with training loss: {}\".format(mean_epoch_training_loss))\n",
    "        logger.log_message(\"Training phase is finished with training loss: {}\".format(mean_epoch_training_loss))\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test trained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "# ## ## ## ## ## ## ## ## ## ## ## ## ## ## ## ## ## ## ## #\n",
      "\n",
      "Mean loss of test dataset is 0.6929679290668385\n",
      "\n",
      "# ## ## ## ## ## ## ## ## ## ## ## ## ## ## ## ## ## ## ## #\n",
      "\n",
      "Cross Validation finished\n"
     ]
    }
   ],
   "source": [
    "model = LstmMse(batch_size=param['model']['batch_size'], \n",
    "                input_dim=param['model']['input_size'], \n",
    "                n_hidden_lstm=opt_n_hidden_lstm, \n",
    "                n_layers=opt_n_lstm_layer,\n",
    "                dropout_rate_lstm= param['model']['dropout_rate_lstm'],\n",
    "                dropout_rate_fc= param['model']['dropout_rate_fc'],\n",
    "                n_hidden_fc=opt_n_hidden_fc,\n",
    "                )\n",
    "\n",
    "checkpoint = torch.load(param[\"filed_location\"][\"model_for_test\"]+\"id\"+str(session_id))\n",
    "model.load_state_dict(checkpoint['model_state_dict'])\n",
    "\n",
    "# Initialize DataSet\n",
    "dataset_test = DataSet(test_data_scaled, timesteps=opt_sequence_size)\n",
    "\n",
    "# Initialize DataLoader\n",
    "data_loader_test = DataLoader(dataset_test, \n",
    "                              batch_size=param[\"model\"][\"batch_size\"], \n",
    "                              num_workers=0, \n",
    "                              shuffle=True, \n",
    "                              drop_last=True\n",
    "                             )\n",
    "\n",
    "# Define Loss Function\n",
    "criterion = LossMse(param[\"model\"][\"input_size\"], param[\"model\"][\"batch_size\"])\n",
    "\n",
    "# Initialize Tester\n",
    "tester = Tester(model=model, criterion=criterion)\n",
    "\n",
    "# Evaluate Testset\n",
    "mean_test_loss = tester.evaluate(data_loader_test)\n",
    "print(\"\\n\"+\"# #\"*20+\"\\n\")\n",
    "logger.log_message(\"\\n\"+\"# #\"*20+\"\\n\")\n",
    "print(\"Mean loss of test dataset is {}\".format(mean_test_loss))\n",
    "logger.log_message(\"Mean loss of test dataset is {}\".format(mean_test_loss))\n",
    "print(\"\\n\"+\"# #\"*20+\"\\n\")\n",
    "logger.log_message(\"\\n\"+\"# #\"*20+\"\\n\")\n",
    "print(\"Cross Validation finished\")\n",
    "logger.log_message(\"Cross Validation finished\")\n",
    "# Delet model\n",
    "os.remove(param[\"filed_location\"][\"model_for_test\"]+\"id\"+str(session_id))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "masterarbeit",
   "language": "python",
   "name": "masterarbeit"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
