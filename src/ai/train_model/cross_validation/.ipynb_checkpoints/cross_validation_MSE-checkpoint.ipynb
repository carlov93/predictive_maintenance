{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader\n",
    "import torch.optim as optim\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import time\n",
    "\n",
    "# own Modules \n",
    "from models_mse import LstmMse\n",
    "from data_set import DataSet\n",
    "from cross_validation import CrossValidationProvider\n",
    "from scaler import DataScaler\n",
    "from trainer import Trainer\n",
    "from loss_module import LossMse\n",
    "from tester import Tester"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Take care of these things before training:\n",
    "- Select correct path and define droped_features\n",
    "- Change parameter of model\n",
    "- Change step_size in cycling_lr\n",
    "- Change filed_location"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Paramerters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "param = {\n",
    "    \"data\" : {\n",
    "        \"path\" : '../../../../data/cpps_data/cpps_data_predictive_maintenance_training.csv' ,\n",
    "    },\n",
    "    \"preprocessing\" : {\n",
    "        \"droped_features\": [\"ID\"\n",
    "                           ],\n",
    "        \"features_not_to_scale\": []\n",
    "    },\n",
    "    \"model\" : {\n",
    "        \"input_size\" : 10,\n",
    "        \"n_hidden_lstm\" : [15], #100\n",
    "        \"sequence_size\" : [25], #100\n",
    "        \"batch_size\" : 8,\n",
    "        \"lstm_layer\" : [1],\n",
    "        \"n_hidden_fc\": [75],\n",
    "        \"dropout_rate_lstm\": 0.0,\n",
    "        \"dropout_rate_fc\": 0.2\n",
    "    },\n",
    "    \"cycling_lr\" : {\n",
    "        \"scheduler_active\" : True, \n",
    "        # Mode can be one of {triangular, triangular2, exp_range}\n",
    "        \"mode\" : \"triangular\", \n",
    "        \"gamma\" : 0.9995,\n",
    "        \"base_lr\" : 0.001, # 0.016, \n",
    "        \"max_lr\" :0.005,  # 0.75\n",
    "    },\n",
    "    \"training\": {\n",
    "        \"stake_training_data\": 0.75,\n",
    "        \"total_number\" : 50000,\n",
    "        \"n_folds_cv\": 5,\n",
    "        \"n_epochs\" : 50,\n",
    "        \"patience\" : 7,\n",
    "    },\n",
    "    \"filed_location\": {\n",
    "        \"trained_model\" : \"../../../../models/cross_validation/MSE_cpps_data\",\n",
    "        \"history\" : \"../../../visualisation/files/cross_validation/MSE/cpps_data.csv\"\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Nested Cross Validation for Time Series Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](../../../../knowledge/pictures/nested_cv.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Split Data into folds\n",
    "- ignored features are getting removed\n",
    "- remaining data are split up into folds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv_provider = CrossValidationProvider(path=param[\"data\"][\"path\"], \n",
    "                                      no_folds=param[\"training\"][\"n_folds_cv\"], \n",
    "                                      amount_data=param[\"training\"][\"total_number\"],\n",
    "                                      stake_training_data = param[\"training\"][\"stake_training_data\"],\n",
    "                                      ignored_features = param['preprocessing']['droped_features']\n",
    "                                     )\n",
    "test_data, folds = cv_provider.provide_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cross Validation Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Data : Fold 1-2\n",
      "Validation Data : Fold 3\n",
      "Amount Training Data: 3000\n",
      "Amount Validation Data: 1500\n",
      "- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -\n",
      "Start with new hyperparameters in grid search: \n",
      "Sequence_size: 25\n",
      "Number LSTM Layers: 1\n",
      "LSTM Number Hidden Dimensions: 15\n",
      "FC NN Number Hidden Dimensions: 75\n",
      "-------- epoch_no. 0 finished with eval loss 1.5978784191867579--------\n",
      "Epoch 0: best model saved with loss: 1.5978784191867579\n",
      "-------- epoch_no. 1 finished with eval loss 1.475566682284293--------\n",
      "Epoch 1: best model saved with loss: 1.475566682284293\n",
      "\n",
      "# ## ## ## ## ## ## ## ## ## ## ## ## ## ## ## ## ## ## ## ## ## ## ## ## ## ## ## ## ## #\n",
      "\n",
      "Training Data : Fold 1-3\n",
      "Validation Data : Fold 4\n",
      "Amount Training Data: 4500\n",
      "Amount Validation Data: 1500\n",
      "- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -\n",
      "Start with new hyperparameters in grid search: \n",
      "Sequence_size: 25\n",
      "Number LSTM Layers: 1\n",
      "LSTM Number Hidden Dimensions: 15\n",
      "FC NN Number Hidden Dimensions: 75\n",
      "-------- epoch_no. 0 finished with eval loss 0.9884349493228871--------\n",
      "Epoch 0: best model saved with loss: 0.9884349493228871\n",
      "-------- epoch_no. 1 finished with eval loss 0.8399860982013785--------\n",
      "Epoch 1: best model saved with loss: 0.8399860982013785\n",
      "\n",
      "# ## ## ## ## ## ## ## ## ## ## ## ## ## ## ## ## ## ## ## ## ## ## ## ## ## ## ## ## ## #\n",
      "\n",
      "Finish of Cross Validation\n"
     ]
    }
   ],
   "source": [
    "statistics_folds = []\n",
    "hist_loss = []\n",
    "for iteration in range (2, param[\"training\"][\"n_folds_cv\"]-1):\n",
    "    # Select folds for current iteration\n",
    "    training_folds = folds[:iteration]\n",
    "    validation_fold = folds[iteration:iteration+1]\n",
    "    print(\"Training Data : Fold 1-\" + str(iteration))\n",
    "    print(\"Validation Data : Fold \"+ str(iteration+1))\n",
    "    \n",
    "    # Concate data of training folds and unpack validation data\n",
    "    raw_training_data = pd.concat(training_folds, axis = 0, ignore_index=True)\n",
    "    raw_validation_data = validation_fold[0]\n",
    "    print(\"Amount Training Data: {}\".format(raw_training_data.shape[0]))\n",
    "    print(\"Amount Validation Data: {}\".format(raw_validation_data.shape[0]))\n",
    "    print(\"- -\"*30)\n",
    "    \n",
    "    # Scale training data and validation data (validation data with mean and variance of training data)\n",
    "    scaler = DataScaler(features_not_to_scale= param['preprocessing']['features_not_to_scale'])\n",
    "    train_data_scaled, validation_data_scaled = scaler.scale_data(raw_training_data, raw_validation_data)\n",
    "    \n",
    "    # Start Training\n",
    "    for n_lstm_layer in param[\"model\"][\"lstm_layer\"]:\n",
    "        for sequence_size  in param[\"model\"][\"sequence_size\"]:\n",
    "            # Initialize DataSet\n",
    "            dataset_train = DataSet(train_data_scaled, timesteps=sequence_size)\n",
    "            dataset_validation = DataSet(validation_data_scaled, timesteps=sequence_size)\n",
    "            \n",
    "            # Initialize DataLoader\n",
    "            data_loader_training = DataLoader(dataset_train, \n",
    "                                              batch_size=param[\"model\"][\"batch_size\"], \n",
    "                                              num_workers=0, \n",
    "                                              shuffle=True, \n",
    "                                              drop_last=True\n",
    "                                             )\n",
    "            data_loader_validation = DataLoader(dataset_validation, \n",
    "                                                batch_size=param[\"model\"][\"batch_size\"], \n",
    "                                                num_workers=0, \n",
    "                                                shuffle=True, \n",
    "                                                drop_last=True\n",
    "                                               )\n",
    "            \n",
    "            for n_hidden_lstm in param[\"model\"][\"n_hidden_lstm\"]:\n",
    "                for n_hidden_fc in param[\"model\"][\"n_hidden_fc\"]:\n",
    "                    print(\"Start with new hyperparameters in grid search: \")\n",
    "                    print(\"Sequence_size: {}\".format(sequence_size))\n",
    "                    print(\"Number LSTM Layers: {}\".format(n_lstm_layer))\n",
    "                    print(\"LSTM Number Hidden Dimensions: {}\".format(n_hidden_lstm))\n",
    "                    print(\"FC NN Number Hidden Dimensions: {}\".format(n_hidden_fc))\n",
    "\n",
    "                    # Create lists to save training loss and validation loss of each epoch\n",
    "                    hist_loss = []\n",
    "                    torch.manual_seed(0)\n",
    "                    model = LstmMse(batch_size=param['model']['batch_size'], \n",
    "                                    input_dim=param['model']['input_size'], \n",
    "                                    n_hidden_lstm=n_hidden_lstm, \n",
    "                                    n_layers=n_lstm_layer,\n",
    "                                    dropout_rate_lstm= param['model']['dropout_rate_lstm'],\n",
    "                                    dropout_rate_fc= param['model']['dropout_rate_fc'],\n",
    "                                    n_hidden_fc=n_hidden_fc\n",
    "                                    )\n",
    "\n",
    "                    # Define Loss Function\n",
    "                    criterion = LossMse(param[\"model\"][\"input_size\"], param[\"model\"][\"batch_size\"])\n",
    "\n",
    "                    # Initialize Optimizer and Cyclic Learning Rate Scheduler\n",
    "                    optimizer = torch.optim.SGD(model.parameters(), lr=1.)  \n",
    "                    scheduler = torch.optim.lr_scheduler.CyclicLR(optimizer=optimizer, \n",
    "                                                                  base_lr=param['cycling_lr']['base_lr'], \n",
    "                                                                  max_lr=param['cycling_lr']['max_lr'], \n",
    "                                                                  step_size_up=(raw_training_data.shape[0]/8)*2, \n",
    "                                                                  mode=param['cycling_lr']['mode'],\n",
    "                                                                  gamma=param['cycling_lr']['gamma']\n",
    "                                                                  )\n",
    "                    # Initialize Trainer\n",
    "                    trainer = Trainer(model=model,\n",
    "                                      optimizer=optimizer,\n",
    "                                      scheduler=scheduler,\n",
    "                                      scheduler_active = param[\"cycling_lr\"][\"scheduler_active\"],\n",
    "                                      criterion=criterion, \n",
    "                                      location_model=param[\"filed_location\"][\"trained_model\"], \n",
    "                                      location_stats=param[\"filed_location\"][\"history\"], \n",
    "                                      patience=param['training']['patience']\n",
    "                                     )\n",
    "                    \n",
    "                    # Measure training time for current configuration\n",
    "                    start = time.time()\n",
    "\n",
    "                    for epoch in range(param['training']['n_epochs']):\n",
    "                        # Train\n",
    "                        mean_epoch_training_loss = trainer.train(data_loader_training)\n",
    "\n",
    "                        # Evaluate\n",
    "                        mean_epoch_validation_loss = trainer.evaluate(data_loader_validation, hist_loss, epoch)\n",
    "                        \n",
    "                        # Cache History\n",
    "                        trainer.cache_history_training(hist_loss, epoch, mean_epoch_training_loss, mean_epoch_validation_loss)\n",
    "\n",
    "                        # Save model if its the best one since the last change in configuration of hyperparameters\n",
    "                        trainer.fold = \"Fold 1-\"+str(iteration)\n",
    "                        status_ok = trainer.save_model(epoch, mean_epoch_validation_loss, param['model']['input_size'], \n",
    "                                                       n_lstm_layer, n_hidden_lstm, n_hidden_fc, sequence_size)\n",
    "\n",
    "                        if not status_ok or epoch == (param['training']['n_epochs'])-1:\n",
    "                            statistics = {\"training_folds\": \"fold 1-\"+str(iteration),\n",
    "                                          \"validation_folds\":\"fold \"+str(iteration+1),\n",
    "                                          \"lowest_validation_loss\": trainer.lowest_loss, \n",
    "                                          \"n_hidden_lstm\" : n_hidden_lstm,\n",
    "                                          \"sequence_size\" : sequence_size,\n",
    "                                          \"lstm_layer\" : n_lstm_layer,\n",
    "                                          \"n_hidden_fc\": n_hidden_fc\n",
    "                                         }\n",
    "                            statistics_folds.append(statistics)\n",
    "                            break\n",
    "                    \n",
    "                    print(\"\")\n",
    "                    print(\"# #\"*30) \n",
    "                    print(\"\")\n",
    "                    \n",
    "# Safe results to csv file\n",
    "df = pd.DataFrame(statistics_folds)\n",
    "df.to_csv(param[\"filed_location\"][\"history\"], sep=\";\", index=False)   \n",
    "print(\"Finish of Cross Validation\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test best Model with test data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Select best model of training phase and change parameter accordingly:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_test = {\n",
    "    \"model_for_testset\" : {\n",
    "        \"path\" : \"../../../models/cross_validation/phm_data_large_InputSize13_LayerLstm2_HiddenLstm100_HiddenFc50_Seq150.pt\",\n",
    "        \"input_size\" : 10,\n",
    "        \"n_hidden_lstm\" : 15,\n",
    "        \"sequence_size\" : 25,\n",
    "        \"batch_size\" : 8,\n",
    "        \"lstm_layer\" : 1,\n",
    "        \"n_hidden_fc\": 75,\n",
    "        \"dropout_rate_lstm\": 0.0,\n",
    "        \"dropout_rate_fc\": 0.2\n",
    "    },\n",
    "    \"iteration_training_dataset\" : 2\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get Mean and Variance of Training Dataset which generated the best Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_folds = folds[:param_test[\"iteration_training_dataset\"]]\n",
    "validation_fold = folds[param_test[\"iteration_training_dataset\"]:param_test[\"iteration_training_dataset\"]+1]\n",
    "raw_training_data = pd.concat(training_folds, axis = 0, ignore_index=True)\n",
    "raw_validation_data = validation_fold[0]\n",
    "scaler = DataScaler(features_not_to_scale= param['preprocessing']['features_not_to_scale'])\n",
    "_, __ = scaler.scale_data(raw_training_data, raw_validation_data)\n",
    "mean, variance = scaler.provide_statistics()\n",
    "print(mean)\n",
    "print(variance)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_training_data =[ 0.0632522,0.10388593, 0.09563544, 0.0777276, 0.22081628, 0.08311531, 0.01382531,\n",
    "                     0.09862897, 0.07814727, -0.0185826, 0.1000127, -0.0161782, -0.22541928]\n",
    "var_training_data =[0.90316232, 0.97237671, 0.98547017, 0.92090347, 1.18086523, 0.92393987,\n",
    "                    0.41744699, 0.97142703, 0.92604794, 0.68786855, 1.25019607, 0.50023143, 0.69425608]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scale test data with mean and variance of training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = DataScaler(features_not_to_scale= param['preprocessing']['features_not_to_scale'])\n",
    "test_data_scaled = scaler.scale_data_test_dataset(test_data, mean_training_data, var_training_data)\n",
    "\n",
    "# Initialize DataSet\n",
    "dataset_test = DataSet(test_data_scaled, timesteps=param_test[\"model_for_testset\"][\"sequence_size\"])\n",
    "\n",
    "# Initialize DataLoader\n",
    "data_loader_test = DataLoader(dataset_test, \n",
    "                              batch_size=param_test[\"model_for_testset\"][\"batch_size\"], \n",
    "                              num_workers=0, \n",
    "                              shuffle=True, \n",
    "                              drop_last=True\n",
    "                             )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create lists to save training loss and validation loss of each epoch\n",
    "torch.manual_seed(0)\n",
    "model = LstmMse(batch_size=param_test['model']['batch_size'], \n",
    "                input_dim=param_test['model']['input_size'], \n",
    "                n_hidden_lstm=param_test['model']['n_hidden_lstm'], \n",
    "                n_layers=param_test['model']['lstm_layer'],\n",
    "                dropout_rate_lstm= param_test['model']['dropout_rate_lstm'],\n",
    "                dropout_rate_fc= param_test['model']['dropout_rate_fc'],\n",
    "                n_hidden_fc=param_test['model']['n_hidden_fc']\n",
    "                )\n",
    "\n",
    "checkpoint = torch.load(param_test[\"model_for_testset\"][\"path\"])\n",
    "model.load_state_dict(checkpoint['model_state_dict'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Start Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0547518821002231\n"
     ]
    }
   ],
   "source": [
    "# Define Loss Function\n",
    "criterion = LossMse(param[\"model\"][\"input_size\"], param[\"model\"][\"batch_size\"])\n",
    "\n",
    "# Initialize Tester\n",
    "tester = Tester(model=model,\n",
    "                criterion=criterion\n",
    "                )\n",
    "\n",
    "# Evaluate Testset\n",
    "mean_test_loss = tester.evaluate(data_loader_test)\n",
    "print(mean_test_loss)\n",
    "df = pd.read_csv(param[\"filed_location\"][\"history\"], sep=\";\")\n",
    "df.at[XXXXXXXXXXXX, 'result_test_set'] = mean_test_loss\n",
    "df.to_csv(param[\"filed_location\"][\"history\"], sep=\";\", index=False)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "masterarbeit",
   "language": "python",
   "name": "masterarbeit"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
