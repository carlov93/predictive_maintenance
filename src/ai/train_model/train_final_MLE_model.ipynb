{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader\n",
    "import torch.optim as optim\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from random import randint\n",
    "\n",
    "# own Modules \n",
    "from models_mle import LstmMle\n",
    "from data_set import DataSetSensors\n",
    "from data_preperator import DataPreperator\n",
    "from trainer import Trainer\n",
    "from loss_module import LossMle\n",
    "from logger import Logger"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train final model after cross validation showed generalisation ability of given model with its hyperparameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parameters phm data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "param = {\n",
    "    \"data\" : {\n",
    "        \"path\" : '../../../data/phm_data_challenge/recipe/dataset_for_each_recipe/training/training_recipe_67.csv' ,\n",
    "    },\n",
    "    \"preprocessing\" : {\n",
    "        \"first_order_difference\" : False,\n",
    "        \"droped_features\" : [\"ID\", \"stage\", \"Lot\", \"runnum\", \"recipe\", \"recipe_step\",\n",
    "                            \"up time\", \"ongoing time\", \n",
    "                            \"ETCHSOURCEUSAGE\", \"ETCHAUXSOURCETIMER\", \n",
    "                            \"ETCHAUX2SOURCETIMER\", \"FIXTURESHUTTERPOSITION\", \"ROTATIONSPEED\"\n",
    "                           ],\n",
    "        \"features_not_to_scale\": []\n",
    "    },\n",
    "    \"model\" : {\n",
    "        \"input_size\" : 12,\n",
    "        \"n_hidden_lstm\" : 15,\n",
    "        \"sequence_size\" : 25,\n",
    "        \"batch_size\" : 8,\n",
    "        \"lstm_layer\" : 1,\n",
    "        \"n_hidden_fc_1\": 75,\n",
    "        \"n_hidden_fc_2\": 25,\n",
    "        \"dropout_rate_lstm\": 0.0,\n",
    "        \"dropout_rate_fc\": 0.2\n",
    "    },\n",
    "    \"cycling_lr\" : {\n",
    "        \"scheduler_active\" : True, \n",
    "        # Mode can be one of {triangular, triangular2, exp_range}\n",
    "        \"mode\" : \"triangular\", \n",
    "        \"gamma\" : 0.9995,\n",
    "        \"base_lr\" : 0.0001,\n",
    "        \"max_lr\" :0.0005\n",
    "    },\n",
    "    \"training\": {\n",
    "        \"n_epochs\" : 30,\n",
    "        \"patience\" : 5,\n",
    "    },\n",
    "    \"filed_location\": {\n",
    "        \"trained_model\" : \"../../../models/MLE_model/phm67_\",\n",
    "        \"log_file\" : \"../../../models/MLE_model/0_logs/phm_\"\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parameters artifical signal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "param = {\n",
    "    \"data\" : { \n",
    "        \"path\" : '../../../data/artifical_signals/artifical_2_signals.csv' ,\n",
    "    },\n",
    "    \"preprocessing\" : {\n",
    "        \"first_order_difference\" : False,\n",
    "        \"droped_features\" : [\"ID\",\n",
    "                           ],\n",
    "        \"features_not_to_scale\": []\n",
    "    },\n",
    "    \"model\" : {\n",
    "        \"input_size\" : 2,\n",
    "        \"n_hidden_lstm\" : 15,\n",
    "        \"sequence_size\" : 25,\n",
    "        \"batch_size\" : 8,\n",
    "        \"lstm_layer\" : 1,\n",
    "        \"n_hidden_fc_1\": 75,\n",
    "        \"n_hidden_fc_2\": 25,\n",
    "        \"dropout_rate_lstm\": 0.0,\n",
    "        \"dropout_rate_fc\": 0.2\n",
    "    },\n",
    "    \"cycling_lr\" : {\n",
    "        \"scheduler_active\" : True,  \n",
    "        # Mode can be one of {triangular, triangular2, exp_range}\n",
    "        \"mode\" : \"triangular\", \n",
    "        \"gamma\" : 0.9995,\n",
    "        \"base_lr\" : 0.0001,\n",
    "        \"max_lr\" :0.0005\n",
    "    },\n",
    "    \"training\": {\n",
    "        \"n_epochs\" : 2,\n",
    "        \"patience\" : 5,\n",
    "    },\n",
    "    \"filed_location\": {\n",
    "        \"trained_model\" : \"../../../models/MLE_model/art2_\",\n",
    "        \"log_file\" : \"../../../models/MLE_model/0_logs/artifical_\"\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parameters cpps Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "param = {\n",
    "    \"data\" : {\n",
    "        \"path\" : '../../../data/cpps_degradation_new/data_obs10/train/obs_space_train_sinusiod_preprocessed.csv' ,\n",
    "    },\n",
    "    \"preprocessing\" : {\n",
    "        \"first_order_difference\" : False,\n",
    "        \"droped_features\" : [\"ID\"\n",
    "                           ],\n",
    "        \"features_not_to_scale\": []\n",
    "    },\n",
    "    \"model\" : {\n",
    "        \"input_size\" : 10,\n",
    "        \"n_hidden_lstm\" : 75,\n",
    "        \"sequence_size\" : 25,\n",
    "        \"batch_size\" : 8,\n",
    "        \"lstm_layer\" : 1,\n",
    "        \"n_hidden_fc_1\": 75,\n",
    "        \"n_hidden_fc_2\": 25,\n",
    "        \"dropout_rate_lstm\": 0.0,\n",
    "        \"dropout_rate_fc\": 0.2\n",
    "    },\n",
    "    \"cycling_lr\" : {\n",
    "        \"scheduler_active\" : True, \n",
    "        # Mode can be one of {triangular, triangular2, exp_range}\n",
    "        \"mode\" : \"triangular\", \n",
    "        \"gamma\" : 0.9995,\n",
    "        \"base_lr\" : 0.0001,\n",
    "        \"max_lr\" :0.0005\n",
    "    },\n",
    "    \"training\": {\n",
    "        \"n_epochs\" : 30,\n",
    "        \"patience\" : 5,\n",
    "    },\n",
    "    \"filed_location\": {\n",
    "        \"trained_model\" : \"../../../models/MLE_model/cpps_A2_\",\n",
    "        \"log_file\" : \"../../../models/MLE_model/0_logs/cpps_A2_\"\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parameters variance dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "param = {\n",
    "    \"data\" : {\n",
    "        \"path\" : '../../../data/variation_in_variance/dataset_increasing_variance.csv' ,\n",
    "    },\n",
    "    \"preprocessing\" : {\n",
    "        \"first_order_difference\" : False,\n",
    "        \"droped_features\" : [\"ID\",\"time\", \"mu\", \"sigma\" \n",
    "                           ],\n",
    "        \"features_not_to_scale\": []\n",
    "    },\n",
    "    \"model\" : {\n",
    "        \"input_size\" : 1,\n",
    "        \"n_hidden_lstm\" : 2,\n",
    "        \"sequence_size\" : 36,\n",
    "        \"batch_size\" : 2,\n",
    "        \"lstm_layer\" : 1,\n",
    "        \"n_hidden_fc_1\": 5,\n",
    "        \"n_hidden_fc_2\": 25,\n",
    "        \"dropout_rate_lstm\": 0.0,\n",
    "        \"dropout_rate_fc\": 0.2\n",
    "    },\n",
    "    \"cycling_lr\" : {\n",
    "        \"scheduler_active\" : True, \n",
    "        # Mode can be one of {triangular, triangular2, exp_range}\n",
    "        \"mode\" : \"triangular\", \n",
    "        \"gamma\" : 0.9995,\n",
    "        \"base_lr\" : 0.0001,\n",
    "        \"max_lr\" :0.0005\n",
    "    },\n",
    "    \"training\": {\n",
    "        \"n_epochs\" : 30,\n",
    "        \"patience\" : 5,\n",
    "    },\n",
    "    \"filed_location\": {\n",
    "        \"trained_model\" : \"../../../models/MLE_model/variance_dataset_increasing_\",\n",
    "        \"log_file\" : \"../../../models/MLE_model/0_logs/variance_dataset_increasing_\"\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing\n",
    "1. First order difference (if selected)\n",
    "2. Split data into train and validation data\n",
    "3. Scale train and validation data with train's mean and variance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = DataPreperator(path=param['data']['path'], \n",
    "                              ignored_features=param['preprocessing']['droped_features'],\n",
    "                              stake_training_data=0.8,\n",
    "                              features_not_to_scale=param['preprocessing']['features_not_to_scale'],\n",
    "                              first_order_difference=param[\"preprocessing\"][\"first_order_difference\"])\n",
    "train_data, _ = train_loader.prepare_data()\n",
    "print(len(train_data))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mean and variance from scale process (only of continious features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean, var = train_loader.provide_statistics()\n",
    "print(mean)\n",
    "print(var)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialize Dataset \n",
    "Time series data must be transformed into a structure of samples with `input` and `target` components before it can be used to fit a supervised learning model. <br>\n",
    "For a time series interested in one-step predictions, the observations at prior time steps, so-called lag observations, are used as `input` and the `target` is the observation at the current time step.\n",
    "\n",
    "For example, a univariate series can be expressed as a supervised learning problem with three time steps for `input` and one step as `target`, as follows:\n",
    "\n",
    "|input|target|\n",
    "|-----|------|\n",
    "[1, 2, 3]|[4]\n",
    "[2, 3, 4]|[5]\n",
    "[3, 4, 5]|[6]\n",
    "\n",
    "The Keras deep learning library provides the `TimeseriesGenerator` to automatically transform both univariate and multivariate time series data into such a format. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_train = DataSetSensors(train_data, timesteps=param[\"model\"][\"sequence_size\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialize DataLoader\n",
    "Actually the data has a other size than in the table above because of multivariate time series data and because of using batches. <br>\n",
    "__First dimension__: batch size --> Defines the number of samples that will be propagated through the network simultaneously. <br>\n",
    "__Second dimension:__ timesteps --> Number of sequence which is passed into the LSTM <br>\n",
    "__Third dimension:__ input_dim --> Is the number of features. In this case data from 7 sensors, collected at the same time. <br>\n",
    "\n",
    "Data is shuffled because each mini batch is indipendent from each other, but samples of a minibatch are in chronological order."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_loader_training = DataLoader(dataset_train, \n",
    "                                  batch_size=param[\"model\"][\"batch_size\"], \n",
    "                                  num_workers=0, \n",
    "                                  shuffle=True, \n",
    "                                  drop_last=True\n",
    "                                 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for batch_idx, data in enumerate(data_loader_training):\n",
    "    x,y = data\n",
    "    print('Data of batch: {}'.format(batch_idx))\n",
    "    print(\"Size of input data: {}\".format(x.size()))\n",
    "    print(\"Size of target data: {}\".format(y.size()))\n",
    "    if batch_idx >=1: break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training Phase 1: Only consider mu\n",
    "Set tau=0 --> sigma=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "K_phase_1=0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialize Neural Network\n",
    "### Options 1-4:\n",
    "#### Architecture 1 - One FCNN (2 layers, last splited for mu and sigma)\n",
    "#### Architecture 2 - Two seperate FCNN for mu and sigma each (2 layers)\n",
    "#### Architecture 3 - Two seperate FCNN for mu and sigma each (3 layers)\n",
    "#### Architecture 4 - Two complete seperate subnetworks (from LSTM layer to last FC layer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(0)\n",
    "model = LstmMle(batch_size=param['model']['batch_size'], \n",
    "                input_dim=param['model']['input_size'], \n",
    "                n_hidden_lstm=param['model']['n_hidden_lstm'], \n",
    "                n_layers=param['model']['lstm_layer'],\n",
    "                dropout_rate_lstm= param['model']['dropout_rate_lstm'],\n",
    "                dropout_rate_fc= param['model']['dropout_rate_fc'],\n",
    "                n_hidden_fc_1=param['model']['n_hidden_fc_1'],\n",
    "                K = K_phase_1,\n",
    "                option = 1\n",
    "                )\n",
    "log_message_architecture = \"Architecture: LSTM module and a subsequent FCNN (2 layers, last splited for mu and sigma)\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialize MLE Loss function as torch module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = LossMle(param[\"model\"][\"input_size\"], param[\"model\"][\"batch_size\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialize Optimizer and Cyclic Learning Rate Scheduler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.SGD(model.parameters(), lr=1.)  \n",
    "scheduler = torch.optim.lr_scheduler.CyclicLR(optimizer=optimizer, \n",
    "                                              base_lr=param['cycling_lr']['base_lr'], \n",
    "                                              max_lr=param['cycling_lr']['max_lr'], \n",
    "                                              step_size_up=(len(train_data)/param['model']['batch_size'])*2, # Authors of Cyclic LR suggest setting step_size 2-8 x training iterations in epoch.\n",
    "                                              mode=param['cycling_lr']['mode'],\n",
    "                                              gamma=param['cycling_lr']['gamma']\n",
    "                                             )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialize Trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = Trainer(model=model,\n",
    "                  optimizer=optimizer,\n",
    "                  scheduler=scheduler,\n",
    "                  scheduler_active = param[\"cycling_lr\"][\"scheduler_active\"],\n",
    "                  criterion=criterion, \n",
    "                  location_model=param[\"filed_location\"][\"trained_model\"], \n",
    "                  patience=param['training']['patience']\n",
    "                 )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Loop\n",
    "An epoch consists of a learning cycle over all batches of training data and an evaluation of the most recent model with the testing data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialise Logger\n",
    "session_id = str(randint(10000, 99999))\n",
    "logger = Logger(param[\"filed_location\"][\"log_file\"], session_id)\n",
    "\n",
    "# Log model architecture and training configuration\n",
    "logger.log_message(\"Architecture and Training configuration:\")\n",
    "logger.log_message(\"Loss function: MLE\")\n",
    "logger.log_message(log_message_architecture)\n",
    "logger.log_message(\"Batch size: {}\".format(param['model']['batch_size']))\n",
    "logger.log_message(\"Input size: {}\".format(param['model']['input_size']))\n",
    "logger.log_message(\"Sequence length: {}\".format(param[\"model\"][\"sequence_size\"]))\n",
    "logger.log_message(\"Hidden units LSTM: {}\".format(param['model']['n_hidden_lstm']))\n",
    "logger.log_message(\"Amount LSTM layer: {}\".format(param['model']['lstm_layer']))\n",
    "logger.log_message(\"Dropout rate LSTM: {}\".format(param['model']['dropout_rate_lstm']))\n",
    "logger.log_message(\"Dropout rate fc NN: {}\".format(param['model']['dropout_rate_fc']))\n",
    "logger.log_message(\"Hidden units fc1: {}\".format(param['model']['n_hidden_fc_1']))\n",
    "logger.log_message(\"Hidden units fc2: {}\".format(param['model']['n_hidden_fc_1']))\n",
    "logger.log_message(\"Cycling LR mode: {}\".format(param['cycling_lr']['mode']))\n",
    "logger.log_message(\"Cycling LR base LR: {}\".format(param['cycling_lr']['base_lr']))\n",
    "logger.log_message(\"Cycling LR max LR: {}\".format(param['cycling_lr']['max_lr']))\n",
    "logger.log_message(\"- -\"*20)\n",
    "\n",
    "print(\"Training phase 1 is started\")\n",
    "logger.log_message(\"Training phase 1 is started\")\n",
    "\n",
    "# Create lists to save training loss and validation loss of each epoch\n",
    "hist_loss = []\n",
    "\n",
    "for epoch in range(param['training']['n_epochs']):\n",
    "    # Train with batches \n",
    "    mean_epoch_training_loss = trainer.train(data_loader_training)\n",
    "\n",
    "    # Save model if its the best one since the last change in configuration of hyperparameters\n",
    "    status_ok, path_model_phase_1 = trainer.save_model(epoch, mean_epoch_training_loss, session_id)\n",
    "    \n",
    "    # Log information of current epoch\n",
    "    logger.log_current_statistics(epoch, mean_epoch_training_loss)\n",
    "        \n",
    "    if not status_ok:\n",
    "        break\n",
    "print(\"Training phase 1 is finished\")\n",
    "logger.log_message(\"Training phase 1 is finished\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training Phase 2: Consider mu and sigma, take pre-trained model form phase 1 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_phase_2 = {\n",
    "    \"model\" : {\n",
    "        \"path\" : path_model_phase_1\n",
    "    },\n",
    "    \"cycling_lr\" : {\n",
    "        \"scheduler_active\" : True, \n",
    "        # Mode can be one of {triangular, triangular2, exp_range}\n",
    "        \"mode\" : \"triangular\", \n",
    "        \"gamma\" : 0.9995,\n",
    "        \"base_lr\" : 0.0001, \n",
    "        \"max_lr\" : 0.0005\n",
    "    },\n",
    "    \"filed_location\": {\n",
    "        \"history\" : \"../../visualisation/files/history_training/MLE/phase2_phm_data_recipe_66.csv\"\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "K_phase_2 = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialize Neural Network\n",
    "### Options 1-4:\n",
    "#### Architecture 1 - One FCNN (2 layers, last splited for mu and sigma)\n",
    "#### Architecture 2 - Two seperate FCNN for mu and sigma each (2 layers)\n",
    "#### Architecture 3 - Two seperate FCNN for mu and sigma each (3 layers)\n",
    "#### Architecture 4 - Two complete seperate subnetworks (from LSTM layer to last FC layer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(0)\n",
    "model = LstmMle_1(batch_size=param['model']['batch_size'], \n",
    "                 input_dim=param['model']['input_size'], \n",
    "                 n_hidden_lstm=param['model']['n_hidden_lstm'], \n",
    "                 n_layers=param['model']['lstm_layer'],\n",
    "                 dropout_rate_lstm= param['model']['dropout_rate_lstm'],\n",
    "                 dropout_rate_fc= param['model']['dropout_rate_fc'],\n",
    "                 n_hidden_fc_1=param['model']['n_hidden_fc_1'],\n",
    "                 K = K_phase_1,\n",
    "                 option = 1\n",
    "                 )\n",
    "log_message_architecture = \"Architecture: LSTM module and a subsequent FCNN (2 layers, last splited for mu and sigma)\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialize MLE Loss function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = LossMle(param[\"model\"][\"input_size\"], param[\"model\"][\"batch_size\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialize Optimizer and Cyclic Learning Rate Scheduler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.SGD(model.parameters(), lr=1.)  \n",
    "scheduler = torch.optim.lr_scheduler.CyclicLR(optimizer=optimizer, \n",
    "                                              base_lr=param_phase_2['cycling_lr']['base_lr'], \n",
    "                                              max_lr=param_phase_2['cycling_lr']['max_lr'], \n",
    "                                              step_size_up=(len(train_data)/param['model']['batch_size'])*2,  # Authors of Cyclic LR suggest setting step_size 2-8 x training iterations in epoch.\n",
    "                                              mode=param_phase_2['cycling_lr']['mode'],\n",
    "                                              gamma=param_phase_2['cycling_lr']['gamma']\n",
    "                                             )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialize Trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = Trainer(model=model,\n",
    "                  optimizer=optimizer,\n",
    "                  scheduler=scheduler,\n",
    "                  scheduler_active = param[\"cycling_lr\"][\"scheduler_active\"],\n",
    "                  criterion=criterion, \n",
    "                  location_model=param[\"filed_location\"][\"trained_model\"],\n",
    "                  patience=param['training']['patience']\n",
    "                 )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Loop\n",
    "An epoch consists of a learning cycle over all batches of training data and an evaluation of the most recent model with the testing data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start Training phase 2\n",
    "logger.log_message(\"Training phase 2 is started\")\n",
    "\n",
    "for epoch in range(param['training']['n_epochs']):\n",
    "    # Train with batches \n",
    "    mean_epoch_training_loss = trainer.train(data_loader_training)\n",
    "\n",
    "    # Save model if its the best one since the last change in configuration of hyperparameters\n",
    "    status_ok = trainer.save_model(epoch, mean_epoch_training_loss, session_id)\n",
    "    \n",
    "    # Log information of current epoch\n",
    "    logger.log_current_statistics(epoch, mean_epoch_training_loss)\n",
    "    \n",
    "    if not status_ok:\n",
    "        break\n",
    "\n",
    "print(\"Training phase 2 is finished\")\n",
    "logger.log_message(\"Training phase 2 is finished\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "masterarbeit",
   "language": "python",
   "name": "masterarbeit"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
