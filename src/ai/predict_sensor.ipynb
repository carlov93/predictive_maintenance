{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader\n",
    "import torch.optim as optim\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# own Modules \n",
    "from models_sub_net_ls import LstmMse_LatentSpace, LstmMle_LatentSpace, AnalysisLayer\n",
    "from models_mse import LstmMse\n",
    "from models_mle import LstmMle_1, LstmMle_2, LstmMle_3\n",
    "from data_preperator import DataPreperatorPrediction\n",
    "from data_set import DataSet\n",
    "from predictor import PredictorMse, PredictorMle, PredictorMseLatentSpaceAnalyser, PredictorMleLatentSpaceAnalyser, PredictorMleSpecial"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Take care of these things before training:\n",
    "- Select correct path and define droped_features\n",
    "- Change parameter of model\n",
    "- Change filed_location"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parameters phm data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "param = {\n",
    "    \"data\" : {\n",
    "        \"path\" : '../../data/phm_data_challenge/recipe/dataset_for_each_recipe/test/test_recipe_67.csv',\n",
    "        \"droped_feature\" : [\"stage\", \"Lot\", \"runnum\", \"recipe\", \"recipe_step\",\n",
    "                            \"up time\", \"ongoing time\", \n",
    "                            \"ETCHSOURCEUSAGE\", \"ETCHAUXSOURCETIMER\", \n",
    "                            \"ETCHAUX2SOURCETIMER\", \"FIXTURESHUTTERPOSITION\", \"ROTATIONSPEED\"\n",
    "                           ],\n",
    "        \"features_not_to_scale\": []\n",
    "    },\n",
    "    \"model\" : {\n",
    "        \"path\" : \"../../models/MLE_model/phm_data_recipe_67Fold_xx_InputSize12_LayerLstm1_HiddenLstm15_HiddenFc75_Seq25.pt\",\n",
    "        \"input_size\" : 12,\n",
    "        \"n_hidden_lstm\" : 15,\n",
    "        \"n_hidden_fc_1\" : 75,\n",
    "        \"n_hidden_fc_2\" : 25,\n",
    "        \"sequence_size\" : 100,\n",
    "        \"batch_size\" : 50,\n",
    "        \"lstm_layer\" : 1,\n",
    "        \"dropout_rate_lstm\": 0.0,\n",
    "        \"dropout_rate_fc\": 0.2,\n",
    "        \"K\":1\n",
    "    },\n",
    "        \"anomaly_detection\" : {\n",
    "        \"threshold_anomaly\" : [2,20], # Individual Threshold for each Sensor\n",
    "        \"no_standard_deviation\" : 2\n",
    "    },\n",
    "    \"results\": {\n",
    "        \"path\" : \"../visualisation/files/prediction/MLE/phm_recipe_67.csv\",\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parameters artifical data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "param = {\n",
    "    \"data\" : {\n",
    "        \"path\" : '../../data/artifical_signals/artifical_2_signals_errors.csv',\n",
    "        \"droped_feature\" : [\"anomaly\"\n",
    "                           ],\n",
    "        \"features_not_to_scale\": []\n",
    "    },\n",
    "    \"model\" : {\n",
    "        \"path\" : \"../../models/MSE_model/artifical_data_Fold_xx_InputSize2_LayerLstm1_HiddenLstm15_HiddenFc75_Seq25.pt\",\n",
    "        \"input_size\" : 2,\n",
    "        \"n_hidden_lstm\" : 15,\n",
    "        \"n_hidden_fc_1\" : 75,\n",
    "        \"n_hidden_fc_2\" : 25,\n",
    "        \"sequence_size\" : 100,\n",
    "        \"batch_size\" : 50,\n",
    "        \"lstm_layer\" : 1,\n",
    "        \"dropout_rate_lstm\": 0.0,\n",
    "        \"dropout_rate_fc\": 0.2,\n",
    "        \"K\":1\n",
    "    },\n",
    "        \"anomaly_detection\" : {\n",
    "        \"threshold_anomaly\" : [0.75,10], # Individual Threshold for each Sensor\n",
    "        \"no_standard_deviation\" : 2\n",
    "    },\n",
    "    \"results\": {\n",
    "        \"path\" : \"../visualisation/files/prediction/MSE/artifical_2_signals.csv\",\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parameters cpps data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "param = {\n",
    "    \"data\" : {\n",
    "        \"path\" : '../../data/cpps_degradation/large_degeneration/cpps_data_large_degeneration_test.csv',\n",
    "        \"droped_feature\" : [\"status\"\n",
    "                           ],\n",
    "        \"features_not_to_scale\": []\n",
    "    },\n",
    "    \"model\" : {\n",
    "        \"path\" : \"../../models/MSE_model/cpps_data_large_degradationFold_xx_InputSize10_LayerLstm1_HiddenLstm15_HiddenFc75_Seq25.pt\",\n",
    "        \"input_size\" : 10,\n",
    "        \"n_hidden_lstm\" : 15,\n",
    "        \"n_hidden_fc_1\" : 75,\n",
    "        \"n_hidden_fc_2\" : 25,\n",
    "        \"sequence_size\" : 100,\n",
    "        \"batch_size\" : 50,\n",
    "        \"lstm_layer\" : 1,\n",
    "        \"dropout_rate_lstm\": 0.0,\n",
    "        \"dropout_rate_fc\": 0.2,\n",
    "        \"K\":1\n",
    "    },\n",
    "        \"anomaly_detection\" : {\n",
    "        \"threshold_anomaly\" : [0.5,0.5,0.6,0.4,0.5,0.6,0.5,0.75,0.5,0.15],\n",
    "        \"no_standard_deviation\" : 2\n",
    "    },\n",
    "    \"results\": {\n",
    "        \"path\" : \"../visualisation/files/prediction/MSE/cpps_large_degradation.csv\", \n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Standarize Data\n",
    "First we have to apply normalisation to data. That is because the model works on the representation given by its input vectors. The scale of those numbers is part of the representation.\n",
    "We should apply the exact same scaling as for training data. That means storing the scale and offset used with your training data, and using that again. <br>\n",
    "__The mean and variance for each feature of the training data with which the model was trained (stake: 0.75):__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mean and variance from phm dataset - recipe 66"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_training_data =[]\n",
    "var_training_data =[]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mean and variance from phm dataset - recipe 67 (29.219 samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_training_data = [0.00236702, 0.3953089,0.48772743 ,0.3857511, 0.49987399, 0.06291772, -0.03491417,\n",
    "                      0.43371134,  0.23365129, -0.06136357, -0.12245359,  0.2516167]\n",
    "var_training_data =[3.62155978e-04, 7.33502893e-01, 8.30760891e-01, 7.06805763e-01, 9.44752420e-01,\n",
    "                    4.07861536e-01, 1.61079596e-01, 8.46321709e-01, 3.65172841e-01, 6.37131077e-01,\n",
    "                    4.60727666e-11, 9.93519995e-01]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mean and variance from phm dataset - recipe 68"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_training_data =[]\n",
    "var_training_data =[]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mean and variance from artifical dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_training_data= [-0.00526595, -0.00968424]\n",
    "var_training_data = [49.30277603, 0.4232726 ]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mean and Variance form cpps dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_training_data = [-0.06259308,  0.03496554,  0.07455726,  0.08494991, -0.00575792, -0.00922697, -0.04273393,  0.06988059, -0.03368263,  0.03892771]\n",
    "var_training_data =[0.14422845, 0.10077938, 0.06166098, 0.06988583, 0.05482907, 0.07040927, 0.04958053, 0.13373154, 0.0509202, 0.02644641]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(488363, 13)\n"
     ]
    }
   ],
   "source": [
    "data_preperator = DataPreperatorPrediction(path=param['data']['path'], \n",
    "                                           ignored_features = param[\"data\"][\"droped_feature\"],\n",
    "                                           mean_training_data=mean_training_data, \n",
    "                                           var_training_data=var_training_data, \n",
    "                                           first_order_difference=False \n",
    "                                          )                                  \n",
    "preprocessed_data = data_preperator.prepare_data()\n",
    "print(preprocessed_data.shape)\n",
    "\n",
    "dataset = DataSet(preprocessed_data, \n",
    "                  timesteps=param[\"model\"][\"sequence_size\"])\n",
    "data_loader = DataLoader(dataset, \n",
    "                         batch_size=param['model']['batch_size'], \n",
    "                         num_workers=0, \n",
    "                         shuffle=False, \n",
    "                         drop_last=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data of batch: 0\n",
      "Size of input data: torch.Size([50, 100, 13])\n",
      "Size of target data: torch.Size([50, 13])\n",
      "Data of batch: 1\n",
      "Size of input data: torch.Size([50, 100, 13])\n",
      "Size of target data: torch.Size([50, 13])\n"
     ]
    }
   ],
   "source": [
    "for batch_idx, data in enumerate(data_loader):\n",
    "    x,y = data\n",
    "    print('Data of batch: {}'.format(batch_idx))\n",
    "    print(\"Size of input data: {}\".format(x.size()))\n",
    "    print(\"Size of target data: {}\".format(y.size()))\n",
    "    if batch_idx >=1: break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define Model and load Parameters of trained model\n",
    "### Model for MSE "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "IncompatibleKeys(missing_keys=[], unexpected_keys=[])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = LstmMse(batch_size=param['model']['batch_size'], \n",
    "                input_dim=param['model']['input_size'], \n",
    "                n_hidden_lstm=param['model']['n_hidden_lstm'], \n",
    "                n_hidden_fc=param['model']['n_hidden_fc_1'], \n",
    "                n_layers=param['model']['lstm_layer'], \n",
    "                dropout_rate_lstm= param['model']['dropout_rate_lstm'],\n",
    "                dropout_rate_fc= param['model']['dropout_rate_fc']\n",
    "                )\n",
    "\n",
    "checkpoint = torch.load(param[\"model\"][\"path\"])\n",
    "model.load_state_dict(checkpoint['model_state_dict'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model for MLE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "IncompatibleKeys(missing_keys=[], unexpected_keys=[])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = LstmMle_1(batch_size=param['model']['batch_size'], \n",
    "                 input_dim=param['model']['input_size'], \n",
    "                 n_hidden_lstm=param['model']['n_hidden_lstm'], \n",
    "                 n_hidden_fc=param['model']['n_hidden_fc_1'], \n",
    "                 n_layers=param['model']['lstm_layer'], \n",
    "                 dropout_rate_lstm= param['model']['dropout_rate_lstm'],\n",
    "                 dropout_rate_fc= param['model']['dropout_rate_fc'],\n",
    "                 K = param['model']['K'])\n",
    "\n",
    "checkpoint = torch.load(param[\"model\"][\"path\"])\n",
    "model.load_state_dict(checkpoint['model_state_dict'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = LstmMle_3(batch_size=param['model']['batch_size'], \n",
    "                 input_dim=param['model']['input_size'], \n",
    "                 n_hidden_lstm=param['model']['n_hidden_lstm'], \n",
    "                 n_hidden_fc_1=param['model']['n_hidden_fc_1'], \n",
    "                 n_hidden_fc_2=param['model']['n_hidden_fc_2'],\n",
    "                 n_layers=param['model']['lstm_layer'], \n",
    "                 dropout_rate_lstm= param['model']['dropout_rate_lstm'],\n",
    "                 dropout_rate_fc= param['model']['dropout_rate_fc'],\n",
    "                 K = param['model']['K'])\n",
    "\n",
    "checkpoint = torch.load(param[\"model\"][\"path\"])\n",
    "model.load_state_dict(checkpoint['model_state_dict'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialize Predictor\n",
    "### Predictor for MSE Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictor = PredictorMse(model=model,\n",
    "                         path_data=param[\"data\"][\"path\"],\n",
    "                         columns_to_ignore=param[\"data\"][\"droped_feature\"],\n",
    "                         threshold_anomaly=param[\"anomaly_detection\"][\"threshold_anomaly\"]\n",
    "                         )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Predictor for MLE Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictor = PredictorMle(model=model,\n",
    "                         path_data=param[\"data\"][\"path\"],\n",
    "                         columns_to_ignore=param[\"data\"][\"droped_feature\"],\n",
    "                         threshold_anomaly=param[\"anomaly_detection\"][\"threshold_anomaly\"],\n",
    "                         no_standard_deviation=param[\"anomaly_detection\"][\"no_standard_deviation\"]\n",
    "                         )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start predicting.\n",
      "Current status: 0 samples are predicted.\n",
      "Current status: 5000 samples are predicted.\n",
      "Current status: 10000 samples are predicted.\n",
      "Current status: 15000 samples are predicted.\n",
      "Current status: 20000 samples are predicted.\n",
      "Current status: 25000 samples are predicted.\n",
      "Current status: 30000 samples are predicted.\n",
      "Current status: 35000 samples are predicted.\n",
      "Current status: 40000 samples are predicted.\n",
      "Current status: 45000 samples are predicted.\n",
      "Current status: 50000 samples are predicted.\n",
      "Current status: 55000 samples are predicted.\n",
      "Current status: 60000 samples are predicted.\n",
      "Current status: 65000 samples are predicted.\n",
      "Current status: 70000 samples are predicted.\n",
      "Current status: 75000 samples are predicted.\n",
      "Current status: 80000 samples are predicted.\n",
      "Current status: 85000 samples are predicted.\n",
      "Current status: 90000 samples are predicted.\n",
      "Current status: 95000 samples are predicted.\n",
      "Current status: 100000 samples are predicted.\n",
      "Current status: 105000 samples are predicted.\n",
      "Current status: 110000 samples are predicted.\n",
      "Current status: 115000 samples are predicted.\n",
      "Current status: 120000 samples are predicted.\n",
      "Current status: 125000 samples are predicted.\n",
      "Current status: 130000 samples are predicted.\n",
      "Current status: 135000 samples are predicted.\n",
      "Current status: 140000 samples are predicted.\n",
      "Current status: 145000 samples are predicted.\n",
      "Current status: 150000 samples are predicted.\n",
      "Current status: 155000 samples are predicted.\n",
      "Current status: 160000 samples are predicted.\n",
      "Current status: 165000 samples are predicted.\n",
      "Current status: 170000 samples are predicted.\n",
      "Current status: 175000 samples are predicted.\n",
      "Current status: 180000 samples are predicted.\n",
      "Current status: 185000 samples are predicted.\n",
      "Current status: 190000 samples are predicted.\n",
      "Current status: 195000 samples are predicted.\n",
      "Current status: 200000 samples are predicted.\n",
      "Current status: 205000 samples are predicted.\n",
      "Current status: 210000 samples are predicted.\n",
      "Current status: 215000 samples are predicted.\n",
      "Current status: 220000 samples are predicted.\n",
      "Current status: 225000 samples are predicted.\n",
      "Current status: 230000 samples are predicted.\n",
      "Current status: 235000 samples are predicted.\n",
      "Current status: 240000 samples are predicted.\n",
      "Current status: 245000 samples are predicted.\n",
      "Current status: 250000 samples are predicted.\n",
      "Current status: 255000 samples are predicted.\n",
      "Current status: 260000 samples are predicted.\n",
      "Current status: 265000 samples are predicted.\n",
      "Current status: 270000 samples are predicted.\n",
      "Current status: 275000 samples are predicted.\n",
      "Current status: 280000 samples are predicted.\n",
      "Current status: 285000 samples are predicted.\n",
      "Current status: 290000 samples are predicted.\n",
      "Current status: 295000 samples are predicted.\n",
      "Current status: 300000 samples are predicted.\n",
      "Current status: 305000 samples are predicted.\n",
      "Current status: 310000 samples are predicted.\n",
      "Current status: 315000 samples are predicted.\n",
      "Current status: 320000 samples are predicted.\n",
      "Current status: 325000 samples are predicted.\n",
      "Current status: 330000 samples are predicted.\n",
      "Current status: 335000 samples are predicted.\n",
      "Current status: 340000 samples are predicted.\n",
      "Current status: 345000 samples are predicted.\n",
      "Current status: 350000 samples are predicted.\n",
      "Current status: 355000 samples are predicted.\n",
      "Current status: 360000 samples are predicted.\n",
      "Current status: 365000 samples are predicted.\n",
      "Current status: 370000 samples are predicted.\n",
      "Current status: 375000 samples are predicted.\n",
      "Current status: 380000 samples are predicted.\n",
      "Current status: 385000 samples are predicted.\n",
      "Current status: 390000 samples are predicted.\n",
      "Current status: 395000 samples are predicted.\n",
      "Current status: 400000 samples are predicted.\n",
      "Current status: 405000 samples are predicted.\n",
      "Current status: 410000 samples are predicted.\n",
      "Current status: 415000 samples are predicted.\n",
      "Current status: 420000 samples are predicted.\n",
      "Current status: 425000 samples are predicted.\n",
      "Current status: 430000 samples are predicted.\n",
      "Current status: 435000 samples are predicted.\n",
      "Current status: 440000 samples are predicted.\n",
      "Current status: 445000 samples are predicted.\n",
      "Current status: 450000 samples are predicted.\n",
      "Current status: 455000 samples are predicted.\n",
      "Current status: 460000 samples are predicted.\n",
      "Current status: 465000 samples are predicted.\n",
      "Current status: 470000 samples are predicted.\n",
      "Current status: 475000 samples are predicted.\n",
      "Current status: 480000 samples are predicted.\n",
      "Current status: 485000 samples are predicted.\n",
      "End of prediction.\n"
     ]
    }
   ],
   "source": [
    "print(\"Start predicting.\")\n",
    "# Write header\n",
    "with open(param[\"results\"][\"path\"], \"a+\") as file:\n",
    "            [file.write(column+\";\") for column in predictor.create_column_names_result()]\n",
    "            file.write(\"\\n\")\n",
    "\n",
    "for batch_number, (input_data, target_data) in enumerate(data_loader):\n",
    "    # Predict sensor values in mini-batches\n",
    "    batch_results = predictor.predict(input_data, target_data)\n",
    "    \n",
    "    # Write results to csv file\n",
    "    with open(param[\"results\"][\"path\"], \"a\") as file:\n",
    "        for batch in batch_results:\n",
    "            # Each result component of a singe prediction (ID, target, prediction, loss, latent space ...) is stored in lists\n",
    "            # thus we have to unpack the list and seperate values with ;\n",
    "            for value in batch:\n",
    "                file.write(str(value)+\";\")\n",
    "            file.write(\"\\n\")\n",
    "\n",
    "    # Print status \n",
    "    if (batch_number*param['model']['batch_size'])%5000 == 0:\n",
    "        print(\"Current status: \" + str(param['model']['batch_size']*batch_number) + \" samples are predicted.\")\n",
    "\n",
    "print(\"End of prediction.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tag anomalous samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_prediction = pd.read_csv(param[\"results\"][\"path\"], sep=\";\")\n",
    "# Values of column \"loss\" are exponentially smoothed and stored in a new column \"smoothed loss\"\n",
    "# New column \"anomaly\" is created and sample is taged with 1 if anomalous behaviour (if smoothed loss is over threshold)\n",
    "results = predictor.detect_anomaly(results_prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>IONGAUGEPRESSURE target</th>\n",
       "      <th>ETCHBEAMVOLTAGE target</th>\n",
       "      <th>ETCHBEAMCURRENT target</th>\n",
       "      <th>ETCHSUPPRESSORVOLTAGE target</th>\n",
       "      <th>ETCHSUPPRESSORCURRENT target</th>\n",
       "      <th>FLOWCOOLFLOWRATE target</th>\n",
       "      <th>FLOWCOOLPRESSURE target</th>\n",
       "      <th>ETCHGASCHANNEL1READBACK target</th>\n",
       "      <th>ETCHPBNGASREADBACK target</th>\n",
       "      <th>...</th>\n",
       "      <th>Anomaly Sensor_3</th>\n",
       "      <th>Anomaly Sensor_4</th>\n",
       "      <th>Anomaly Sensor_5</th>\n",
       "      <th>Anomaly Sensor_6</th>\n",
       "      <th>Anomaly Sensor_7</th>\n",
       "      <th>Anomaly Sensor_8</th>\n",
       "      <th>Anomaly Sensor_9</th>\n",
       "      <th>Anomaly Sensor_10</th>\n",
       "      <th>Anomaly Sensor_11</th>\n",
       "      <th>Anomaly Sensor_12</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>101.0</td>\n",
       "      <td>-0.142369</td>\n",
       "      <td>-0.189067</td>\n",
       "      <td>-0.271174</td>\n",
       "      <td>-0.214045</td>\n",
       "      <td>-0.358999</td>\n",
       "      <td>-0.543526</td>\n",
       "      <td>-0.353632</td>\n",
       "      <td>0.327329</td>\n",
       "      <td>0.206600</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>102.0</td>\n",
       "      <td>-0.142369</td>\n",
       "      <td>-0.189266</td>\n",
       "      <td>-0.270474</td>\n",
       "      <td>-0.214155</td>\n",
       "      <td>-0.356171</td>\n",
       "      <td>-0.549200</td>\n",
       "      <td>-0.353632</td>\n",
       "      <td>0.341207</td>\n",
       "      <td>0.206600</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>103.0</td>\n",
       "      <td>-0.142369</td>\n",
       "      <td>-0.189266</td>\n",
       "      <td>-0.266971</td>\n",
       "      <td>-0.214100</td>\n",
       "      <td>-0.344493</td>\n",
       "      <td>-0.549200</td>\n",
       "      <td>-0.353632</td>\n",
       "      <td>0.341207</td>\n",
       "      <td>0.206600</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>104.0</td>\n",
       "      <td>-0.142369</td>\n",
       "      <td>-0.189200</td>\n",
       "      <td>-0.269071</td>\n",
       "      <td>-0.213990</td>\n",
       "      <td>-0.355817</td>\n",
       "      <td>-0.537146</td>\n",
       "      <td>-0.353632</td>\n",
       "      <td>0.328964</td>\n",
       "      <td>0.210369</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>105.0</td>\n",
       "      <td>-0.142369</td>\n",
       "      <td>-0.188933</td>\n",
       "      <td>-0.265804</td>\n",
       "      <td>-0.213880</td>\n",
       "      <td>-0.345910</td>\n",
       "      <td>-0.537146</td>\n",
       "      <td>-0.353632</td>\n",
       "      <td>0.328964</td>\n",
       "      <td>0.210369</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 62 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      ID  IONGAUGEPRESSURE target  ETCHBEAMVOLTAGE target  \\\n",
       "0  101.0                -0.142369               -0.189067   \n",
       "1  102.0                -0.142369               -0.189266   \n",
       "2  103.0                -0.142369               -0.189266   \n",
       "3  104.0                -0.142369               -0.189200   \n",
       "4  105.0                -0.142369               -0.188933   \n",
       "\n",
       "   ETCHBEAMCURRENT target  ETCHSUPPRESSORVOLTAGE target  \\\n",
       "0               -0.271174                     -0.214045   \n",
       "1               -0.270474                     -0.214155   \n",
       "2               -0.266971                     -0.214100   \n",
       "3               -0.269071                     -0.213990   \n",
       "4               -0.265804                     -0.213880   \n",
       "\n",
       "   ETCHSUPPRESSORCURRENT target  FLOWCOOLFLOWRATE target  \\\n",
       "0                     -0.358999                -0.543526   \n",
       "1                     -0.356171                -0.549200   \n",
       "2                     -0.344493                -0.549200   \n",
       "3                     -0.355817                -0.537146   \n",
       "4                     -0.345910                -0.537146   \n",
       "\n",
       "   FLOWCOOLPRESSURE target  ETCHGASCHANNEL1READBACK target  \\\n",
       "0                -0.353632                        0.327329   \n",
       "1                -0.353632                        0.341207   \n",
       "2                -0.353632                        0.341207   \n",
       "3                -0.353632                        0.328964   \n",
       "4                -0.353632                        0.328964   \n",
       "\n",
       "   ETCHPBNGASREADBACK target  ...  Anomaly Sensor_3  Anomaly Sensor_4  \\\n",
       "0                   0.206600  ...                 0                 0   \n",
       "1                   0.206600  ...                 0                 0   \n",
       "2                   0.206600  ...                 0                 0   \n",
       "3                   0.210369  ...                 0                 0   \n",
       "4                   0.210369  ...                 0                 0   \n",
       "\n",
       "   Anomaly Sensor_5  Anomaly Sensor_6  Anomaly Sensor_7  Anomaly Sensor_8  \\\n",
       "0                 0                 0                 0                 0   \n",
       "1                 0                 0                 0                 0   \n",
       "2                 0                 0                 0                 0   \n",
       "3                 0                 0                 0                 0   \n",
       "4                 0                 0                 0                 0   \n",
       "\n",
       "   Anomaly Sensor_9  Anomaly Sensor_10  Anomaly Sensor_11  Anomaly Sensor_12  \n",
       "0                 0                  0                  0                  0  \n",
       "1                 0                  0                  0                  0  \n",
       "2                 0                  0                  0                  0  \n",
       "3                 0                  0                  0                  0  \n",
       "4                 0                  0                  0                  0  \n",
       "\n",
       "[5 rows x 62 columns]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Combine prediction data with data which was not consider for inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "original_sensor_data = pd.read_csv(param[\"data\"][\"path\"])\n",
    "data_of_droped_feature = original_sensor_data.loc[:, param[\"data\"][\"droped_feature\"]+[\"ID\"]]\n",
    "complete_data = results.merge(right=data_of_droped_feature, how=\"inner\", on=\"ID\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save data to csv file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "complete_data.to_csv(param[\"results\"][\"path\"], sep=\";\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "masterarbeit",
   "language": "python",
   "name": "masterarbeit"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
