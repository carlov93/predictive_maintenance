{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader\n",
    "import torch.optim as optim\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# own Modules \n",
    "from models_mse import LstmMse\n",
    "from models_mle import LstmMle_1, LstmMle_2, LstmMle_3, LstmMle_4\n",
    "from data_preperator import DataPreperatorPrediction\n",
    "from data_set import DataSet\n",
    "from predictor import PredictorMse, PredictorMle\n",
    "from predictor_cell_state import PredictorMleCellState\n",
    "from anomaly_classifier import AnomalyClassifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Mode: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "mode = \"MLE\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parameters phm data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "param = {\n",
    "    \"data\" : {\n",
    "        \"path\" : '../../data/phm_data_challenge/recipe/dataset_for_each_recipe/test/test_recipe_67_labeled_short.csv',\n",
    "        \"droped_feature\" : [\"stage\", \"Lot\", \"runnum\", \"recipe\", \"recipe_step\",\n",
    "                            \"up time\", \"ongoing time\", \n",
    "                            \"ETCHSOURCEUSAGE\", \"ETCHAUXSOURCETIMER\", \n",
    "                            \"ETCHAUX2SOURCETIMER\", \"FIXTURESHUTTERPOSITION\", \"ROTATIONSPEED\", \"status\"\n",
    "                           ],\n",
    "        \"features_not_to_scale\": []\n",
    "    },\n",
    "    \"model\" : {\n",
    "        \"path\" : \"../../models/{}_model/phm67_SQ36_id40787.pt\".format(mode),\n",
    "        \"input_size\" : 12,\n",
    "        \"n_hidden_lstm\" : 13,\n",
    "        \"n_hidden_fc_1\" : 55,\n",
    "        \"n_hidden_fc_2\" : 55,\n",
    "        \"sequence_size\" : 8,\n",
    "        \"batch_size\" : 50,\n",
    "        \"lstm_layer\" : 1,\n",
    "        \"dropout_rate_lstm\": 0.0,\n",
    "        \"dropout_rate_fc\": 0.2,\n",
    "        \"K\":1\n",
    "    },\n",
    "    \"results\": {\n",
    "        \"path\" : \"../visualisation/files/prediction/{}/2_research_question/phm_67_SQ36.csv\".format(mode),\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parameters artifical data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "param = {\n",
    "    \"data\" : {\n",
    "        \"path\" : '../../data/artifical_signals/artifical_2_signals_errors.csv',\n",
    "        \"droped_feature\" : [\"anomaly\"\n",
    "                           ],\n",
    "        \"features_not_to_scale\": []\n",
    "    },\n",
    "    \"model\" : {\n",
    "        \"path\" : \"../../models/{}_model/art2_id82478.pt\".format(mode),\n",
    "        \"input_size\" : 2,\n",
    "        \"n_hidden_lstm\" : 8,\n",
    "        \"n_hidden_fc_1\" : 55,\n",
    "        \"n_hidden_fc_2\" : 25,\n",
    "        \"sequence_size\" : 21,\n",
    "        \"batch_size\" : 50,\n",
    "        \"lstm_layer\" : 1,\n",
    "        \"dropout_rate_lstm\": 0.0,\n",
    "        \"dropout_rate_fc\": 0.2,\n",
    "        \"K\":1\n",
    "    },\n",
    "    \"results\": {\n",
    "        \"path\" : \"../visualisation/files/prediction/{}/artfic.csv\".format(mode),\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parameters cpps data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "param = {\n",
    "    \"data\" : {\n",
    "        \"path\" : '../../data/cpps_degradation_new/data_obs10/test/obs_space_error_sinusiod_preprocessed.csv',\n",
    "        \"droped_feature\" : [\"status\"\n",
    "                           ],\n",
    "        \"features_not_to_scale\": []\n",
    "    },\n",
    "    \"model\" : {\n",
    "        \"path\" : \"../../models/{}_model/cpps_id61644.pt\".format(mode),\n",
    "        \"input_size\" : 10,\n",
    "        \"n_hidden_lstm\" : 21,\n",
    "        \"n_hidden_fc_1\" : 55,\n",
    "        \"n_hidden_fc_2\" : 25,\n",
    "        \"sequence_size\" : 8,\n",
    "        \"batch_size\" : 50,\n",
    "        \"lstm_layer\" : 1,\n",
    "        \"dropout_rate_lstm\": 0.0,\n",
    "        \"dropout_rate_fc\": 0.2,\n",
    "        \"K\":1\n",
    "    },\n",
    "    \"results\": {\n",
    "        \"path\" : \"../visualisation/files/prediction/{}/cpps.csv\".format(mode), \n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parameters ramming data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "param = {\n",
    "    \"data\" : {\n",
    "        \"path\" : '../../data/ramming_data/data_train_preprocessed.csv',\n",
    "        \"droped_feature\" : [\"status\"\n",
    "                           ],\n",
    "        \"features_not_to_scale\": []\n",
    "    },\n",
    "    \"model\" : {\n",
    "        \"path\" : \"../../models/{}_model/ramming_id59939.pt\".format(mode),\n",
    "        \"input_size\" : 50,\n",
    "        \"n_hidden_lstm\" : 100,\n",
    "        \"n_hidden_fc_1\" : 100,\n",
    "        \"n_hidden_fc_2\" : 25,\n",
    "        \"sequence_size\" : 8,\n",
    "        \"batch_size\" : 50,\n",
    "        \"lstm_layer\" : 1,\n",
    "        \"dropout_rate_lstm\": 0.0,\n",
    "        \"dropout_rate_fc\": 0.2,\n",
    "        \"K\":1\n",
    "    },\n",
    "    \"results\": {\n",
    "        \"path\" : \"../visualisation/files/prediction/{}/ramming_train.csv\".format(mode), \n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parameters variance data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "param = {\n",
    "    \"data\" : {\n",
    "        \"path\" : '../../data/variation_in_variance/dataset_increasing_variance.csv',\n",
    "        \"droped_feature\" : [\"time\", \"mu\", \"sigma\" \n",
    "                           ],\n",
    "        \"features_not_to_scale\": []\n",
    "    },\n",
    "    \"model\" : {\n",
    "        \"path\" : \"../../models/{}_model/variance_dataset_increasing_id30944.pt\".format(mode),\n",
    "        \"input_size\" : 1,\n",
    "        \"n_hidden_lstm\" : 2,\n",
    "        \"n_hidden_fc_1\" : 5,\n",
    "        \"n_hidden_fc_2\" : 5,\n",
    "        \"sequence_size\" : 36,\n",
    "        \"batch_size\" : 8,\n",
    "        \"lstm_layer\" : 1,\n",
    "        \"dropout_rate_lstm\": 0.0,\n",
    "        \"dropout_rate_fc\": 0.2,\n",
    "        \"K\":1\n",
    "    },\n",
    "    \"results\": {\n",
    "        \"path\" : \"../visualisation/files/prediction/{}/variance_data_new.csv\".format(mode), \n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Standarize Data\n",
    "First we have to apply normalisation to data. That is because the model works on the representation given by its input vectors. The scale of those numbers is part of the representation.\n",
    "We should apply the exact same scaling as for training data. That means storing the scale and offset used with your training data, and using that again. <br>\n",
    "__The mean and variance for each feature of the training data with which the model was trained (stake: 0.75):__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mean and variance from phm dataset - recipe 67 (29.219 samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_training_data = [0.00236702, 0.3953089,0.48772743 ,0.3857511, 0.49987399, 0.06291772, -0.03491417,\n",
    "                      0.43371134,  0.23365129, -0.06136357, -0.12245359,  0.2516167]\n",
    "var_training_data =[3.62155978e-04, 7.33502893e-01, 8.30760891e-01, 7.06805763e-01, 9.44752420e-01,\n",
    "                    4.07861536e-01, 1.61079596e-01, 8.46321709e-01, 3.65172841e-01, 6.37131077e-01,\n",
    "                    4.60727666e-11, 9.93519995e-01]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mean and variance from artifical dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_training_data= [-0.00526595, -0.00968424]\n",
    "var_training_data = [49.30277603, 0.4232726 ]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mean and Variance from cpps dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_training_data = [-0.06942446, -0.03770532, -0.01556928, -0.04071856, -0.04357261, -0.06645444, -0.06779275, \n",
    "                      -0.01516744, -0.04079089, -0.03428566]\n",
    "var_training_data = [0.15114678, 0.1605483,  0.07761351, 0.06546521, 0.09442645, 0.10429262,\n",
    "                     0.09060877, 0.07936068, 0.06720505, 0.09224595]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mean and Variance from ramming dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_training_data = [ 0.66994451,  1.3511098,  -1.04790224,  1.37593449,  1.52857629,  0.83683179,\n",
    "                      -0.59174875,  0.2276904,  -0.30149707,  0.00659975, -0.61039718, -1.40061621,\n",
    "                      -1.53808693,  0.78614893, -2.94451175, -0.79364646,  1.07591868,  2.14055191,\n",
    "                      -1.10448315,  0.76984998, -0.84240901,  0.21670491, -0.9561242,   0.61804757,\n",
    "                      -1.98817668, -0.99106694, -0.54200903,  1.12792316, -0.31659197,  2.63421421,\n",
    "                      1.13675707, -2.27478105,  2.37692224,  2.35124433,  1.50218261, -2.04472942,\n",
    "                      2.50210775, -1.04667747, -0.95624398,  1.22236255,  0.41518363,  1.33544657,\n",
    "                      0.75903185, -0.41838231, -0.4369283,   0.60359194,  1.64544459, -2.07289658,\n",
    "                      2.64145022,  2.16104454]\n",
    "var_training_data =[10.35878551, 11.88827756, 21.27251956,  6.79734976, 14.07217738, 10.10726723,\n",
    "                    15.16597587, 18.13221091,  4.27889808,  6.80582467,  2.44855596, 10.04390927,\n",
    "                    7.28379418, 20.37918374, 28.1458679,   8.56124384, 18.35279778,  9.91740212,\n",
    "                    17.30631408,  6.05122757,  4.67344851,  9.97122612,  6.89223197,  8.30739432,\n",
    "                    22.54071533, 13.78669072,  3.27853119,  5.49280397, 15.4525496,  36.09376821,\n",
    "                    5.22595375, 36.02409948, 26.91623789, 13.31752889, 23.78231544,  9.16123925,\n",
    "                    19.53221726,  7.35945676,  5.67204309,  5.67213501, 11.63879868,  3.3638181,\n",
    "                    12.8887632,  15.69187744, 10.40778802,  5.5671719,  10.16745852, 17.93333693,\n",
    "                    8.56480474, 15.36198149]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mean and Variance from variance dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_training_data =[0.06844859]\n",
    "var_training_data = [112.93894129]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3000, 2)\n"
     ]
    }
   ],
   "source": [
    "data_preperator = DataPreperatorPrediction(path=param['data']['path'], \n",
    "                                           ignored_features = param[\"data\"][\"droped_feature\"],\n",
    "                                           mean_training_data=mean_training_data, \n",
    "                                           var_training_data=var_training_data, \n",
    "                                           first_order_difference=False \n",
    "                                          )                                  \n",
    "preprocessed_data = data_preperator.prepare_data()\n",
    "print(preprocessed_data.shape)\n",
    "\n",
    "dataset = DataSet(preprocessed_data, \n",
    "                  timesteps=param[\"model\"][\"sequence_size\"])\n",
    "data_loader = DataLoader(dataset, \n",
    "                         batch_size=param['model']['batch_size'], \n",
    "                         num_workers=0, \n",
    "                         shuffle=False, \n",
    "                         drop_last=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data of batch: 0\n",
      "Size of input data: torch.Size([8, 36, 2])\n",
      "Size of target data: torch.Size([8, 2])\n",
      "Data of batch: 1\n",
      "Size of input data: torch.Size([8, 36, 2])\n",
      "Size of target data: torch.Size([8, 2])\n"
     ]
    }
   ],
   "source": [
    "for batch_idx, data in enumerate(data_loader):\n",
    "    x,y = data\n",
    "    print('Data of batch: {}'.format(batch_idx))\n",
    "    print(\"Size of input data: {}\".format(x.size()))\n",
    "    print(\"Size of target data: {}\".format(y.size()))\n",
    "    if batch_idx >=1: break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define Model and load Parameters of trained model\n",
    "### Model for MSE "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Error(s) in loading state_dict for LstmMse:\n\tMissing key(s) in state_dict: \"fc2.weight\", \"fc2.bias\". \n\tUnexpected key(s) in state_dict: \"fc_y_hat.weight\", \"fc_y_hat.bias\", \"fc_tau.weight\", \"fc_tau.bias\". ",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-17-fe21ef8e2fc1>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0mcheckpoint\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparam\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"model\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"path\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_state_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcheckpoint\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'model_state_dict'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/jupyter_notebooks/masterarbeit/venv_pm/lib/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36mload_state_dict\u001b[0;34m(self, state_dict, strict)\u001b[0m\n\u001b[1;32m    775\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0merror_msgs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    776\u001b[0m             raise RuntimeError('Error(s) in loading state_dict for {}:\\n\\t{}'.format(\n\u001b[0;32m--> 777\u001b[0;31m                                self.__class__.__name__, \"\\n\\t\".join(error_msgs)))\n\u001b[0m\u001b[1;32m    778\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0m_IncompatibleKeys\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmissing_keys\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0munexpected_keys\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    779\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Error(s) in loading state_dict for LstmMse:\n\tMissing key(s) in state_dict: \"fc2.weight\", \"fc2.bias\". \n\tUnexpected key(s) in state_dict: \"fc_y_hat.weight\", \"fc_y_hat.bias\", \"fc_tau.weight\", \"fc_tau.bias\". "
     ]
    }
   ],
   "source": [
    "model = LstmMse(batch_size=param['model']['batch_size'], \n",
    "                input_dim=param['model']['input_size'], \n",
    "                n_hidden_lstm=param['model']['n_hidden_lstm'], \n",
    "                n_hidden_fc=param['model']['n_hidden_fc_1'], \n",
    "                n_layers=param['model']['lstm_layer'], \n",
    "                dropout_rate_lstm= param['model']['dropout_rate_lstm'],\n",
    "                dropout_rate_fc= param['model']['dropout_rate_fc']\n",
    "                )\n",
    "\n",
    "checkpoint = torch.load(param[\"model\"][\"path\"])\n",
    "model.load_state_dict(checkpoint['model_state_dict'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model for MLE\n",
    "#### Architecture 1 - One FCNN (2 layers, last splited for mu and sigma)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "IncompatibleKeys(missing_keys=[], unexpected_keys=[])"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = LstmMle_1(batch_size=param['model']['batch_size'], \n",
    "                 input_dim=param['model']['input_size'], \n",
    "                 n_hidden_lstm=param['model']['n_hidden_lstm'], \n",
    "                 n_hidden_fc_1=param['model']['n_hidden_fc_1'], \n",
    "                 n_layers=param['model']['lstm_layer'], \n",
    "                 dropout_rate_lstm= param['model']['dropout_rate_lstm'],\n",
    "                 dropout_rate_fc= param['model']['dropout_rate_fc'],\n",
    "                 K = param['model']['K'])\n",
    "\n",
    "checkpoint = torch.load(param[\"model\"][\"path\"])\n",
    "model.load_state_dict(checkpoint['model_state_dict'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Architecture 2 - Two seperate FCNN for mu and sigma each (2 layers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "IncompatibleKeys(missing_keys=[], unexpected_keys=[])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = LstmMle_2(batch_size=param['model']['batch_size'], \n",
    "                 input_dim=param['model']['input_size'], \n",
    "                 n_hidden_lstm=param['model']['n_hidden_lstm'], \n",
    "                 n_layers=param['model']['lstm_layer'],\n",
    "                 dropout_rate_lstm= param['model']['dropout_rate_lstm'],\n",
    "                 dropout_rate_fc= param['model']['dropout_rate_fc'],\n",
    "                 n_hidden_fc_1=param['model']['n_hidden_fc_1'],\n",
    "                 K = param['model']['K']\n",
    "                 )\n",
    "\n",
    "checkpoint = torch.load(param[\"model\"][\"path\"])\n",
    "model.load_state_dict(checkpoint['model_state_dict'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Architecture 3 - Two seperate FCNN for mu and sigma each (3 layers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "IncompatibleKeys(missing_keys=[], unexpected_keys=[])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.manual_seed(0)\n",
    "model = LstmMle_3(batch_size=param['model']['batch_size'], \n",
    "                 input_dim=param['model']['input_size'], \n",
    "                 n_hidden_lstm=param['model']['n_hidden_lstm'], \n",
    "                 n_layers=param['model']['lstm_layer'],\n",
    "                 dropout_rate_lstm= param['model']['dropout_rate_lstm'],\n",
    "                 dropout_rate_fc= param['model']['dropout_rate_fc'],\n",
    "                 n_hidden_fc_1=param['model']['n_hidden_fc_1'],\n",
    "                 n_hidden_fc_2=param['model']['n_hidden_fc_2'],\n",
    "                 K = param['model']['K']\n",
    "                 )\n",
    "checkpoint = torch.load(param[\"model\"][\"path\"])\n",
    "model.load_state_dict(checkpoint['model_state_dict'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Architecture 4 - Two complete seperate subnetworks (from LSTM layer to last FC layer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "IncompatibleKeys(missing_keys=[], unexpected_keys=[])"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.manual_seed(0)\n",
    "model = LstmMle_4(batch_size=param['model']['batch_size'], \n",
    "                 input_dim=param['model']['input_size'], \n",
    "                 n_hidden_lstm=param['model']['n_hidden_lstm'], \n",
    "                 n_layers=param['model']['lstm_layer'],\n",
    "                 dropout_rate_lstm= param['model']['dropout_rate_lstm'],\n",
    "                 dropout_rate_fc= param['model']['dropout_rate_fc'],\n",
    "                 n_hidden_fc_1=param['model']['n_hidden_fc_1'],\n",
    "                 K = param['model']['K']\n",
    "                 )\n",
    "checkpoint = torch.load(param[\"model\"][\"path\"])\n",
    "model.load_state_dict(checkpoint['model_state_dict'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialize Predictor\n",
    "### For MSE Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictor = PredictorMse(model=model,\n",
    "                         path_data=param[\"data\"][\"path\"],\n",
    "                         columns_to_ignore=param[\"data\"][\"droped_feature\"]\n",
    "                         )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### For MLE Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictor = PredictorMle(model=model,\n",
    "                         path_data=param[\"data\"][\"path\"],\n",
    "                         columns_to_ignore=param[\"data\"][\"droped_feature\"],\n",
    "                         )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start predicting.\n",
      "Current status: 0 samples are predicted.\n",
      "End of prediction.\n"
     ]
    }
   ],
   "source": [
    "print(\"Start predicting.\")\n",
    "# Write header\n",
    "with open(param[\"results\"][\"path\"], \"a+\") as file:\n",
    "            [file.write(column+\";\") for column in predictor.create_column_names_result()]\n",
    "            file.write(\"\\n\")\n",
    "\n",
    "for batch_number, (input_data, target_data) in enumerate(data_loader):\n",
    "    # Predict sensor values in mini-batches\n",
    "    batch_results = predictor.predict(input_data, target_data)\n",
    "    \n",
    "    # Write results to csv file\n",
    "    with open(param[\"results\"][\"path\"], \"a\") as file:\n",
    "        for batch in batch_results:\n",
    "            # Each result component of a singe prediction (ID, target, prediction, loss, latent space ...) is stored in lists\n",
    "            # thus we have to unpack the list and seperate values with ;\n",
    "            for value in batch:\n",
    "                file.write(str(value)+\";\")\n",
    "            file.write(\"\\n\")\n",
    "\n",
    "    # Print status \n",
    "    if (batch_number*param['model']['batch_size'])%5000 == 0:\n",
    "        print(\"Current status: \" + str(param['model']['batch_size']*batch_number) + \" samples are predicted.\")\n",
    "\n",
    "print(\"End of prediction.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "masterarbeit",
   "language": "python",
   "name": "masterarbeit"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
