{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader\n",
    "import torch.optim as optim\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# own Modules \n",
    "from models.models import LstmMseDropout\n",
    "from utils.data_loader import DataPreperatorPrediction, DataSet\n",
    "from loss_module import LossModuleMse, LossModuleMle\n",
    "from utils.predictor import Predictor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "param = {\n",
    "    \"data\" : {\n",
    "        \"stake_training_data\" : 0.75, \n",
    "        \"path\" : '../../data/artifical_signals/NewBlade_degradation_freq.csv', \n",
    "    },\n",
    "    \"model\" : {\n",
    "        \"path\" : \"../../models/MSE_model/best_model_InputSize8_LayerLstm2_HiddenLstm300_HiddenFc75_Seq150.pt\",\n",
    "        \"input_size\" : 8,\n",
    "        \"n_hidden_lstm\" : 300,\n",
    "        \"sequence_size\" : 50,\n",
    "        \"batch_size\" : 1,  # Have to be 1 in prediction mode!!!\n",
    "        \"lstm_layer\" : 2,\n",
    "        \"n_hidden_fc\": 75,\n",
    "        \"dropout_rate\": 0.2\n",
    "    },\n",
    "    \"results\": {\n",
    "        \"path_prediction\" : \"../visualisation/files/prediction/NewBlade_degradation_freq.csv\",\n",
    "        \"path_residual\" : \"../visualisation/files/residuals/NewBlade_degradation_freq.csv\"\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Standarize Data\n",
    "First we have to apply normalisation to data. That is because the model works on the representation given by its input vectors. The scale of those numbers is part of the representation.\n",
    "We should apply the exact same scaling as for training data. That means storing the scale and offset used with your training data, and using that again. <br>\n",
    "__The mean and variance for each feature of the training data with which the model was trained (stake: 0.75):__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mean and Variance from NewBlade Training Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_training_data = [-5.37536613e-02, -2.53111489e-04, -8.82854465e+05, 7.79034183e+02, 1.45531178e+04, 1.37766733e+03, 6.50149764e-01] \n",
    "var_training_data = [1.25303578e-01, 1.16898690e-03, 2.86060835e+06, 1.64515717e+06, 6.85728371e+06, 3.63196175e+05, 8.21463343e-03]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mean and Variance from Artifical Training Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_training_data= [-5.31764899e-02, -3.98576146e-04, -8.82773455e+05,  8.25672897e+02, 1.47034247e+04,  1.42685595e+03,  6.62155736e-01,  1.23172374e-02]\n",
    "var_training_data = [1.28792583e-01, 1.21258617e-03, 2.90245238e+06, 1.72279458e+06, 6.83095901e+06, 3.12357562e+05, 3.89033076e-03, 5.01164766e+01]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mean and Variance from Artifical Training Data with First Order Difference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_training_data=[-9.91425250e-05,  1.20717099e-05,  3.30086724e+00,  2.28118387e+00, 5.70847231e+00,  1.32484425e+00,  1.20857364e-04, -5.39472122e-04]\n",
    "var_training_data =[1.04538899e-02, 1.32004086e-03, 2.77006730e+01, 6.11135598e+02, 5.13248993e+00, 3.31823108e+02, 1.11835724e-03 1.14592843e+00]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_preperator = DataPreperatorPrediction(param['data']['path'], mean_training_data, var_training_data, param[\"model\"][\"input_size\"])\n",
    "preprocessed_data = data_preperator.provide_data()\n",
    "dataset = DataSet(preprocessed_data, timesteps=param[\"model\"][\"sequence_size\"])\n",
    "data_loader = DataLoader(dataset, batch_size=param['model']['batch_size'], num_workers=1, shuffle=False, drop_last=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define Model and load Parameters of trained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "IncompatibleKeys(missing_keys=[], unexpected_keys=[])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = LstmMseDropout(batch_size=param['model']['batch_size'], \n",
    "                       input_dim=param['model']['input_size'], \n",
    "                       n_hidden_lstm=param['model']['n_hidden_lstm'], \n",
    "                       n_hidden_fc=param['model']['n_hidden_fc'], \n",
    "                       n_layers=param['model']['lstm_layer'], \n",
    "                       dropout_rate= param['model']['dropout_rate'])\n",
    "\n",
    "checkpoint = torch.load(param[\"model\"][\"path\"])\n",
    "model.load_state_dict(checkpoint['model_state_dict'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define Loss Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = LossModuleMse(param[\"model\"][\"input_size\"], param[\"model\"][\"batch_size\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialize Trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictor = Predictor(model=model,\n",
    "                      criterion=criterion,\n",
    "                      path_data=param[\"data\"][\"path\"],\n",
    "                      path_results=param[\"results\"][\"path_prediction\"],\n",
    "                      path_residuals=param[\"results\"][\"path_residual\"],\n",
    "                      columns_to_ignore=[\"timestamp\"]\n",
    "                      )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start predicting\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/carlovoss/jupyter_notebooks/masterarbeit/venv_pm/lib/python3.6/site-packages/torch/nn/functional.py:1374: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n",
      "  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished predicting\n"
     ]
    }
   ],
   "source": [
    "results = predictor.predict(data_loader)\n",
    "predictor.save_prediction(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compute Residuals for New Blade"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "residuals = predictor.compute_residuals(results)\n",
    "predictor.save_residuals(residuals)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "masterarbeit",
   "language": "python",
   "name": "masterarbeit"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
