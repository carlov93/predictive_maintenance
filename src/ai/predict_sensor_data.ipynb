{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader\n",
    "import torch.optim as optim\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# own Modules \n",
    "from models.models import LstmMseDropout\n",
    "from utils.data_loader import DataPreperatorPrediction, DataSet\n",
    "from loss_module import LossModuleMse, LossModuleMle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "hyperparam = {\n",
    "    \"data\" : {\n",
    "        \"stake_training_data\" : 0.75, \n",
    "        \"path_new\" : '../../data/vega_shrinkwrapper_original/NewBlade/', \n",
    "        \"path_worn\" : '../../data/vega_shrinkwrapper_original/WornBlade/',\n",
    "        \"columns\" :[\"timestamp\", \"cut_torque_target\", \"cut_lag error_target\", \n",
    "                    \"cut_position_target\", \"cut_speed_target\", \"film_position_target\", \n",
    "                    \"film_speed_target\", \"film_lag_error_target\", \"cut_torque_predicted\", \n",
    "                    \"cut_lag error_predicted\", \"cut_position_predicted\", \"cut_speed_predicted\", \n",
    "                    \"film_position_predicted\", \"film_speed_predicted\", \"film_lag_error_predicted\", \"prediction_loss\"]\n",
    "    },\n",
    "    \"model\" : {\n",
    "        \"path\" : \"../../models/MSE_model/best_model_aws_tanh.pt\",\n",
    "        \"input_size\" : 7,\n",
    "        \"n_hidden\" : 150,\n",
    "        \"sequence_size\" : 5,\n",
    "        \"batch_size\" : 1,\n",
    "        \"lstm_layer\" : 3,\n",
    "    },\n",
    "    \"filed_location\": {\n",
    "        \"prediction_new\" : \"../visualisation/files/prediction/prediction_new_blade_tanh_seq20.csv\",\n",
    "        \"prediction_worn\" : \"../visualisation/files/prediction/prediction_worn_blade_tanh_seq20.csv\",\n",
    "        \"residual_new\" : \"../visualisation/files/residuals/residual_new_blade_seq5.csv\",\n",
    "        \"residual_worn\" : \"../visualisation/files/residuals/residual_worn_blade_seq5.csv\"\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predict new blade data\n",
    "## Standarize Validation Data\n",
    "First we have to apply normalisation to data. That is because the model works on the representation given by its input vectors. The scale of those numbers is part of the representation.\n",
    "We should apply the exact same scaling as for training data. That means storing the scale and offset used with your training data, and using that again. <br>\n",
    "__The mean and variance for each feature of the training data with which the model was trained (stake: 0.75):__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mean and Variance from NewBlade Training Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_training_data = [-5.37536613e-02, -2.53111489e-04, -8.82854465e+05, 7.79034183e+02, 1.45531178e+04, 1.37766733e+03, 6.50149764e-01] \n",
    "var_training_data = [1.25303578e-01, 1.16898690e-03, 2.86060835e+06, 1.64515717e+06, 6.85728371e+06, 3.63196175e+05, 8.21463343e-03]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mean and Variance from Artifical Training Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_training_data= [-5.31764899e-02, -3.98576146e-04, -8.82773455e+05,  8.25672897e+02, 1.47034247e+04,  1.42685595e+03,  6.62155736e-01,  1.23172374e-02]\n",
    "var_training_data = [1.28792583e-01, 1.21258617e-03, 2.90245238e+06, 1.72279458e+06, 6.83095901e+06, 3.12357562e+05, 3.89033076e-03, 5.01164766e+01]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load and scale training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_blade_loader = DataPreperatorPrediction(hyperparam['data']['path_new']+'NewBlade001.csv', mean_training_data, var_training_data, hyperparam[\"model\"][\"input_size\"])\n",
    "preprocessed_new_blade = new_blade_loader.provide_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load worn blade dataset and scale them with mean and variance of training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "worn_blade_loader = DataPreperatorPrediction(hyperparam['data']['path_worn']+'WornBlade001.csv', mean_training_data, var_training_data, hyperparam[\"model\"][\"input_size\"])\n",
    "preprocessed_worn_blade = worn_blade_loader.provide_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_worn_blade = DataSet(preprocessed_worn_blade, timesteps=hyperparam[\"model\"][\"sequence_size\"])\n",
    "dataset_new_blade = DataSet(preprocessed_new_blade, timesteps=hyperparam[\"model\"][\"sequence_size\"])\n",
    "\n",
    "data_loader_worn_blade = DataLoader(dataset_worn_blade, batch_size=1, num_workers=1, shuffle=False, drop_last=True)\n",
    "data_loader_new_blade = DataLoader(dataset_new_blade, batch_size=1, num_workers=1, shuffle=False, drop_last=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LstmMseDropout(\n",
       "  (lstm): LSTM(7, 150, num_layers=3, batch_first=True, dropout=0.2)\n",
       "  (fc1): Linear(in_features=150, out_features=50, bias=True)\n",
       "  (dropout): Dropout(p=0.2)\n",
       "  (fc2): Linear(in_features=50, out_features=7, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Initiate and load model\n",
    "model = LstmMseDropout(batch_size=hyperparam['model']['batch_size'], input_dim=hyperparam['model']['input_size'], \n",
    "             n_hidden=hyperparam['model']['n_hidden'], n_layers=hyperparam['model']['lstm_layer'], dropout_rate=0.2)\n",
    "\n",
    "PATH = hyperparam[\"model\"][\"path\"]\n",
    "checkpoint = torch.load(PATH)\n",
    "model.load_state_dict(checkpoint['model_state_dict'])\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lstm.weight_ih_l0 torch.Size([600, 7])\n",
      "lstm.weight_hh_l0 torch.Size([600, 150])\n",
      "lstm.bias_ih_l0 torch.Size([600])\n",
      "lstm.bias_hh_l0 torch.Size([600])\n",
      "lstm.weight_ih_l1 torch.Size([600, 150])\n",
      "lstm.weight_hh_l1 torch.Size([600, 150])\n",
      "lstm.bias_ih_l1 torch.Size([600])\n",
      "lstm.bias_hh_l1 torch.Size([600])\n",
      "lstm.weight_ih_l2 torch.Size([600, 150])\n",
      "lstm.weight_hh_l2 torch.Size([600, 150])\n",
      "lstm.bias_ih_l2 torch.Size([600])\n",
      "lstm.bias_hh_l2 torch.Size([600])\n",
      "fc1.weight torch.Size([50, 150])\n",
      "fc1.bias torch.Size([50])\n",
      "fc2.weight torch.Size([7, 50])\n",
      "fc2.bias torch.Size([7])\n"
     ]
    }
   ],
   "source": [
    "for name, param in model.named_parameters():\n",
    "    if param.requires_grad:\n",
    "        print(name, param.data.size())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predict new blade"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start predicting\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/carlovoss/jupyter_notebooks/masterarbeit/venv_pm/lib/python3.6/site-packages/torch/nn/functional.py:1374: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n",
      "  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished\n"
     ]
    }
   ],
   "source": [
    "# Initiate and load model\n",
    "model = LstmMseDropout(batch_size=hyperparam['model']['batch_size'], input_dim=hyperparam['model']['input_size'], \n",
    "             n_hidden=hyperparam['model']['n_hidden'], n_layers=hyperparam['model']['lstm_layer'], dropout_rate=0.2)\n",
    "\n",
    "PATH = hyperparam[\"model\"][\"path\"]\n",
    "checkpoint = torch.load(PATH)\n",
    "model.load_state_dict(checkpoint['model_state_dict'])\n",
    "model.eval()\n",
    "\n",
    "# Create empty dataframe\n",
    "columns = hyperparam[\"data\"][\"columns\"]\n",
    "\n",
    "df = pd.DataFrame(columns=columns)\n",
    "\n",
    "print(\"Start predicting\")    \n",
    "##### Predict #####\n",
    "for batch_number, data in enumerate(data_loader_new_blade):\n",
    "    \n",
    "    input_data, target_data = data\n",
    "    hidden = model.init_hidden()\n",
    "\n",
    "    # Forward propagation\n",
    "    output = model(input_data, hidden)\n",
    "    \n",
    "    # Calculate loss\n",
    "    criterion = LossModuleMse(hyperparam[\"model\"][\"input_size\"], hyperparam[\"model\"][\"batch_size\"])\n",
    "    loss = criterion(output, target_data)\n",
    "    \n",
    "    # Add values to dataframe \n",
    "    output = torch.squeeze(output)\n",
    "    target_data = torch.squeeze(target_data)\n",
    "    target_data_np = target_data.data.numpy().tolist()\n",
    "    predicted_data_np = output.data.numpy().tolist()\n",
    "    data = [batch_number] + target_data_np + predicted_data_np + [loss.item()]\n",
    "    df = df.append(pd.Series(data, index=df.columns ), ignore_index=True)\n",
    "\n",
    "# Save dataframe as csv file\n",
    "df.to_csv(hyperparam[\"filed_location\"][\"prediction_new\"], sep=\";\", index=False)\n",
    "\n",
    "print(\"Finished\")  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predict worn blade"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start predicting\n",
      "Finished\n"
     ]
    }
   ],
   "source": [
    "# Initiate and load model\n",
    "model = LstmMseDropout(batch_size=hyperparam['model']['batch_size'], input_dim=hyperparam['model']['input_size'], \n",
    "             n_hidden=hyperparam['model']['n_hidden'], n_layers=hyperparam['model']['lstm_layer'], dropout_rate=0.2)\n",
    "\n",
    "PATH = hyperparam[\"model\"][\"path\"]\n",
    "checkpoint = torch.load(PATH)\n",
    "model.load_state_dict(checkpoint['model_state_dict'])\n",
    "model.eval()\n",
    "\n",
    "# Create empty dataframe\n",
    "columns = hyperparam[\"data\"][\"columns\"]\n",
    "df = pd.DataFrame(columns=columns)\n",
    "\n",
    "print(\"Start predicting\")    \n",
    "##### Predict #####\n",
    "for batch_number, data in enumerate(data_loader_worn_blade):\n",
    "    \n",
    "    input_data, target_data = data\n",
    "    hidden = model.init_hidden()\n",
    "\n",
    "    # Forward propagation\n",
    "    output = model(input_data, hidden)\n",
    "    \n",
    "    # Calculate loss\n",
    "    criterion = LossModuleMse(hyperparam[\"model\"][\"input_size\"], hyperparam[\"model\"][\"batch_size\"])\n",
    "    loss = criterion(output, target_data)\n",
    "    \n",
    "    # Add values to dataframe \n",
    "    output = torch.squeeze(output)\n",
    "    target_data = torch.squeeze(target_data)\n",
    "    target_data_np = target_data.data.numpy().tolist()\n",
    "    predicted_data_np = output.data.numpy().tolist()\n",
    "    data = [batch_number] + target_data_np + predicted_data_np + [loss.item()]\n",
    "    df = df.append(pd.Series(data, index=df.columns ), ignore_index=True)\n",
    "\n",
    "# Save dataframe as csv file\n",
    "df.to_csv(hyperparam[\"filed_location\"][\"prediction_worn\"], sep=\";\", index=False)\n",
    "\n",
    "print(\"Finished\") "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compute Residuals for New Blade"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_new_blade = pd.read_csv(hyperparam[\"filed_location\"][\"prediction_new\"], sep=\";\")\n",
    "\n",
    "# Create empty dataframe\n",
    "columns_residual = [\"timestamp\", \"cut_torque_residuas\", \"cut_lag_error_residual\", \"cut_position_residual\", \"cut_speed_residual\", \n",
    "           \"film_position_residual\", \"film_speed_residual\", \"film_lag_error_residual\"]\n",
    "columns_original = data_new_blade.columns\n",
    "df = pd.DataFrame(columns=columns)\n",
    "\n",
    "# Compute Residuals\n",
    "for i in range(1,8):\n",
    "    df[columns_residual[i]] = data_new_blade[columns_original[i]] - data_new_blade[columns_original[i+7]]\n",
    "\n",
    "df[\"timestamp\"]=data_new_blade[\"timestamp\"]\n",
    "\n",
    "# Save dataframe as csv file\n",
    "df.to_csv(hyperparam[\"filed_location\"][\"residual_new\"], sep=\";\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compute Residuals for New Blade"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_new_blade = pd.read_csv(hyperparam[\"filed_location\"][\"prediction_worn\"], sep=\";\")\n",
    "\n",
    "# Create empty dataframe\n",
    "columns_residual = [\"timestamp\", \"cut_torque_residuas\", \"cut_lag_error_residual\", \"cut_position_residual\", \"cut_speed_residual\", \n",
    "           \"film_position_residual\", \"film_speed_residual\", \"film_lag_error_residual\"]\n",
    "columns_original = data_new_blade.columns\n",
    "df = pd.DataFrame(columns=columns)\n",
    "\n",
    "# Compute Residuals\n",
    "for i in range(1,8):\n",
    "    df[columns_residual[i]] = data_new_blade[columns_original[i]] - data_new_blade[columns_original[i+7]]\n",
    "\n",
    "df[\"timestamp\"]=data_new_blade[\"timestamp\"]\n",
    "\n",
    "# Save dataframe as csv file\n",
    "df.to_csv(hyperparam[\"filed_location\"][\"residual_worn\"], sep=\";\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "masterarbeit",
   "language": "python",
   "name": "masterarbeit"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
