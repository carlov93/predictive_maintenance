{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.nn.utils import clip_grad_norm_\n",
    "from torch.autograd import Variable\n",
    "from torchsummary import summary\n",
    "import pandas as pd\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import matplotlib.pyplot as plt\n",
    "import torch.optim as optim\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from keras.preprocessing.sequence import TimeseriesGenerator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "hyperparam = {\n",
    "    \"data\" : {\n",
    "        \"stake_training_data\" : 0.75, \n",
    "        \"path\" : '../../data/vega_shrinkwrapper_original/'\n",
    "    },\n",
    "    \"model\" : {\n",
    "        \"input_size\" : 7,\n",
    "        \"n_hidden\" : 150,\n",
    "        \"sequence_size\" : 20,\n",
    "        \"batch_size\" : 8,\n",
    "        \"lstm_layer\" : 3,\n",
    "        \"gaussian\" : 1\n",
    "    },\n",
    "    \"cycling_lr\" : {\n",
    "        # step_size is the number of training iterations (total samples/batch_size) per half cycle. \n",
    "        # Authors suggest setting step_size 2-8 x training iterations in epoch.\n",
    "        \"step_size\" : (2048/8)/4, \n",
    "        # Mode can be one of {triangular, triangular2, exp_range}\n",
    "        \"mode\" : \"triangular2\", \n",
    "        \"gamma\" : 0.9995,\n",
    "        \"base_lr\" : 3e-3, \n",
    "        \"max_lr\" :0.1\n",
    "    },\n",
    "    \"training\": {\n",
    "        \"n_epochs\" : 5,\n",
    "        \"patience\" : 50,\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Split train and test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataPreperator():\n",
    "    def __init__(self, path):\n",
    "        self.path = path\n",
    "        \n",
    "    def load_data(self):\n",
    "        return pd.read_csv(self.path)\n",
    "    \n",
    "    def preprocess_data(self, train_data, validation_data):\n",
    "        # Remove time feature\n",
    "        train_data = train_data.drop(labels=\"Timestamp\", axis=1)\n",
    "        validation_data = validation_data.drop(labels=\"Timestamp\", axis=1)\n",
    "        # Initialise standard scaler\n",
    "        scaler = StandardScaler()\n",
    "        scaler.fit(train_data)\n",
    "        # Transform data\n",
    "        train_preprocessed = scaler.transform(train_data)\n",
    "        validation_preprocessed = scaler.transform(validation_data)\n",
    "        return train_preprocessed, validation_preprocessed \n",
    "        \n",
    "    def provide_data(self, stake_training_data):\n",
    "        dataset = self.load_data()\n",
    "        amount_training_data = round(len(dataset)*stake_training_data)\n",
    "        train_data = dataset.iloc[0:amount_training_data,:]\n",
    "        validation_data = dataset.iloc[amount_training_data:,:]\n",
    "        train_preprocessed, validation_preporcessed = self.preprocess_data(train_data, validation_data)\n",
    "        \n",
    "        return train_preprocessed, validation_preporcessed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-5.37536613e-02 -2.53111489e-04 -8.82854465e+05  7.79034183e+02\n",
      "  1.45531178e+04  1.37766733e+03  6.50149764e-01]\n",
      "[1.25303578e-01 1.16898690e-03 2.86060835e+06 1.64515717e+06\n",
      " 6.85728371e+06 3.63196175e+05 8.21463343e-03]\n",
      "(1536, 7)\n"
     ]
    }
   ],
   "source": [
    "train_loader = DataPreperator(path=hyperparam['data']['path']+'NewBlade001.csv')\n",
    "train_data, validation_data = train_loader.provide_data(stake_training_data=hyperparam['data']['stake_training_data'])\n",
    "print(train_data.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Datahandler \n",
    "Time series data must be transformed into a structure of samples with `input` and `target` components before it can be used to fit a supervised learning model. <br>\n",
    "For a time series interested in one-step predictions, the observations at prior time steps, so-called lag observations, are used as `input` and the `target` is the observation at the current time step.\n",
    "\n",
    "For example, a univariate series can be expressed as a supervised learning problem with three time steps for `input` and one step as `target`, as follows:\n",
    "\n",
    "|input|target|\n",
    "|-----|------|\n",
    "[1, 2, 3]|[4]\n",
    "[2, 3, 4]|[5]\n",
    "[3, 4, 5]|[6]\n",
    "\n",
    "The Keras deep learning library provides the `TimeseriesGenerator` to automatically transform both univariate and multivariate time series data into such a format. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Size of multivariate time series\n",
    "Actually the data has a other size than in the table above because of multivariate time series data and because of using batches. <br>\n",
    "__First dimension__: batch size --> Defines the number of samples that will be propagated through the network simultaneously. <br>\n",
    "__Second dimension:__ timesteps --> Number of sequence which is passed into the LSTM <br>\n",
    "__Third dimension:__ input_dim --> Is the number of features. In this case data from 7 sensors, collected at the same time. <br>\n",
    "\n",
    "![](../../knowledge/pictures/input_shape.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataSet(Dataset):\n",
    "    def __init__(self, data, timesteps):\n",
    "        # Data as numpy array is provided\n",
    "        self.data = data\n",
    "        # Data generator is initialized, batch_size=1 is indipendent of neural network's batch_size \n",
    "        self.generator = TimeseriesGenerator(self.data, self.data, length=timesteps, batch_size=1)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        x, y = self.generator[index]\n",
    "        x_torch = torch.from_numpy(x)\n",
    "        # Dimension 0 with size 1 (created by TimeseriesGenerator because of batch_size=1) gets removed \n",
    "        # because DataLoader will add a dimension 0 with size=batch_size as well\n",
    "        x_torch = torch.squeeze(x_torch) # torch.Size([1, timesteps, 7]) --> torch.Size([timesteps, 7])\n",
    "        y_torch = torch.from_numpy(y)\n",
    "        return (x_torch.float(), y_torch.float()) \n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.generator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<__main__.DataProvider object at 0x13eec8c18>\n"
     ]
    }
   ],
   "source": [
    "dataset_train = DataSet(train_data, timesteps=hyperparam[\"model\"][\"sequence_size\"])\n",
    "dataset_validation = DataSet(validation_data, timesteps=hyperparam[\"model\"][\"sequence_size\"])\n",
    "\n",
    "# Data is shuffled because each mini batch is indipendent from each other, but samples of a minibatch are in chronological order\n",
    "data_loader_training = DataLoader(dataset_train, batch_size=hyperparam[\"model\"][\"batch_size\"], \n",
    "                                  num_workers=1, shuffle=True, drop_last=True)\n",
    "data_loader_validation = DataLoader(dataset_validation, batch_size=hyperparam[\"model\"][\"batch_size\"], \n",
    "                                    num_workers=1, shuffle=True, drop_last=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data of batch: 0\n",
      "Size of input data: torch.Size([8, 20, 7])\n",
      "Size of target data: torch.Size([8, 1, 7])\n",
      "Data of batch: 1\n",
      "Size of input data: torch.Size([8, 20, 7])\n",
      "Size of target data: torch.Size([8, 1, 7])\n"
     ]
    }
   ],
   "source": [
    "for batch_idx, data in enumerate(data_loader_training):\n",
    "    x,y = data\n",
    "    print('Data of batch: {}'.format(batch_idx))\n",
    "    print(\"Size of input data: {}\".format(x.size()))\n",
    "    print(\"Size of target data: {}\".format(y.size()))\n",
    "    if batch_idx >=1: break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define Architecture of Neural Network\n",
    "__Parameters for LSTM Modul:__\n",
    "- input_size : The number of expected features in the input x\n",
    "- hidden_size :The number of features in the hidden state h\n",
    "- num_layers : Number of recurrent layers. E.g., setting num_layers=2 would mean stacking two LSTMs together to form a stacked LSTM, with the second LSTM taking in outputs of the first LSTM and computing the final results.\n",
    "- batch_first : If True, then the input __and output__ tensors are provided as (batch, seq, feature).\n",
    "- dropout â€“ If non-zero, introduces a Dropout layer on the outputs of each LSTM layer except the last layer, with dropout probability equal to dropout. Default: 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTM(nn.Module):\n",
    "    def __init__(self, batch_size, input_dim, n_hidden, n_layers):\n",
    "        super(LSTM, self).__init__()\n",
    "        # Attributes for LSTM Network\n",
    "        self.input_dim = input_dim\n",
    "        self.n_hidden = n_hidden\n",
    "        self.n_layers = n_layers\n",
    "        self.batch_size = batch_size\n",
    "        \n",
    "        # Attribut for Gaussians\n",
    "        self.n_gaus_param = hyperparm[\"model\"][\"gaussian\"]\n",
    "        \n",
    "        # Definition of NN layer\n",
    "        # batch_first = True because dataloader creates batches and batch_size is 0. dimension\n",
    "        self.lstm = nn.LSTM(input_size = self.input_dim, hidden_size = self.n_hidden, num_layers = self.n_layers, batch_first = True)\n",
    "        self.fc1 = nn.Linear(self.n_hidden, self.n_gaus_param * self.input_dim)\n",
    "        \n",
    "    def forward(self, input_data, hidden):\n",
    "        # Forward propagate LSTM\n",
    "        # LSTM in Pytorch return two results the first one usually called output and the second one (hidden_state, cell_state). \n",
    "        # As output the LSTM returns all the hidden_states for all the timesteps (seq), in other words all of the hidden states throughout\n",
    "        # the sequence\n",
    "        # As hidden_state the LSTM returns just the most recent hidden state\n",
    "        lstm_out, (hidden_state, cell_state) = self.lstm(input_data, hidden)\n",
    "        # Length of input data can varry \n",
    "        length_seq = input_data.size()[1]\n",
    "        # Select the output from the last sequence \n",
    "        last_out = lstm_out[:,length_seq-1,:]\n",
    "        out = self.fc1(last_out)\n",
    "        # Reshape out to shape torch.Size(batch_size, n_features, 2)\n",
    "        raw_output = out.view(self.batch_size, self.input_dim, 2)\n",
    "        # y_hat and tau alternate, y_hat and tau are next to each other for each feature \n",
    "        y_hat = raw_output[:,:,0]\n",
    "        tau = raw_output[:,:,1]\n",
    "        #Ïƒ = exp(Ï„) guarantees Ïƒ > 0 and provides numerical stability in the learning process\n",
    "        sigma = torch.exp(tau)\n",
    "        return y_hat, sigma\n",
    "    \n",
    "    def init_hidden(self):\n",
    "        # This method is for initializing hidden state as well as cell state\n",
    "        # We need to detach the hidden state to prevent exploding/vanishing gradients\n",
    "        # The axes semantics are (num_layers, minibatch_size, hidden_dim)\n",
    "        h0 = torch.zeros(self.n_layers, self.batch_size, self.n_hidden, requires_grad=False)\n",
    "        c0 = torch.zeros(self.n_layers, self.batch_size, self.n_hidden, requires_grad=False)\n",
    "        return [t for t in (h0, c0)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialize Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = LSTM(batch_size=hyperparam['model']['batch_size'], input_dim=hyperparam['model']['input_size'], \n",
    "             n_hidden=hyperparam['model']['n_hidden'], n_layers=hyperparam['model']['lstm_layer'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cyclic Learning Rate\n",
    "CLR is used to enhance the way the learning rate is scheduled during training, to provide better convergence and help in regularizing deep learning models. It eliminates the need to experimentally find the best values for the global learning rate. Allowing the learning rate to cyclically vary between lower and upper boundary values. The idea is to divide the training process into cycles determined by a stepsize parameter, which defines the number of iterations in half a cycle.\n",
    "![](../../knowledge/pictures/scheduler.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Find base_lr and max_lr\n",
    "Calculate the upper bound of the learning rate for your model. The way to do this is to:\n",
    "1. Define an initial learning rate, the lower boundary of the range you want to test (letâ€™s say 1e-7)\n",
    "2. Define an upper boundary of the range (letâ€™s say 0.1)\n",
    "3. Define an exponential scheme to run through this step by step: <br>\n",
    "```python\n",
    "lr_lambda = lambda x: math.exp(x * math.log(end_lr / start_lr) / (lr_find_epochs * len( data_loader[\"train\"])))\n",
    "scheduler = torch.optim.lr_scheduler.LambdaLR(optimizer, lr_lambda)\n",
    "```\n",
    "<br>\n",
    "__Script for this is in: __  <br>\n",
    "`find_base_and_max_lr.ipynb`\n",
    "\n",
    "- A good upper bound (max_lr) is not on the lowest point, but about a factor of 10 to the left. (In this case 3e-3)\n",
    "- A good lower bound (base_lr), according to the paper and other sources, is the upper bound, divided by a factor 6.\n",
    "\n",
    "### 2. Implement cyclic learning rate with founded parameters\n",
    "\n",
    "## Initialize optimizer and Cyclic Learning Rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lr=1. because of scheduler (1*learning_rate_schedular)\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=5.)  \n",
    "\n",
    "scheduler = torch.optim.lr_scheduler.CyclicLR(optimizer=optimizer, base_lr=hyperparam['cycling_lr']['base_lr'], \n",
    "                                              max_lr=hyperparam['cycling_lr']['max_lr'], step_size_up=hyperparam['cycling_lr']['step_size'], \n",
    "                                              mode=hyperparam['cycling_lr']['mode'], gamma=hyperparam['cycling_lr']['gamma'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define Loss function\n",
    "\n",
    "- torch.exp(input) --> Returns a new tensor with the exponential of the elements of the input tensor : $y_i = e^{x_i}$\n",
    "- normal_distribution = torch.distributions.Normal(loc=, scale=) --> Create a normal (Gaussian) distribution parameterized by loc (mean) and scale (standard deviation).\n",
    "- normal_distribution.log_prob(value) --> Returns the log of the probability density function (in this case of normal distribution) evaluated at value.\n",
    "- torch.sum(input=, dim=) --> Returns the sum of all elements in the input tensor; dim is the dimension to reduce \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loss function Prof. Niggemann"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss_function(output, target_data):\n",
    "    y_hat, sigma = output\n",
    "    # target_data.size(batch_size,1,7) because timestep = 1\n",
    "    # new target_data.size(batch_size,7)\n",
    "    target_data = torch.squeeze(target_data)\n",
    "    \n",
    "    term = ((target_data-y_hat)/sigma).pow(2) + 2*torch.log(sigma)\n",
    "    loss_batches = torch.sum(input=term, dim=1)\n",
    "    \n",
    "    # The value being returned by a loss function MUST BE a scalar value. Not a vector/tensor.\n",
    "    mean_loss = torch.sum(loss_batches)/hyperparam['model']['batch_size']\n",
    "    \n",
    "    # The value being returned must be a Variable. This is so that it can be used to update the parameters. \n",
    "    # A Variable is tracking the operations being done on it so that it can backpropagate to get the gradient.\n",
    "    return Variable(mean_loss, requires_grad=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loss function MSE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Loop\n",
    "An epoch consists of a learning cycle over all batches of training data and an evaluation of the most recent model with the testing data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start model training\n",
      "12.079024314880371\n",
      "12.087864875793457\n",
      "11.510387420654297\n",
      "8.481731414794922\n",
      "6.671802520751953\n",
      "6.226179599761963\n",
      "6.192843914031982\n",
      "6.159883499145508\n",
      "7.386605262756348\n",
      "6.6125006675720215\n",
      "7.362203598022461\n",
      "5.786321640014648\n",
      "5.233246803283691\n",
      "4.625585079193115\n",
      "4.492910385131836\n",
      "3.853297233581543\n",
      "3.744445323944092\n",
      "3.8679301738739014\n",
      "3.813821792602539\n",
      "3.9062719345092773\n",
      "3.6826629638671875\n",
      "3.704373598098755\n",
      "3.675485372543335\n",
      "3.673600912094116\n",
      "3.714611291885376\n",
      "3.7475595474243164\n",
      "3.787816286087036\n",
      "3.705082654953003\n",
      "3.659180164337158\n",
      "3.6409966945648193\n",
      "3.6579365730285645\n",
      "3.6035008430480957\n",
      "3.6083884239196777\n",
      "3.7473623752593994\n",
      "3.619199275970459\n",
      "3.708082437515259\n",
      "3.639326572418213\n",
      "3.747343063354492\n",
      "4.296777248382568\n",
      "4.690136432647705\n",
      "5.2683186531066895\n",
      "5.670783996582031\n",
      "5.894678592681885\n",
      "6.1887969970703125\n",
      "6.148601531982422\n",
      "6.255386829376221\n",
      "6.433719158172607\n",
      "6.633268356323242\n",
      "7.021653175354004\n",
      "7.562901973724365\n",
      "7.9708147048950195\n",
      "7.459197521209717\n",
      "6.988081932067871\n",
      "7.07415246963501\n",
      "7.040964603424072\n",
      "7.131873607635498\n",
      "7.182490825653076\n",
      "7.22646951675415\n",
      "7.097586631774902\n",
      "7.65023136138916\n",
      "7.160261631011963\n",
      "7.933635711669922\n",
      "9.487971305847168\n",
      "8.700249671936035\n",
      "9.545829772949219\n",
      "8.68794059753418\n",
      "8.858139991760254\n",
      "9.439567565917969\n",
      "9.220812797546387\n",
      "8.300493240356445\n",
      "7.260474681854248\n",
      "6.632235050201416\n",
      "6.261312961578369\n",
      "6.575664520263672\n",
      "6.395598411560059\n",
      "5.485024452209473\n",
      "4.661965370178223\n",
      "4.54384708404541\n",
      "4.397060871124268\n",
      "4.4410400390625\n",
      "4.0186567306518555\n",
      "4.505235195159912\n",
      "5.094252109527588\n",
      "4.174870491027832\n",
      "4.231082439422607\n",
      "4.2481369972229\n",
      "5.637520790100098\n",
      "5.216441631317139\n",
      "5.5162353515625\n",
      "6.250248908996582\n",
      "6.471773624420166\n",
      "6.142293930053711\n",
      "6.614624500274658\n",
      "7.386223793029785\n",
      "8.520745277404785\n",
      "7.871740818023682\n",
      "9.396478652954102\n",
      "9.840242385864258\n",
      "10.154020309448242\n",
      "8.12548828125\n",
      "8.796814918518066\n",
      "26.034847259521484\n",
      "83.11595153808594\n",
      "13.88045883178711\n",
      "8.46920108795166\n",
      "13.994980812072754\n",
      "5.101722717285156\n",
      "8.059181213378906\n",
      "6.7911376953125\n",
      "10.392021179199219\n",
      "4.809101104736328\n",
      "5.070477485656738\n",
      "2.8898284435272217\n",
      "3.227853775024414\n",
      "2.9344701766967773\n",
      "2.5945491790771484\n",
      "1.9011006355285645\n",
      "2.325927734375\n",
      "1.754257321357727\n",
      "2.091843366622925\n",
      "1.3363760709762573\n",
      "1.140654444694519\n",
      "1.690300464630127\n",
      "0.819692075252533\n",
      "1.0671950578689575\n",
      "0.9921406507492065\n",
      "0.8534781336784363\n",
      "0.6804972887039185\n",
      "0.6318610906600952\n",
      "0.23952145874500275\n",
      "0.3906046748161316\n",
      "0.3247146010398865\n",
      "0.22710052132606506\n",
      "0.36857250332832336\n",
      "0.267666757106781\n",
      "0.4766075015068054\n",
      "0.5063287019729614\n",
      "0.5281676054000854\n",
      "0.47959089279174805\n",
      "0.5089619755744934\n",
      "0.6301696300506592\n",
      "0.7804517149925232\n",
      "1.211754560470581\n",
      "2.2671353816986084\n",
      "3.183872699737549\n",
      "3.493353843688965\n",
      "3.6803133487701416\n",
      "4.125345230102539\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/Users/carlovoss/anaconda3/lib/python3.6/multiprocessing/queues.py\", line 240, in _feed\n",
      "    send_bytes(obj)\n",
      "  File \"/Users/carlovoss/anaconda3/lib/python3.6/multiprocessing/connection.py\", line 200, in send_bytes\n",
      "    self._send_bytes(m[offset:offset + size])\n",
      "  File \"/Users/carlovoss/anaconda3/lib/python3.6/multiprocessing/connection.py\", line 404, in _send_bytes\n",
      "    self._send(header + buf)\n",
      "  File \"/Users/carlovoss/anaconda3/lib/python3.6/multiprocessing/connection.py\", line 368, in _send\n",
      "    n = write(self._handle, buf)\n",
      "BrokenPipeError: [Errno 32] Broken pipe\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-107-48f6b181b368>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     34\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m         \u001b[0;31m# Forward propagation\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 36\u001b[0;31m         \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhidden\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     37\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m         \u001b[0;31m# Calculate loss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/jupyter_notebooks/masterarbeit/venv_pm/lib/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    491\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    492\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 493\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    494\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    495\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-57-769a8f71ae70>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_data, hidden)\u001b[0m\n\u001b[1;32m     22\u001b[0m         \u001b[0;31m# the sequence\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m         \u001b[0;31m# As hidden_state the LSTM returns just the most recent hidden state\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 24\u001b[0;31m         \u001b[0mlstm_out\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mhidden_state\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcell_state\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlstm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhidden\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     25\u001b[0m         \u001b[0;31m# Length of input data can varry\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m         \u001b[0mlength_seq\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minput_data\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/jupyter_notebooks/masterarbeit/venv_pm/lib/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    491\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    492\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 493\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    494\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    495\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/jupyter_notebooks/masterarbeit/venv_pm/lib/python3.6/site-packages/torch/nn/modules/rnn.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input, hx)\u001b[0m\n\u001b[1;32m    557\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward_packed\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    558\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 559\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    560\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    561\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/jupyter_notebooks/masterarbeit/venv_pm/lib/python3.6/site-packages/torch/nn/modules/rnn.py\u001b[0m in \u001b[0;36mforward_tensor\u001b[0;34m(self, input, hx)\u001b[0m\n\u001b[1;32m    537\u001b[0m         \u001b[0munsorted_indices\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    538\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 539\u001b[0;31m         \u001b[0moutput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhidden\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_sizes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_batch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msorted_indices\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    540\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    541\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0moutput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpermute_hidden\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhidden\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0munsorted_indices\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/jupyter_notebooks/masterarbeit/venv_pm/lib/python3.6/site-packages/torch/nn/modules/rnn.py\u001b[0m in \u001b[0;36mforward_impl\u001b[0;34m(self, input, hx, batch_sizes, max_batch_size, sorted_indices)\u001b[0m\n\u001b[1;32m    520\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mbatch_sizes\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    521\u001b[0m             result = _VF.lstm(input, hx, self._get_flat_weights(), self.bias, self.num_layers,\n\u001b[0;32m--> 522\u001b[0;31m                               self.dropout, self.training, self.bidirectional, self.batch_first)\n\u001b[0m\u001b[1;32m    523\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    524\u001b[0m             result = _VF.lstm(input, batch_sizes, hx, self._get_flat_weights(), self.bias,\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "print(\"Start model training\")\n",
    "# Create lists to save training loss and validation loss of each epoch\n",
    "hist_training_loss = []\n",
    "hist_validation_loss = []\n",
    "lr_find_lr = []\n",
    "epoch_training_loss = []\n",
    "epoch_validation_loss = []\n",
    "\n",
    "# Set first comparative value\n",
    "lowest_loss = 99\n",
    "trials = 0\n",
    "\n",
    "for epoch in range(hyperparam[\"training\"][\"n_epochs\"]+1):\n",
    "    # Empty list for recording performance \n",
    "    epoch_training_loss = []\n",
    "    epoch_validation_loss = []\n",
    "    \n",
    "    # Set model to training mode before train the neural network.\n",
    "    checkpoint = torch.load(\"../../models/best_model.pt\")\n",
    "    model.load_state_dict(checkpoint['model_state_dict'])\n",
    "    optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "    model.train()\n",
    "    \n",
    "    ##### Training #####\n",
    "    for batch_number, (input_data, target_data) in enumerate(data_loader_training):\n",
    "        # The LSTM has to be reinitialised, otherwise the LSTM will treat a new batch as a continuation of a sequence.\n",
    "        # When batches of data are independent sequences, then you should reinitialise the hidden state before each batch. \n",
    "        # But if your data is made up of really long sequences and you cut it up into batches making sure that each batch \n",
    "        # follows on from the previous batch, then in that case you wouldnâ€™t reinitialise the hidden state before each batch.\n",
    "        # In the current workflow of class DataProvoider independent sequences are returned. \n",
    "        hidden = model.init_hidden()\n",
    "        \n",
    "        # Zero out gradient, else they will accumulate between minibatches\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # Forward propagation\n",
    "        output = model(input_data, hidden)\n",
    "         \n",
    "        # Calculate loss\n",
    "        loss = loss_function(output, target_data)\n",
    "        \n",
    "        print(loss.item())\n",
    "        epoch_training_loss.append(loss.item())\n",
    "    \n",
    "        # Backward pass\n",
    "        loss.backward()\n",
    "        \n",
    "        # Update parameters\n",
    "        optimizer.step()\n",
    "        \n",
    "        # Update LR\n",
    "        #scheduler.step()\n",
    "        lr_step = optimizer.state_dict()[\"param_groups\"][0][\"lr\"]\n",
    "        lr_find_lr.append(lr_step)\n",
    "    \n",
    "    # Save mean of loss over all training iterations\n",
    "    mean_epoch_training_loss = sum(epoch_training_loss) / float(len(epoch_training_loss))\n",
    "    hist_training_loss.append(mean_epoch_training_loss)\n",
    "    \n",
    "    print(\"-------- epoch_no. {} finished with training loss {}--------\".format(epoch, mean_epoch_training_loss))\n",
    "        \n",
    "    ##### Evaluation #######\n",
    "    for input_data, target_data in data_loader_validation:\n",
    "        # Change model to evaluation (prediction) mode\n",
    "        model.eval()\n",
    "        hidden = model.init_hidden()\n",
    "        \n",
    "        # Predict \n",
    "        out = model(input_data, hidden)\n",
    "        \n",
    "         # Calculate loss\n",
    "        loss = loss_function(output, target_data)\n",
    "        epoch_validation_loss.append(loss.item())\n",
    "        \n",
    "    # Save mean of loss over all validation iterations to epoch history  \n",
    "    mean_epoch_validation_loss = sum(epoch_validation_loss) / float(len(epoch_validation_loss))\n",
    "    hist_validation_loss.append(mean_epoch_validation_loss)\n",
    "    \n",
    "    print(\"-------- epoch_no. {} finished with eval loss {}--------\".format(epoch, mean_epoch_validation_loss))\n",
    "        \n",
    "    # Check after every evaluation whether the latest model is the best one or not\n",
    "    # If this is the case, set current score to best_score, reset trails and save the model.\n",
    "    if mean_epoch_validation_loss < lowest_loss:\n",
    "        trials = 0\n",
    "        lowest_loss = mean_epoch_validation_loss\n",
    "        torch.save({\n",
    "            'epoch': epoch,\n",
    "            'model_state_dict': model.state_dict(),\n",
    "            'optimizer_state_dict': optimizer.state_dict(),\n",
    "            'loss': mean_epoch_validation_loss\n",
    "        }, \"../../models/best_model.pt\")\n",
    "        print(\"Epoch {}: best model saved with loss: {}\".format(epoch, mean_epoch_validation_loss))\n",
    "    \n",
    "    # Else: Increase trails by one and start new epoch as long as not too many epochs \n",
    "    # were unsuccessful (controlled by patience)\n",
    "    else:\n",
    "        trials += 1\n",
    "        if trials >= hyperparam['training']['patience'] :\n",
    "            print(f'Early stopping on epoch {epoch}')\n",
    "            break  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>y</th>\n",
       "      <th>x</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.00003</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.00003</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.00003</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.00003</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.00003</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         y  x\n",
       "0  0.00003  0\n",
       "1  0.00003  1\n",
       "2  0.00003  2\n",
       "3  0.00003  3\n",
       "4  0.00003  4"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "x = range(len(lr_find_lr))\n",
    "data = pd.DataFrame(data={'y': lr_find_lr, 'x': x})\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 504x504 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "f, ax = plt.subplots(figsize=(7, 7))\n",
    "#ax.set(xscale=\"log\")\n",
    "sns.scatterplot(x=data.x, y=data.y, ax=ax)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1080x1080 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "epoch_no = range(len(epoch_training_loss))\n",
    "loss = pd.DataFrame(data={'y': epoch_training_loss, 'x': epoch_no})\n",
    "f, ax = plt.subplots(figsize=(15, 15))\n",
    "sns.scatterplot(x=loss.x, y=loss.y, ax=ax)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "masterarbeit",
   "language": "python",
   "name": "masterarbeit"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
