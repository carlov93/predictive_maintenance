{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.nn.utils import clip_grad_norm_\n",
    "from torch.autograd import Variable\n",
    "from torchsummary import summary\n",
    "import pandas as pd\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import matplotlib.pyplot as plt\n",
    "import torch.optim as optim\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from keras.preprocessing.sequence import TimeseriesGenerator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test TimeSeriesGenerator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataProvider(Dataset):\n",
    "    def __init__(self, data, timesteps):\n",
    "        # Data as numpy array is provided\n",
    "        self.data = data\n",
    "        # Data generator is initialized, batch_size=1 is indipendent of neural network's batch_size \n",
    "        self.generator = TimeseriesGenerator(self.data, self.data, length=timesteps, batch_size=1)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        x, y = self.generator[index]\n",
    "        x_torch = torch.from_numpy(x)\n",
    "        # Dimension 0 with size 1 (created by TimeseriesGenerator because of batch_size=1) gets removed \n",
    "        # because DataLoader will add a dimension 0 with size=batch_size as well\n",
    "        x_torch = torch.squeeze(x_torch) # torch.Size([1, timesteps, 7]) --> torch.Size([timesteps, 7])\n",
    "        y_torch = torch.from_numpy(y)\n",
    "        return (x_torch.float(), y_torch.float()) \n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.generator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5, 6)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = np.array([[1,2,3,4,5,6],\n",
    "                 [7,8,9,10,11,12],\n",
    "                  [13,14,15,16,17,18],\n",
    "                  [19,20,21,22,23,24],\n",
    "                  [25,26,27,28,29,30]])\n",
    "\n",
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "`start_index+length=3 > end_index=1` is disallowed, as no part of the sequence would be left to be used as current step.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-35-fb51aff9f968>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Data is shuffled because each mini batch is indipendent from each other, but samples of a minibatch are in chronological order\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mdataset_train\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mDataProvider\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimesteps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mdata_loader_training\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mDataLoader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_workers\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshuffle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdrop_last\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-11-f67fedbdffce>\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, data, timesteps)\u001b[0m\n\u001b[1;32m      4\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m         \u001b[0;31m# Data generator is initialized, batch_size=1 is indipendent of neural network's batch_size\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgenerator\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTimeseriesGenerator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlength\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtimesteps\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__getitem__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindex\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/jupyter_notebooks/masterarbeit/venv_pm/lib/python3.6/site-packages/keras_preprocessing/sequence.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, data, targets, length, sampling_rate, stride, start_index, end_index, shuffle, reverse, batch_size)\u001b[0m\n\u001b[1;32m    353\u001b[0m                              \u001b[0;34m'is disallowed, as no part of the sequence '\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    354\u001b[0m                              \u001b[0;34m'would be left to be used as current step.'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 355\u001b[0;31m                              % (self.start_index, self.end_index))\n\u001b[0m\u001b[1;32m    356\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    357\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__len__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: `start_index+length=3 > end_index=1` is disallowed, as no part of the sequence would be left to be used as current step."
     ]
    }
   ],
   "source": [
    "# Data is shuffled because each mini batch is indipendent from each other, but samples of a minibatch are in chronological order\n",
    "dataset_train = DataProvider(data, timesteps=3)\n",
    "data_loader_training = DataLoader(dataset_train, batch_size=1, num_workers=1, shuffle=False, drop_last=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[ 1.,  2.,  3.,  4.,  5.,  6.],\n",
      "         [ 7.,  8.,  9., 10., 11., 12.]]])\n",
      "------\n",
      "tensor([[[13., 14., 15., 16., 17., 18.]]])\n",
      "------\n",
      "tensor([[[ 7.,  8.,  9., 10., 11., 12.],\n",
      "         [13., 14., 15., 16., 17., 18.]]])\n",
      "------\n",
      "tensor([[[19., 20., 21., 22., 23., 24.]]])\n",
      "------\n",
      "tensor([[[13., 14., 15., 16., 17., 18.],\n",
      "         [19., 20., 21., 22., 23., 24.]]])\n",
      "------\n",
      "tensor([[[25., 26., 27., 28., 29., 30.]]])\n",
      "------\n"
     ]
    }
   ],
   "source": [
    "for batch_idx, data in enumerate(data_loader_training):\n",
    "    x,y = data\n",
    "    print(x)\n",
    "    print(\"------\")\n",
    "    print(y)\n",
    "    print(\"------\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss_function(output, target_data):\n",
    "    y_hat, sigma = output\n",
    "    # target_data.size(batch_size,1,7) because timestep = 1\n",
    "    # new target_data.size(batch_size,7)\n",
    "    target_data = torch.squeeze(target_data)\n",
    "    \n",
    "    term = ((target_data-y_hat)/sigma).pow(2) + 2*torch.log(sigma)\n",
    "    print(term)\n",
    "    loss_batches = torch.sum(input=term, dim=1)\n",
    "    \n",
    "    # The value being returned by a loss function MUST BE a scalar value. Not a vector/tensor.\n",
    "    mean_loss = torch.sum(loss_batches)\n",
    "    \n",
    "    # The value being returned must be a Variable. This is so that it can be used to update the parameters. \n",
    "    # A Variable is tracking the operations being done on it so that it can backpropagate to get the gradient.\n",
    "    return mean_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_hat = torch.tensor([[0.4, 0.6]])\n",
    "sigma = torch.tensor([[0.3, 0.2]])\n",
    "output = y_hat, sigma\n",
    "target_data = torch.tensor([[[0.5, 0.4]]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "y_hat: torch.Size([1, 2])\n",
      "sigma: torch.Size([1, 2])\n",
      "target_data: torch.Size([1, 1, 2])\n"
     ]
    }
   ],
   "source": [
    "print(\"y_hat: {}\".format(y_hat.size()))\n",
    "print(\"sigma: {}\".format(sigma.size()))\n",
    "print(\"target_data: {}\".format(target_data.size()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-2.2968, -2.2189]])\n",
      "tensor(-4.5157)\n"
     ]
    }
   ],
   "source": [
    "loss=loss_function(output, target_data)\n",
    "print(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def my_loss(output, target):\n",
    "    loss = torch.mean((output - target)**2)\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(2.6438, grad_fn=<MeanBackward0>)\n"
     ]
    }
   ],
   "source": [
    "model = nn.Linear(2, 2)\n",
    "x = torch.randn(1, 2)\n",
    "target = torch.randn(1, 2)\n",
    "output = model(x)\n",
    "loss = my_loss(output, target)\n",
    "print(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(2.6438, grad_fn=<MeanBackward0>)\n"
     ]
    }
   ],
   "source": [
    "loss.backward()\n",
    "print(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-4.2110, -0.6090],\n",
      "        [ 2.2242,  0.3216]])\n"
     ]
    }
   ],
   "source": [
    "print(model.weight.grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1., 1.],\n",
      "        [1., 1.]], requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "a = Variable(torch.ones(2, 2), requires_grad=True)\n",
    "print(a)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test shape of LSTM output layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_data(stake_training_data, path):\n",
    "    dataset = pd.read_csv(path)\n",
    "    amount_training_data = round(len(dataset)*stake_training_data)\n",
    "    train_data = dataset.iloc[0:amount_training_data,:]\n",
    "    validation_data = dataset.iloc[amount_training_data:,:]\n",
    "    return train_data, validation_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_test_data(path):\n",
    "    dataset = pd.read_csv(path)\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data = load_test_data(path='../../data/vega_shrinkwrapper_original/WornBlade001.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Timestamp</th>\n",
       "      <th>pCut Motor: Torque</th>\n",
       "      <th>pCut CTRL Position controller: Lag error</th>\n",
       "      <th>pCut CTRL Position controller: Actual position</th>\n",
       "      <th>pCut CTRL Position controller: Actual speed</th>\n",
       "      <th>pSvolFilm CTRL Position controller: Actual position</th>\n",
       "      <th>pSvolFilm CTRL Position controller: Actual speed</th>\n",
       "      <th>pSvolFilm CTRL Position controller: Lag error</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-0.188</td>\n",
       "      <td>-0.112131</td>\n",
       "      <td>-0.002490</td>\n",
       "      <td>-884606</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>11128</td>\n",
       "      <td>2.504289</td>\n",
       "      <td>0.261085</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-0.184</td>\n",
       "      <td>-0.088931</td>\n",
       "      <td>-0.003863</td>\n",
       "      <td>-884606</td>\n",
       "      <td>17.166138</td>\n",
       "      <td>11128</td>\n",
       "      <td>-2.504289</td>\n",
       "      <td>0.260083</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-0.180</td>\n",
       "      <td>-0.115141</td>\n",
       "      <td>0.001630</td>\n",
       "      <td>-884606</td>\n",
       "      <td>-6.866455</td>\n",
       "      <td>11128</td>\n",
       "      <td>7.513016</td>\n",
       "      <td>0.259081</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-0.176</td>\n",
       "      <td>-0.111815</td>\n",
       "      <td>0.003003</td>\n",
       "      <td>-884606</td>\n",
       "      <td>-13.732910</td>\n",
       "      <td>11128</td>\n",
       "      <td>-2.504289</td>\n",
       "      <td>0.260083</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-0.172</td>\n",
       "      <td>-0.130970</td>\n",
       "      <td>0.004376</td>\n",
       "      <td>-884606</td>\n",
       "      <td>-6.866455</td>\n",
       "      <td>11128</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.261085</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Timestamp  pCut Motor: Torque   pCut CTRL Position controller: Lag error  \\\n",
       "0     -0.188           -0.112131                                  -0.002490   \n",
       "1     -0.184           -0.088931                                  -0.003863   \n",
       "2     -0.180           -0.115141                                   0.001630   \n",
       "3     -0.176           -0.111815                                   0.003003   \n",
       "4     -0.172           -0.130970                                   0.004376   \n",
       "\n",
       "    pCut CTRL Position controller: Actual position  \\\n",
       "0                                          -884606   \n",
       "1                                          -884606   \n",
       "2                                          -884606   \n",
       "3                                          -884606   \n",
       "4                                          -884606   \n",
       "\n",
       "    pCut CTRL Position controller: Actual speed  \\\n",
       "0                                      0.000000   \n",
       "1                                     17.166138   \n",
       "2                                     -6.866455   \n",
       "3                                    -13.732910   \n",
       "4                                     -6.866455   \n",
       "\n",
       "    pSvolFilm CTRL Position controller: Actual position  \\\n",
       "0                                              11128      \n",
       "1                                              11128      \n",
       "2                                              11128      \n",
       "3                                              11128      \n",
       "4                                              11128      \n",
       "\n",
       "    pSvolFilm CTRL Position controller: Actual speed  \\\n",
       "0                                           2.504289   \n",
       "1                                          -2.504289   \n",
       "2                                           7.513016   \n",
       "3                                          -2.504289   \n",
       "4                                           0.000000   \n",
       "\n",
       "    pSvolFilm CTRL Position controller: Lag error  \n",
       "0                                        0.261085  \n",
       "1                                        0.260083  \n",
       "2                                        0.259081  \n",
       "3                                        0.260083  \n",
       "4                                        0.261085  "
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data, validation_data = split_data(stake_training_data=0.75, path='../../data/vega_shrinkwrapper_original/NewBlade001.csv')\n",
    "train_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CSVDataset(Dataset):\n",
    "    def __init__(self, data, timesteps):\n",
    "        # All data are loaded from csv file and converted to an numpy array\n",
    "        self.data = data.values\n",
    "        # Data generator is initialized \n",
    "        self.generator = TimeseriesGenerator(self.data, self.data, length=timesteps, batch_size=1)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        x, y = self.generator[index]\n",
    "        x_torch = torch.from_numpy(x)\n",
    "        # Dimension 0 with size 1 (created by TimeseriesGenerator because of batch_size=1) gets removed \n",
    "        # because DataLoader will add a dimension 0 with size=batch_size as well\n",
    "        x_torch = torch.squeeze(x_torch) # torch.Size([1, timesteps, 7]) --> torch.Size([timesteps, 7])\n",
    "        y_torch = torch.from_numpy(y)\n",
    "        return (x_torch.float(), y_torch.float()) \n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.generator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_train = CSVDataset(train_data, timesteps=4)\n",
    "dataset_validation = CSVDataset(validation_data, timesteps=4)\n",
    "dataset_test = CSVDataset(test_data, timesteps=4)\n",
    "\n",
    "data_loader_training = DataLoader(dataset_train, batch_size=32, num_workers=1, shuffle=False)\n",
    "data_loader_validation = DataLoader(dataset_validation, batch_size=32, num_workers=1, shuffle=False)\n",
    "data_loader_test = DataLoader(dataset_test, batch_size=32, num_workers=1, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTM(nn.Module):\n",
    "    def __init__(self, batch_size, input_dim, seq_len, n_hidden=256, n_layers=1):\n",
    "        super(LSTM, self).__init__()\n",
    "        # Attributes for LSTM Network\n",
    "        self.input_dim = input_dim\n",
    "        self.n_hidden = n_hidden\n",
    "        self.n_layers = n_layers\n",
    "        self.batch_size = batch_size\n",
    "        self.seq_len = seq_len\n",
    "        \n",
    "        # Attribut for Gaussians\n",
    "        self.n_gaussians = 2\n",
    "        \n",
    "        # Definition of NN layer\n",
    "        # batch_first = True because dataloader creates batches and batch_size is 0. dimension\n",
    "        self.lstm = nn.LSTM(input_size = self.input_dim, hidden_size = self.n_hidden, num_layers = self.n_layers, batch_first = True)\n",
    "        self.fc1 = nn.Linear(self.n_hidden, self.n_gaussians * self.input_dim)\n",
    "        \n",
    "        \n",
    "    def forward(self, input_data):\n",
    "        # Forward propagate LSTM\n",
    "        # LSTM in Pytorch return two results the first one usually called output and the second one (hidden_state, cell_state). \n",
    "        # As output the LSTM returns all the hidden_states for all the timesteps (seq), in other words all of the hidden states throughout\n",
    "        # the sequence\n",
    "        # As hidden_state the LSTM returns just the most recent hidden state\n",
    "        lstm_out, (hidden_state, cell_state) = self.lstm(input_data)\n",
    "        # Select the output from the last sequence \n",
    "        ln = input_data.size()[1]\n",
    "        print(ln)\n",
    "        last_out = lstm_out[:,ln-1,:]\n",
    "        out = self.fc1(last_out)\n",
    "        # Reshape out to shape torch.Size(batch_size, n_features, 2)\n",
    "        raw_output = out.view(self.batch_size, self.input_dim, 2)\n",
    "        # y_hat and tau alternate, y_hat and tau are next to each other for each feature \n",
    "        y_hat = raw_output[:,:,0]\n",
    "        tau = raw_output[:,:,1]\n",
    "        #σ = exp(τ) guarantees σ > 0 and provides numerical stability in the learning process\n",
    "        sigma = torch.exp(tau)\n",
    "        \n",
    "        return y_hat, sigma\n",
    "    \n",
    "    def get_tensors(self, input_data):\n",
    "        # Forward propagate LSTM\n",
    "        # shape of self.hidden: (h, c), where h and c both have shape (num_layers, batch_size, hidden_dim)\n",
    "        lstm_out, (hidden_state, cell_state) = self.lstm(input_data)\n",
    "        last_out = lstm_out[:,self.seq_len -1,:]\n",
    "        out = self.fc1(last_out)\n",
    "        # Reshape out to shape torch.Size(batch_size, n_features, 2)\n",
    "        raw_output = out.view(self.batch_size, self.input_dim, 2)\n",
    "        y_hat = raw_output[:,:,0]\n",
    "        tau = raw_output[:,:,1]\n",
    "        #σ = exp(τ) guarantees σ > 0 and provides numerical stability in the learning process\n",
    "        sigma = torch.exp(tau)\n",
    "        \n",
    "        return lstm_out, last_out, out, hidden_state, raw_output\n",
    "    \n",
    "    def init_hidden(self):\n",
    "        # This is what we'll initialise our hidden state as well as our cell state\n",
    "        # The axes semantics are (num_layers, minibatch_size, hidden_dim)\n",
    "        h0 = torch.zeros(self.n_layers, self.batch_size, self.n_hidden)\n",
    "        c0 = torch.zeros(self.n_layers, self.batch_size, self.n_hidden)\n",
    "        return [t for t in (h0, c0)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = LSTM(batch_size=32, input_dim=8, seq_len=4, n_hidden=20, n_layers=1)\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_epochs = 1\n",
    "patience = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss_function(output, target_data):\n",
    "    y_hat, sigma = output\n",
    "    # target_data.size(batch_size,1,7) because timestep = 1\n",
    "    # new target_data.size(batch_size,7)\n",
    "    target_data = torch.squeeze(target_data)\n",
    "    \n",
    "    term = ((target_data-y_hat)/sigma)**2 + 2*torch.log(sigma)\n",
    "    return torch.sum(input=term, dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start model training\n",
      "4\n",
      "shape last_out: torch.Size([32, 20])\n",
      "-------- batch_no. 0 --------\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "only one element tensors can be converted to Python scalars",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-42-d7b21fcb005f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     46\u001b[0m         \u001b[0;31m# Calculate loss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     47\u001b[0m         \u001b[0miteration_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mloss_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_data\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 48\u001b[0;31m         \u001b[0mepoch_training_loss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miteration_loss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     49\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     50\u001b[0m         \u001b[0;31m# Backward pass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: only one element tensors can be converted to Python scalars"
     ]
    }
   ],
   "source": [
    "print(\"Start model training\")\n",
    "# Create lists to save training loss and validation loss of each epoch\n",
    "hist_training_loss = []\n",
    "hist_validation_loss = []\n",
    "\n",
    "for epoch in range(1, n_epochs +1):\n",
    "    # Empty list for recording performance \n",
    "    epoch_training_loss = []\n",
    "    epoch_validation_loss = []\n",
    "    \n",
    "    # Also, we need to clear out the hidden state of the LSTM,\n",
    "    # detaching it from its history on the last instance.\n",
    "    hidden = model.init_hidden()\n",
    "    \n",
    "    ##### Training #####\n",
    "    for batch_number, (input_data, target_data) in enumerate(data_loader_training):\n",
    "        # Set model to training mode before train the neural network.\n",
    "        model.train()\n",
    "        \n",
    "        # Zero out gradient, else they will accumulate between minibatches\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # We need to detach the hidden state to prevent exploding/vanishing gradients\n",
    "        hidden = [_.detach() for _ in hidden]\n",
    "        \n",
    "        # Forward propagation\n",
    "        output = model(input_data)\n",
    "        lstm_out, last_out, out, hidden_state, raw_output = model.get_tensors(input_data)\n",
    "        #print(\"shape lstm_out: {}\".format(lstm_out.size()))\n",
    "        print(\"shape last_out: {}\".format(last_out.size()))\n",
    "        #print(\"shape hidden_state: {}\".format(hidden_state.size()))\n",
    "        #print(\"shape out: {}\".format(out.size()))\n",
    "        #print(\"raw_output: {}\".format(raw_output.size()))\n",
    "        #print(\"lstm_out\")\n",
    "        #print(lstm_out)\n",
    "        #print(\"last_out\")\n",
    "        #print(last_out)\n",
    "        #print(\"hidden_state\")\n",
    "        #print(hidden_state)\n",
    "        #print(\"out\")\n",
    "        #print(out)\n",
    "        #print(\"raw_output\")\n",
    "        #print(raw_output)\n",
    "        print(\"-------- batch_no. {} --------\".format(batch_number))\n",
    "        \n",
    "        # Calculate loss\n",
    "        iteration_loss = loss_function(output, target_data)\n",
    "        epoch_training_loss.append(iteration_loss.item())\n",
    "    \n",
    "        # Backward pass\n",
    "        loss.backward()\n",
    "        \n",
    "        # Update LR\n",
    "        scheduler.step()\n",
    "        \n",
    "        # Update parameters\n",
    "        optimizer.step()\n",
    "    \n",
    "    # Save mean of loss over all training iterations\n",
    "    mean_epoch_training_loss = sum(epoch_training_loss) / float(len(epoch_training_loss))\n",
    "    hist_training_loss.append(mean_epoch_training_loss)\n",
    "        \n",
    "    ##### Evaluation #######\n",
    "    for input_data, target_data in data_loader_validation:\n",
    "        # Change model to evaluation (prediction) mode\n",
    "        model.eval()\n",
    "        \n",
    "        # Predict \n",
    "        out = model(input_data)\n",
    "        \n",
    "         # Calculate loss\n",
    "        iteration_loss = loss_function(output, target_data)\n",
    "        epoch_validation_loss.append(iteration_loss.item())\n",
    "        \n",
    "    # Save mean of loss over all validation iterations to epoch history  \n",
    "    mean_epoch_validation_loss = sum(epoch_validation_loss) / float(len(epoch_validation_loss))\n",
    "    hist_validation_loss.append(mean_epoch_validation_loss)\n",
    "        \n",
    "    # Check after every evaluation whether the latest model is the best one or not\n",
    "    # If this is the case, set current score to best_score, reset trails and save the model.\n",
    "    if mean_epoch_validation_loss < lowest_loss:\n",
    "        trials = 0\n",
    "        lowest_loss = mean_epoch_validation_loss\n",
    "        torch.save({\n",
    "            'epoch': epoch,\n",
    "            'model_state_dict': model.state_dict(),\n",
    "            'optimizer_state_dict': optimizer.state_dict(),\n",
    "            'loss': mean_epoch_validation_loss\n",
    "        }, \"/best_model.pt\")\n",
    "        print(\"Epoch {}: best model saved with loss: {}\".format(epoch, mean_epoch_validation_loss))\n",
    "    \n",
    "    # Else: Increase trails by one and start new epoch as long as not too many epochs \n",
    "    # were unsuccessful (controlled by patience)\n",
    "    else:\n",
    "        trials += 1\n",
    "        if trials >= patience:\n",
    "            print(f'Early stopping on epoch {epoch}')\n",
    "            break  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 28747122.0\n",
      "1 24992560.0\n",
      "2 25456364.0\n",
      "3 26002216.0\n",
      "4 24025132.0\n",
      "5 18750016.0\n",
      "6 12396245.0\n",
      "7 7170443.0\n",
      "8 3959234.0\n",
      "9 2227442.5\n",
      "10 1353926.375\n",
      "11 903256.9375\n",
      "12 656832.125\n",
      "13 508778.96875\n",
      "14 410948.75\n",
      "15 340759.125\n",
      "16 287266.4375\n",
      "17 244870.78125\n",
      "18 210316.46875\n",
      "19 181663.09375\n",
      "20 157640.890625\n",
      "21 137377.015625\n",
      "22 120173.703125\n",
      "23 105479.5078125\n",
      "24 92875.4453125\n",
      "25 82014.640625\n",
      "26 72622.0546875\n",
      "27 64475.6796875\n",
      "28 57378.328125\n",
      "29 51179.109375\n",
      "30 45750.671875\n",
      "31 40983.09765625\n",
      "32 36785.42578125\n",
      "33 33081.46484375\n",
      "34 29807.4921875\n",
      "35 26907.1171875\n",
      "36 24327.73046875\n",
      "37 22032.25390625\n",
      "38 19984.181640625\n",
      "39 18154.50390625\n",
      "40 16517.63671875\n",
      "41 15054.314453125\n",
      "42 13741.15234375\n",
      "43 12559.498046875\n",
      "44 11494.630859375\n",
      "45 10533.8994140625\n",
      "46 9664.5986328125\n",
      "47 8877.3798828125\n",
      "48 8163.650390625\n",
      "49 7515.51318359375\n",
      "50 6926.6064453125\n",
      "51 6390.4912109375\n",
      "52 5901.822265625\n",
      "53 5455.99169921875\n",
      "54 5048.515625\n",
      "55 4675.82080078125\n",
      "56 4334.5615234375\n",
      "57 4021.4599609375\n",
      "58 3734.09765625\n",
      "59 3470.0439453125\n",
      "60 3227.272705078125\n",
      "61 3003.911865234375\n",
      "62 2798.0712890625\n",
      "63 2608.311279296875\n",
      "64 2433.09375\n",
      "65 2271.156005859375\n",
      "66 2121.37646484375\n",
      "67 1982.7197265625\n",
      "68 1853.71044921875\n",
      "69 1734.0814208984375\n",
      "70 1623.0770263671875\n",
      "71 1520.2625732421875\n",
      "72 1424.741943359375\n",
      "73 1335.89697265625\n",
      "74 1253.20849609375\n",
      "75 1176.20751953125\n",
      "76 1104.475830078125\n",
      "77 1037.5341796875\n",
      "78 975.0667724609375\n",
      "79 916.767822265625\n",
      "80 862.2606811523438\n",
      "81 811.3619384765625\n",
      "82 763.7293090820312\n",
      "83 719.159912109375\n",
      "84 677.4263916015625\n",
      "85 638.311279296875\n",
      "86 601.6571044921875\n",
      "87 567.2970581054688\n",
      "88 535.0513916015625\n",
      "89 504.7998962402344\n",
      "90 476.40020751953125\n",
      "91 449.72515869140625\n",
      "92 424.651123046875\n",
      "93 401.0771484375\n",
      "94 378.90289306640625\n",
      "95 358.0667419433594\n",
      "96 338.4400939941406\n",
      "97 319.96636962890625\n",
      "98 302.5709533691406\n",
      "99 286.1775207519531\n",
      "100 270.728515625\n",
      "101 256.16705322265625\n",
      "102 242.43893432617188\n",
      "103 229.49708557128906\n",
      "104 217.2823028564453\n",
      "105 205.75428771972656\n",
      "106 194.87115478515625\n",
      "107 184.59335327148438\n",
      "108 174.88905334472656\n",
      "109 165.72340393066406\n",
      "110 157.07040405273438\n",
      "111 148.88189697265625\n",
      "112 141.14077758789062\n",
      "113 133.82177734375\n",
      "114 126.90168762207031\n",
      "115 120.35667419433594\n",
      "116 114.16342163085938\n",
      "117 108.30412292480469\n",
      "118 102.75672149658203\n",
      "119 97.50486755371094\n",
      "120 92.5329360961914\n",
      "121 87.82535552978516\n",
      "122 83.36688995361328\n",
      "123 79.14350891113281\n",
      "124 75.14507293701172\n",
      "125 71.35416412353516\n",
      "126 67.75991821289062\n",
      "127 64.35343933105469\n",
      "128 61.124908447265625\n",
      "129 58.06305694580078\n",
      "130 55.161067962646484\n",
      "131 52.408935546875\n",
      "132 49.79766845703125\n",
      "133 47.32172393798828\n",
      "134 44.97177505493164\n",
      "135 42.74199295043945\n",
      "136 40.62702560424805\n",
      "137 38.61924362182617\n",
      "138 36.71405029296875\n",
      "139 34.906150817871094\n",
      "140 33.188758850097656\n",
      "141 31.558914184570312\n",
      "142 30.01056671142578\n",
      "143 28.540536880493164\n",
      "144 27.144969940185547\n",
      "145 25.818836212158203\n",
      "146 24.558935165405273\n",
      "147 23.362226486206055\n",
      "148 22.225177764892578\n",
      "149 21.145084381103516\n",
      "150 20.118066787719727\n",
      "151 19.142921447753906\n",
      "152 18.216398239135742\n",
      "153 17.335704803466797\n",
      "154 16.49772834777832\n",
      "155 15.701851844787598\n",
      "156 14.94466781616211\n",
      "157 14.224604606628418\n",
      "158 13.54076099395752\n",
      "159 12.89034652709961\n",
      "160 12.271679878234863\n",
      "161 11.683244705200195\n",
      "162 11.123507499694824\n",
      "163 10.591048240661621\n",
      "164 10.084732055664062\n",
      "165 9.603178977966309\n",
      "166 9.144969940185547\n",
      "167 8.709001541137695\n",
      "168 8.294363021850586\n",
      "169 7.899753093719482\n",
      "170 7.524402618408203\n",
      "171 7.167060852050781\n",
      "172 6.826887607574463\n",
      "173 6.503343105316162\n",
      "174 6.195176124572754\n",
      "175 5.90199613571167\n",
      "176 5.622952461242676\n",
      "177 5.357089996337891\n",
      "178 5.104165554046631\n",
      "179 4.863608360290527\n",
      "180 4.634329795837402\n",
      "181 4.4160542488098145\n",
      "182 4.208139896392822\n",
      "183 4.010414123535156\n",
      "184 3.822059154510498\n",
      "185 3.642568826675415\n",
      "186 3.4717154502868652\n",
      "187 3.3088455200195312\n",
      "188 3.1538937091827393\n",
      "189 3.0061147212982178\n",
      "190 2.8657238483428955\n",
      "191 2.7316455841064453\n",
      "192 2.6039958000183105\n",
      "193 2.482459545135498\n",
      "194 2.366607189178467\n",
      "195 2.2561850547790527\n",
      "196 2.1512081623077393\n",
      "197 2.0509705543518066\n",
      "198 1.9556305408477783\n",
      "199 1.8645875453948975\n",
      "200 1.7780208587646484\n",
      "201 1.6954419612884521\n",
      "202 1.6167861223220825\n",
      "203 1.5418380498886108\n",
      "204 1.4703240394592285\n",
      "205 1.402209997177124\n",
      "206 1.3372464179992676\n",
      "207 1.2753820419311523\n",
      "208 1.216426134109497\n",
      "209 1.1602048873901367\n",
      "210 1.1066123247146606\n",
      "211 1.0555408000946045\n",
      "212 1.0068690776824951\n",
      "213 0.9604578614234924\n",
      "214 0.9161478281021118\n",
      "215 0.8739306926727295\n",
      "216 0.833755373954773\n",
      "217 0.7954400777816772\n",
      "218 0.758729100227356\n",
      "219 0.7239165306091309\n",
      "220 0.6906074285507202\n",
      "221 0.6589218378067017\n",
      "222 0.6287174224853516\n",
      "223 0.5998964905738831\n",
      "224 0.572367787361145\n",
      "225 0.5461106300354004\n",
      "226 0.5210918188095093\n",
      "227 0.4972468316555023\n",
      "228 0.4745071828365326\n",
      "229 0.4528571665287018\n",
      "230 0.4321141242980957\n",
      "231 0.41239964962005615\n",
      "232 0.39357781410217285\n",
      "233 0.37561193108558655\n",
      "234 0.358504056930542\n",
      "235 0.3421090245246887\n",
      "236 0.3264743387699127\n",
      "237 0.3116280436515808\n",
      "238 0.2974703311920166\n",
      "239 0.2839395999908447\n",
      "240 0.27101534605026245\n",
      "241 0.2586571276187897\n",
      "242 0.24688664078712463\n",
      "243 0.23569557070732117\n",
      "244 0.22494982182979584\n",
      "245 0.2147701531648636\n",
      "246 0.20503579080104828\n",
      "247 0.19572395086288452\n",
      "248 0.18684646487236023\n",
      "249 0.17835497856140137\n",
      "250 0.17028842866420746\n",
      "251 0.16259807348251343\n",
      "252 0.15522384643554688\n",
      "253 0.14819727838039398\n",
      "254 0.1415126472711563\n",
      "255 0.13511067628860474\n",
      "256 0.12901145219802856\n",
      "257 0.1231590062379837\n",
      "258 0.11760210990905762\n",
      "259 0.1122891753911972\n",
      "260 0.10723540931940079\n",
      "261 0.10240796208381653\n",
      "262 0.09777868539094925\n",
      "263 0.09337404370307922\n",
      "264 0.08917310833930969\n",
      "265 0.08515668660402298\n",
      "266 0.08133544027805328\n",
      "267 0.07767544686794281\n",
      "268 0.07419335097074509\n",
      "269 0.07084909081459045\n",
      "270 0.06767281889915466\n",
      "271 0.06463629752397537\n",
      "272 0.06174355000257492\n",
      "273 0.05897219106554985\n",
      "274 0.05632682889699936\n",
      "275 0.05379620939493179\n",
      "276 0.05140024423599243\n",
      "277 0.04911157116293907\n",
      "278 0.046910662204027176\n",
      "279 0.044804006814956665\n",
      "280 0.04280266538262367\n",
      "281 0.040889590978622437\n",
      "282 0.03908170014619827\n",
      "283 0.03733367100358009\n",
      "284 0.03566042333841324\n",
      "285 0.03406505659222603\n",
      "286 0.032547857612371445\n",
      "287 0.031108874827623367\n",
      "288 0.029720265418291092\n",
      "289 0.028389396145939827\n",
      "290 0.02712385170161724\n",
      "291 0.025920812040567398\n",
      "292 0.024775998666882515\n",
      "293 0.02368009090423584\n",
      "294 0.02262958697974682\n",
      "295 0.021629713475704193\n",
      "296 0.020684311166405678\n",
      "297 0.019776886329054832\n",
      "298 0.01889984682202339\n",
      "299 0.018063578754663467\n",
      "300 0.017267098650336266\n",
      "301 0.01651100255548954\n",
      "302 0.015785817056894302\n",
      "303 0.015094662085175514\n",
      "304 0.014432612806558609\n",
      "305 0.013796505518257618\n",
      "306 0.013194482773542404\n",
      "307 0.012621494941413403\n",
      "308 0.012073365971446037\n",
      "309 0.011542439460754395\n",
      "310 0.011047225445508957\n",
      "311 0.010571536608040333\n",
      "312 0.010112880729138851\n",
      "313 0.0096739511936903\n",
      "314 0.009253152646124363\n",
      "315 0.008865895681083202\n",
      "316 0.008483071811497211\n",
      "317 0.008119896054267883\n",
      "318 0.007778806611895561\n",
      "319 0.007438722997903824\n",
      "320 0.007122475653886795\n",
      "321 0.006822413299232721\n",
      "322 0.006530189421027899\n",
      "323 0.006254149135202169\n",
      "324 0.00599084235727787\n",
      "325 0.00574177410453558\n",
      "326 0.005502004642039537\n",
      "327 0.005269872024655342\n",
      "328 0.005044995341449976\n",
      "329 0.004836282692849636\n",
      "330 0.0046348972246050835\n",
      "331 0.004443051293492317\n",
      "332 0.004259274806827307\n",
      "333 0.004082344006747007\n",
      "334 0.003915645182132721\n",
      "335 0.003758035134524107\n",
      "336 0.0036022858694195747\n",
      "337 0.003455579513683915\n",
      "338 0.0033155488781630993\n",
      "339 0.0031810502987354994\n",
      "340 0.003053245833143592\n",
      "341 0.0029348565731197596\n",
      "342 0.002817898988723755\n",
      "343 0.0027067558839917183\n",
      "344 0.0025997350458055735\n",
      "345 0.0024986970238387585\n",
      "346 0.002400441560894251\n",
      "347 0.00230891490355134\n",
      "348 0.002220785478129983\n",
      "349 0.0021358539815992117\n",
      "350 0.0020561323035508394\n",
      "351 0.001976555911824107\n",
      "352 0.0019030782859772444\n",
      "353 0.0018305081175640225\n",
      "354 0.0017637086566537619\n",
      "355 0.0016970906872302294\n",
      "356 0.0016362356254830956\n",
      "357 0.00157622538972646\n",
      "358 0.0015182180795818567\n",
      "359 0.0014634068356826901\n",
      "360 0.0014118370600044727\n",
      "361 0.0013620738172903657\n",
      "362 0.0013104736572131515\n",
      "363 0.0012662520166486502\n",
      "364 0.0012214225716888905\n",
      "365 0.0011791589204221964\n",
      "366 0.0011371419532224536\n",
      "367 0.00109892210457474\n",
      "368 0.0010621100664138794\n",
      "369 0.001026656711474061\n",
      "370 0.00099275098182261\n",
      "371 0.0009591660345904529\n",
      "372 0.0009275954216718674\n",
      "373 0.0008954200311563909\n",
      "374 0.0008660485618747771\n",
      "375 0.0008377388585358858\n",
      "376 0.0008096387609839439\n",
      "377 0.0007843999774195254\n",
      "378 0.0007599898963235319\n",
      "379 0.0007366660865955055\n",
      "380 0.0007131130550988019\n",
      "381 0.0006910126539878547\n",
      "382 0.000669231521897018\n",
      "383 0.0006481257732957602\n",
      "384 0.0006285412819124758\n",
      "385 0.0006084477063268423\n",
      "386 0.0005908372113481164\n",
      "387 0.0005719433538615704\n",
      "388 0.0005553833907470107\n",
      "389 0.0005382981617003679\n",
      "390 0.0005225897766649723\n",
      "391 0.00050767557695508\n",
      "392 0.0004918735940009356\n",
      "393 0.00047844875371083617\n",
      "394 0.00046540022594854236\n",
      "395 0.0004515505279414356\n",
      "396 0.000438401271821931\n",
      "397 0.000425691221607849\n",
      "398 0.0004139428783673793\n",
      "399 0.0004026408423669636\n",
      "400 0.00039115920662879944\n",
      "401 0.00038091500755399466\n",
      "402 0.00037017613067291677\n",
      "403 0.000360506703145802\n",
      "404 0.00035018191556446254\n",
      "405 0.0003409359778743237\n",
      "406 0.0003310796746518463\n",
      "407 0.0003231024311389774\n",
      "408 0.0003145604277960956\n",
      "409 0.0003070253296755254\n",
      "410 0.00029917352367192507\n",
      "411 0.00029170766356401145\n",
      "412 0.000284508423646912\n",
      "413 0.0002773021114990115\n",
      "414 0.0002702094498090446\n",
      "415 0.0002635327400639653\n",
      "416 0.0002567032934166491\n",
      "417 0.00025061372434720397\n",
      "418 0.00024524269974790514\n",
      "419 0.0002389829751336947\n",
      "420 0.0002332200965611264\n",
      "421 0.0002278440515510738\n",
      "422 0.0002228086959803477\n",
      "423 0.00021736671624239534\n",
      "424 0.0002120264107361436\n",
      "425 0.00020725947979371995\n",
      "426 0.00020257369033060968\n",
      "427 0.00019829408847726882\n",
      "428 0.0001935758045874536\n",
      "429 0.00018994923448190093\n",
      "430 0.00018545513739809394\n",
      "431 0.0001814812421798706\n",
      "432 0.00017717477749101818\n",
      "433 0.000173868567799218\n",
      "434 0.00016949574637692422\n",
      "435 0.00016623795090708882\n",
      "436 0.0001627215533517301\n",
      "437 0.000159118149895221\n",
      "438 0.00015629976405762136\n",
      "439 0.00015303846157621592\n",
      "440 0.0001496960176154971\n",
      "441 0.00014697061851620674\n",
      "442 0.00014296226436272264\n",
      "443 0.00014022474351804703\n",
      "444 0.00013731293438468128\n",
      "445 0.00013445633521769196\n",
      "446 0.00013202583068050444\n",
      "447 0.00012913253158330917\n",
      "448 0.0001267951010959223\n",
      "449 0.00012457651610020548\n",
      "450 0.00012229735148139298\n",
      "451 0.00011913885828107595\n",
      "452 0.00011679674935294315\n",
      "453 0.000114332367957104\n",
      "454 0.00011266159708611667\n",
      "455 0.00011037164222216234\n",
      "456 0.0001084938267013058\n",
      "457 0.00010618615488056093\n",
      "458 0.00010436162119731307\n",
      "459 0.00010245402518194169\n",
      "460 0.00010047790419775993\n",
      "461 9.8695709311869e-05\n",
      "462 9.721384412841871e-05\n",
      "463 9.531441901344806e-05\n",
      "464 9.363400749862194e-05\n",
      "465 9.17593133635819e-05\n",
      "466 9.01618113857694e-05\n",
      "467 8.851470920490101e-05\n",
      "468 8.708964742254466e-05\n",
      "469 8.529603655915707e-05\n",
      "470 8.421079110121354e-05\n",
      "471 8.216898277169093e-05\n",
      "472 8.096110104816034e-05\n",
      "473 7.943712262203917e-05\n",
      "474 7.835907308617607e-05\n",
      "475 7.7189935836941e-05\n",
      "476 7.600283424835652e-05\n",
      "477 7.462861685780808e-05\n",
      "478 7.321585871977732e-05\n",
      "479 7.198748062364757e-05\n",
      "480 7.094903412507847e-05\n",
      "481 6.977964221732691e-05\n",
      "482 6.879368447698653e-05\n",
      "483 6.768138700863346e-05\n",
      "484 6.660066719632596e-05\n",
      "485 6.564641807926819e-05\n",
      "486 6.448692147387192e-05\n",
      "487 6.336272053886205e-05\n",
      "488 6.243826646823436e-05\n",
      "489 6.15300887147896e-05\n",
      "490 6.077492798794992e-05\n",
      "491 5.969261837890372e-05\n",
      "492 5.857047290191986e-05\n",
      "493 5.7781919167609885e-05\n",
      "494 5.704715658794157e-05\n",
      "495 5.6315289839403704e-05\n",
      "496 5.541797509067692e-05\n",
      "497 5.4587544582318515e-05\n",
      "498 5.385101030697115e-05\n",
      "499 5.310506094247103e-05\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "class MyReLU(torch.autograd.Function):\n",
    "    @staticmethod\n",
    "    def forward(ctx, input):\n",
    "        ctx.save_for_backward(input)\n",
    "        return input.clamp(min=0)\n",
    "\n",
    "    @staticmethod\n",
    "    def backward(ctx, grad_output):\n",
    "        input, = ctx.saved_tensors\n",
    "        #print(grad_output)\n",
    "        grad_input = grad_output.clone()\n",
    "        grad_input[input < 0] = 0\n",
    "        return grad_input\n",
    "\n",
    "class MyLoss(torch.autograd.Function):  \n",
    "    @staticmethod\n",
    "    def forward(ctx, y, y_pred):\n",
    "        ctx.save_for_backward(y, y_pred)\n",
    "        return (y_pred - y).pow(2).sum()\n",
    "\n",
    "    @staticmethod\n",
    "    def backward(ctx, grad_output):\n",
    "        yy, yy_pred = ctx.saved_tensors\n",
    "        grad_input = grad_output.clone()\n",
    "        grad_input = torch.neg(2.0 * (yy_pred - yy))\n",
    "        return grad_input, None\n",
    "\n",
    "dtype = torch.float\n",
    "device = torch.device(\"cpu\")\n",
    "N, D_in, H, D_out = 64, 1000, 100, 10\n",
    "x = torch.randn(N, D_in, device=device, dtype=dtype)\n",
    "y = torch.randn(N, D_out, device=device, dtype=dtype)\n",
    "\n",
    "w1 = torch.randn(D_in, H, device=device, dtype=dtype, requires_grad=True)\n",
    "w2 = torch.randn(H, D_out, device=device, dtype=dtype, requires_grad=True)\n",
    "\n",
    "learning_rate = 1e-6\n",
    "\n",
    "for t in range(500):\n",
    "    relu = MyReLU.apply\n",
    "    myloss = MyLoss.apply \n",
    "    y_pred = relu(x.mm(w1)).mm(w2)\n",
    "    loss = myloss(y_pred, y)\n",
    "    print(t, loss.item())\n",
    "    loss.backward()\n",
    "    with torch.no_grad():\n",
    "        w1 -= learning_rate * w1.grad\n",
    "        w2 -= learning_rate * w2.grad\n",
    "        w1.grad.zero_()\n",
    "        w2.grad.zero_()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "masterarbeit",
   "language": "python",
   "name": "masterarbeit"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
