{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.nn.utils import clip_grad_norm_\n",
    "from torch.autograd import Variable\n",
    "from torchsummary import summary\n",
    "import pandas as pd\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import matplotlib.pyplot as plt\n",
    "import torch.optim as optim\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from keras.preprocessing.sequence import TimeseriesGenerator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "hyperparam = {\n",
    "    \"data\" : {\n",
    "        \"stake_training_data\" : 0.75, \n",
    "        \"path\" : '../../data/vega_shrinkwrapper_standardized/'\n",
    "    },\n",
    "    \"model\" : {\n",
    "        \"input_size\" : 7,\n",
    "        \"n_hidden\" : 150,\n",
    "        \"sequence_size\" : 20,\n",
    "        \"batch_size\" : 1,\n",
    "        \"lstm_layer\" : 3,\n",
    "    },\n",
    "    \"cycling_lr\" : {\n",
    "        # step_size is the number of training iterations (total samples/batch_size) per half cycle. \n",
    "        # Authors suggest setting step_size 2-8 x training iterations in epoch.\n",
    "        \"step_size\" : (1536/8)*4, \n",
    "        # Mode can be one of {triangular, triangular2, exp_range}\n",
    "        \"mode\" : \"triangular2\", \n",
    "        \"gamma\" : 0.9995,\n",
    "        \"base_lr\" : 3e-3, \n",
    "        \"max_lr\" :0.1\n",
    "    },\n",
    "    \"training\": {\n",
    "        \"n_epochs\" : 20,\n",
    "        \"patience\" : 50,\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Split train and test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataSplitter():\n",
    "    def __init__(self, path):\n",
    "        self.path = path\n",
    "        \n",
    "    def load_data(self):\n",
    "        return pd.read_csv(self.path)\n",
    "        \n",
    "    def split_data(self, stake_training_data):\n",
    "        dataset = self.load_data()\n",
    "        amount_training_data = round(len(dataset)*stake_training_data)\n",
    "        train_data = dataset.iloc[0:amount_training_data,:]\n",
    "        validation_data = dataset.iloc[amount_training_data:,:]\n",
    "        return train_data, validation_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = DataSplitter(path=hyperparam['data']['path']+'NewBlade001.csv')\n",
    "train_data, validation_data = train_loader.split_data(stake_training_data=hyperparam['data']['stake_training_data'], )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Datahandler \n",
    "Time series data must be transformed into a structure of samples with `input` and `target` components before it can be used to fit a supervised learning model. <br>\n",
    "For a time series interested in one-step predictions, the observations at prior time steps, so-called lag observations, are used as `input` and the `target` is the observation at the current time step.\n",
    "\n",
    "For example, a univariate series can be expressed as a supervised learning problem with three time steps for `input` and one step as `target`, as follows:\n",
    "\n",
    "|input|target|\n",
    "|-----|------|\n",
    "[1, 2, 3]|[4]\n",
    "[2, 3, 4]|[5]\n",
    "[3, 4, 5]|[6]\n",
    "\n",
    "The Keras deep learning library provides the `TimeseriesGenerator` to automatically transform both univariate and multivariate time series data into such a format. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Size of multivariate time series\n",
    "Actually the data has a other size than in the table above because of multivariate time series data and because of using batches. <br>\n",
    "__First dimension__: batch size --> Defines the number of samples that will be propagated through the network simultaneously. <br>\n",
    "__Second dimension:__ timesteps --> Number of sequence which is passed into the LSTM <br>\n",
    "__Third dimension:__ input_dim --> Is the number of features. In this case data from 7 sensors, collected at the same time. <br>\n",
    "\n",
    "![](../../knowledge/pictures/input_shape.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataProvider(Dataset):\n",
    "    def __init__(self, data, timesteps):\n",
    "        # All data are loaded from csv file and converted to an numpy array\n",
    "        self.data = data.values\n",
    "        # Data generator is initialized, batch_size=1 is indipendent of neural network's batch_size \n",
    "        self.generator = TimeseriesGenerator(self.data, self.data, length=timesteps, batch_size=1)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        x, y = self.generator[index]\n",
    "        x_torch = torch.from_numpy(x)\n",
    "        # Dimension 0 with size 1 (created by TimeseriesGenerator because of batch_size=1) gets removed \n",
    "        # because DataLoader will add a dimension 0 with size=batch_size as well\n",
    "        x_torch = torch.squeeze(x_torch) # torch.Size([1, timesteps, 7]) --> torch.Size([timesteps, 7])\n",
    "        y_torch = torch.from_numpy(y)\n",
    "        return (x_torch.float(), y_torch.float()) \n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.generator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_train = DataProvider(train_data, timesteps=hyperparam[\"model\"][\"sequence_size\"])\n",
    "dataset_validation = DataProvider(validation_data, timesteps=hyperparam[\"model\"][\"sequence_size\"])\n",
    "\n",
    "data_loader_training = DataLoader(dataset_train, batch_size=hyperparam[\"model\"][\"batch_size\"], \n",
    "                                  num_workers=1, shuffle=False, drop_last=True)\n",
    "data_loader_validation = DataLoader(dataset_validation, batch_size=hyperparam[\"model\"][\"batch_size\"], \n",
    "                                    num_workers=1, shuffle=False, drop_last=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define Architecture of Neural Network\n",
    "__Parameters for LSTM Modul:__\n",
    "- input_size : The number of expected features in the input x\n",
    "- hidden_size :The number of features in the hidden state h\n",
    "- num_layers : Number of recurrent layers. E.g., setting num_layers=2 would mean stacking two LSTMs together to form a stacked LSTM, with the second LSTM taking in outputs of the first LSTM and computing the final results.\n",
    "- batch_first : If True, then the input __and output__ tensors are provided as (batch, seq, feature).\n",
    "- dropout – If non-zero, introduces a Dropout layer on the outputs of each LSTM layer except the last layer, with dropout probability equal to dropout. Default: 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTM(nn.Module):\n",
    "    def __init__(self, batch_size, input_dim, n_hidden, n_layers):\n",
    "        super(LSTM, self).__init__()\n",
    "        # Attributes for LSTM Network\n",
    "        self.input_dim = input_dim\n",
    "        self.n_hidden = n_hidden\n",
    "        self.n_layers = n_layers\n",
    "        self.batch_size = batch_size\n",
    "        \n",
    "        # Attribut for Gaussians\n",
    "        self.n_gaus_param = 2\n",
    "        \n",
    "        # Definition of NN layer\n",
    "        # batch_first = True because dataloader creates batches and batch_size is 0. dimension\n",
    "        self.lstm = nn.LSTM(input_size = self.input_dim, hidden_size = self.n_hidden, num_layers = self.n_layers, batch_first = True)\n",
    "        self.fc1 = nn.Linear(self.n_hidden, self.n_gaus_param * self.input_dim)\n",
    "        \n",
    "    def forward(self, input_data, hidden):\n",
    "        # Forward propagate LSTM\n",
    "        # LSTM in Pytorch return two results the first one usually called output and the second one (hidden_state, cell_state). \n",
    "        # As output the LSTM returns all the hidden_states for all the timesteps (seq), in other words all of the hidden states throughout\n",
    "        # the sequence\n",
    "        # As hidden_state the LSTM returns just the most recent hidden state\n",
    "        lstm_out, (hidden_state, cell_state) = self.lstm(input_data, hidden)\n",
    "        # Length of input data can varry \n",
    "        length_seq = input_data.size()[1]\n",
    "        # Select the output from the last sequence \n",
    "        last_out = lstm_out[:,length_seq-1,:]\n",
    "        out = self.fc1(last_out)\n",
    "        # Reshape out to shape torch.Size(batch_size, n_features, 2)\n",
    "        raw_output = out.view(self.batch_size, self.input_dim, 2)\n",
    "        # y_hat and tau alternate, y_hat and tau are next to each other for each feature \n",
    "        y_hat = raw_output[:,:,0]\n",
    "        tau = raw_output[:,:,1]\n",
    "        # σ = exp(τ) guarantees σ > 0 and provides numerical stability in the learning process\n",
    "        sigma = torch.exp(tau)\n",
    "        return y_hat, sigma\n",
    "    \n",
    "    def init_hidden(self):\n",
    "        # This method is for initializing hidden state as well as cell state\n",
    "        # We need to detach the hidden state to prevent exploding/vanishing gradients\n",
    "        # The axes semantics are (num_layers, minibatch_size, hidden_dim)\n",
    "        h0 = torch.zeros(self.n_layers, self.batch_size, self.n_hidden, requires_grad=False)\n",
    "        c0 = torch.zeros(self.n_layers, self.batch_size, self.n_hidden, requires_grad=False)\n",
    "        return [t for t in (h0, c0)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialize Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = LSTM(batch_size=hyperparam['model']['batch_size'], input_dim=hyperparam['model']['input_size'], \n",
    "             n_hidden=hyperparam['model']['n_hidden'], n_layers=hyperparam['model']['lstm_layer'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cyclic Learning Rate\n",
    "CLR is used to enhance the way the learning rate is scheduled during training, to provide better convergence and help in regularizing deep learning models. It eliminates the need to experimentally find the best values for the global learning rate. Allowing the learning rate to cyclically vary between lower and upper boundary values. The idea is to divide the training process into cycles determined by a stepsize parameter, which defines the number of iterations in half a cycle.\n",
    "![](../../knowledge/pictures/scheduler.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Find base_lr and max_lr\n",
    "Calculate the upper bound of the learning rate for your model. The way to do this is to:\n",
    "1. Define an initial learning rate, the lower boundary of the range you want to test (let’s say 1e-7)\n",
    "2. Define an upper boundary of the range (let’s say 0.1)\n",
    "3. Define an exponential scheme to run through this step by step: <br>\n",
    "```python\n",
    "lr_lambda = lambda x: math.exp(x * math.log(end_lr / start_lr) / (lr_find_epochs * len( data_loader[\"train\"])))\n",
    "scheduler = torch.optim.lr_scheduler.LambdaLR(optimizer, lr_lambda)\n",
    "```\n",
    "<br>\n",
    "__Script for this is in: __  <br>\n",
    "`find_base_and_max_lr.ipynb`\n",
    "\n",
    "- A good upper bound (max_lr) is not on the lowest point, but about a factor of 10 to the left. (In this case 3e-3)\n",
    "- A good lower bound (base_lr), according to the paper and other sources, is the upper bound, divided by a factor 6.\n",
    "\n",
    "### 2. Implement cyclic learning rate with founded parameters\n",
    "\n",
    "## Initialize optimizer and Cyclic Learning Rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lr=1. because of scheduler (1*learning_rate_schedular)\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=0.1)  \n",
    "\n",
    "scheduler = torch.optim.lr_scheduler.CyclicLR(optimizer=optimizer, base_lr=hyperparam['cycling_lr']['base_lr'], \n",
    "                                              max_lr=hyperparam['cycling_lr']['max_lr'], step_size_up=hyperparam['cycling_lr']['step_size'], \n",
    "                                              mode=hyperparam['cycling_lr']['mode'], gamma=hyperparam['cycling_lr']['gamma'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define Loss function\n",
    "\n",
    "- torch.exp(input) --> Returns a new tensor with the exponential of the elements of the input tensor : $y_i = e^{x_i}$\n",
    "- normal_distribution = torch.distributions.Normal(loc=, scale=) --> Create a normal (Gaussian) distribution parameterized by loc (mean) and scale (standard deviation).\n",
    "- normal_distribution.log_prob(value) --> Returns the log of the probability density function (in this case of normal distribution) evaluated at value.\n",
    "- torch.sum(input=, dim=) --> Returns the sum of all elements in the input tensor; dim is the dimension to reduce \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loss function Prof. Niggemann"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LogLikelihood(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super(LogLikelihood, self).__init__()\n",
    "        \n",
    "    def forward(self, model_output, target_data):\n",
    "        y_hat, sigma = model_output\n",
    "        #print(\"y_hat -- {}\".format(y_hat))\n",
    "        #print(\"sigma -- {}\".format(sigma))\n",
    "\n",
    "        # target_data.size(batch_size,1,7) because timestep = 1\n",
    "        # new target_data.size(batch_size,7)\n",
    "        target_data = torch.squeeze(target_data)\n",
    "        #print(\"target -- {}\".format(target_data))\n",
    "        \n",
    "        term = ((target_data-y_hat)/sigma)**2 + 2*torch.log(sigma)\n",
    "        #print(\"Term -- {}\".format(term))\n",
    "        loss_batches = torch.sum(input=term, dim=1)\n",
    "        #print(\"Loss sum -- {}\".format(loss_batches))\n",
    "    \n",
    "        # The value being returned by a loss function MUST BE a scalar value. Not a vector/tensor.\n",
    "        # In this case we take the mean of mini-batches' losses\n",
    "        mean_loss = torch.sum(loss_batches)/hyperparam['model']['batch_size']\n",
    "        #print(mean_loss)\n",
    "        return mean_loss     "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Loop\n",
    "An epoch consists of a learning cycle over all batches of training data and an evaluation of the most recent model with the testing data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start model training\n",
      "-------- epoch_no. 0 finished with training loss 5.976187707939847--------\n",
      "-------- epoch_no. 0 finished with eval loss 10.160571336746216--------\n",
      "Epoch 0: best model saved with loss: 10.160571336746216\n",
      "-------- epoch_no. 1 finished with training loss 5.976187707939847--------\n",
      "-------- epoch_no. 1 finished with eval loss 10.160571336746216--------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/Users/carlovoss/anaconda3/lib/python3.6/multiprocessing/queues.py\", line 240, in _feed\n",
      "    send_bytes(obj)\n",
      "  File \"/Users/carlovoss/anaconda3/lib/python3.6/multiprocessing/connection.py\", line 200, in send_bytes\n",
      "    self._send_bytes(m[offset:offset + size])\n",
      "  File \"/Users/carlovoss/anaconda3/lib/python3.6/multiprocessing/connection.py\", line 404, in _send_bytes\n",
      "    self._send(header + buf)\n",
      "  File \"/Users/carlovoss/anaconda3/lib/python3.6/multiprocessing/connection.py\", line 368, in _send\n",
      "    n = write(self._handle, buf)\n",
      "BrokenPipeError: [Errno 32] Broken pipe\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-95-663984da9b16>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     42\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m         \u001b[0;31m# Backward pass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 44\u001b[0;31m         \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     45\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m         \u001b[0;31m# Update parameters\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/jupyter_notebooks/masterarbeit/venv_pm/lib/python3.6/site-packages/torch/tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph)\u001b[0m\n\u001b[1;32m    105\u001b[0m                 \u001b[0mproducts\u001b[0m\u001b[0;34m.\u001b[0m \u001b[0mDefaults\u001b[0m \u001b[0mto\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    106\u001b[0m         \"\"\"\n\u001b[0;32m--> 107\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    108\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    109\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/jupyter_notebooks/masterarbeit/venv_pm/lib/python3.6/site-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables)\u001b[0m\n\u001b[1;32m     91\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[1;32m     92\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 93\u001b[0;31m         allow_unreachable=True)  # allow_unreachable flag\n\u001b[0m\u001b[1;32m     94\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     95\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "print(\"Start model training\")\n",
    "# Create lists to save training loss and validation loss of each epoch\n",
    "hist_training_loss = []\n",
    "hist_validation_loss = []\n",
    "lr_find_lr = []\n",
    "epoch_training_loss = []\n",
    "epoch_validation_loss = []\n",
    "\n",
    "# Set first comparative value\n",
    "lowest_loss = 99\n",
    "trials = 0\n",
    "\n",
    "for epoch in range(hyperparam[\"training\"][\"n_epochs\"]+1):\n",
    "    # Empty list for recording performance \n",
    "    epoch_training_loss = []\n",
    "    epoch_validation_loss = []\n",
    "    \n",
    "    model.train()\n",
    "    \n",
    "    ##### Training #####\n",
    "    for batch_number, data in enumerate(data_loader_training):\n",
    "        # The LSTM has to be reinitialised, otherwise the LSTM will treat a new batch as a continuation of a sequence.\n",
    "        # When batches of data are independent sequences, then you should reinitialise the hidden state before each batch. \n",
    "        # But if your data is made up of really long sequences and you cut it up into batches making sure that each batch \n",
    "        # follows on from the previous batch, then in that case you wouldn’t reinitialise the hidden state before each batch.\n",
    "        # In the current workflow of class DataProvoider independent sequences are returned. \n",
    "        \n",
    "        input_data, target_data = data\n",
    "        \n",
    "        hidden = model.init_hidden()\n",
    "        \n",
    "        # Zero out gradient, else they will accumulate between minibatches\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # Forward propagation\n",
    "        output = model(input_data, hidden)\n",
    "         \n",
    "        # Calculate loss\n",
    "        criterion = LogLikelihood()\n",
    "        loss = criterion(output, target_data)\n",
    "        epoch_training_loss.append(loss.item())\n",
    "    \n",
    "        # Backward pass\n",
    "        loss.backward()\n",
    "        \n",
    "        # Update parameters\n",
    "        optimizer.step()\n",
    "        \n",
    "        # Update LR\n",
    "        #scheduler.step()\n",
    "        lr_step = optimizer.state_dict()[\"param_groups\"][0][\"lr\"]\n",
    "        lr_find_lr.append(lr_step)\n",
    "    \n",
    "    \n",
    "    # Save mean of loss over all training iterations\n",
    "    mean_epoch_training_loss = sum(epoch_training_loss) / float(len(epoch_training_loss))\n",
    "    hist_training_loss.append(mean_epoch_training_loss)\n",
    "    \n",
    "    print(\"-------- epoch_no. {} finished with training loss {}--------\".format(epoch, mean_epoch_training_loss))\n",
    "        \n",
    "    ##### Evaluation #######\n",
    "    for input_data, target_data in data_loader_validation:\n",
    "        # Change model to evaluation (prediction) mode\n",
    "        model.eval()\n",
    "        hidden = model.init_hidden()\n",
    "        \n",
    "        # Predict \n",
    "        output = model(input_data, hidden)\n",
    "        \n",
    "         # Calculate loss\n",
    "        #loss = loss_function(output, target_data)\n",
    "        criterion = LogLikelihood()\n",
    "        loss = criterion(output, target_data)\n",
    "        epoch_validation_loss.append(loss.item())\n",
    "        \n",
    "    # Save mean of loss over all validation iterations to epoch history  \n",
    "    mean_epoch_validation_loss = sum(epoch_validation_loss) / float(len(epoch_validation_loss))\n",
    "    hist_validation_loss.append(mean_epoch_validation_loss)\n",
    "    \n",
    "    print(\"-------- epoch_no. {} finished with eval loss {}--------\".format(epoch, mean_epoch_validation_loss))\n",
    "        \n",
    "    # Check after every evaluation whether the latest model is the best one or not\n",
    "    # If this is the case, set current score to best_score, reset trails and save the model.\n",
    "    if mean_epoch_validation_loss < lowest_loss:\n",
    "        trials = 0\n",
    "        lowest_loss = mean_epoch_validation_loss\n",
    "        torch.save({\n",
    "            'epoch': epoch,\n",
    "            'model_state_dict': model.state_dict(),\n",
    "            'optimizer_state_dict': optimizer.state_dict(),\n",
    "            'loss': mean_epoch_validation_loss\n",
    "        }, \"../../models/best_model.pt\")\n",
    "        print(\"Epoch {}: best model saved with loss: {}\".format(epoch, mean_epoch_validation_loss))\n",
    "    \n",
    "    # Else: Increase trails by one and start new epoch as long as not too many epochs \n",
    "    # were unsuccessful (controlled by patience)\n",
    "    else:\n",
    "        trials += 1\n",
    "        if trials >= hyperparam['training']['patience'] :\n",
    "            print(f'Early stopping on epoch {epoch}')\n",
    "            break  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "x = range(len(lr_find_lr))\n",
    "data = pd.DataFrame(data={'y': lr_find_lr, 'x': x})\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "f, ax = plt.subplots(figsize=(7, 7))\n",
    "#ax.set(xscale=\"log\")\n",
    "plt.ylim(0, 0.1)\n",
    "plt.xlim(0, 3000)\n",
    "\n",
    "sns.scatterplot(x=data.x, y=data.y, ax=ax)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epoch_no = range(len(epoch_training_loss))\n",
    "loss = pd.DataFrame(data={'y': epoch_training_loss, 'x': epoch_no})\n",
    "f, ax = plt.subplots(figsize=(15, 15))\n",
    "sns.scatterplot(x=loss.x, y=loss.y, ax=ax)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "masterarbeit",
   "language": "python",
   "name": "masterarbeit"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
